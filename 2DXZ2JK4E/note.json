{
  "paragraphs": [
    {
      "text": "%md\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\n# Machine Learning with Spark MLlib Using Scala\n\nThis notebook contains code that uses the Machine Learning (\u003cem\u003eML\u003c/em\u003e) Library embedded in Spark, *MLlib*, with the Splice Machine Spark Adapter to realize in-process machine learning. Specifically, the example in this notebook uses data that tracks international shipments to learn, and then predicts how late a shipment will be, based on various factors.\n\n\u003cp class\u003d\"noteIcon\"\u003eThis notebook implements the same functionality as the previous notebook in this class, \u003cem\u003eMachine Learning with Spark MLlib Using Python\u003c/em\u003e. This notebook uses Scala instead of Python as the implementation language.\u003c/p\u003e \n\nThe remainder of this notebook contains these sections:\n\n* \u003cem\u003eCreating our Splice Machine Database\u003c/em\u003e walks you through setting up our database with our sample data.\n* \u003cem\u003eCreating, Training, and Deploying our Learning Model\u003c/em\u003e walks you through our Machine Learning sample code.\n* \u003cem\u003eProgram Listing\u003c/em\u003e contains a listing of all of the code used in this notebook.",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 06:59:20.006",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\u003ch1\u003eMachine Learning with Spark MLlib Using Scala\u003c/h1\u003e\n\u003cp\u003eThis notebook contains code that uses the Machine Learning (\u003cem\u003eML\u003c/em\u003e) Library embedded in Spark, \u003cem\u003eMLlib\u003c/em\u003e, with the Splice Machine Spark Adapter to realize in-process machine learning. Specifically, the example in this notebook uses data that tracks international shipments to learn, and then predicts how late a shipment will be, based on various factors.\u003c/p\u003e\n\u003cp class\u003d\"noteIcon\"\u003eThis notebook implements the same functionality as the previous notebook in this class, \u003cem\u003eMachine Learning with Spark MLlib Using Python\u003c/em\u003e. This notebook uses Scala instead of Python as the implementation language.\u003c/p\u003e\n\u003cp\u003eThe remainder of this notebook contains these sections:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\u003cem\u003eCreating our Splice Machine Database\u003c/em\u003e walks you through setting up our database with our sample data.\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eCreating, Training, and Deploying our Learning Model\u003c/em\u003e walks you through our Machine Learning sample code.\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eProgram Listing\u003c/em\u003e contains a listing of all of the code used in this notebook.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542399640554_-240228168",
      "id": "20180129-160012_924943773",
      "dateCreated": "2018-11-16 12:20:40.000",
      "dateStarted": "2018-11-30 21:47:59.698",
      "dateFinished": "2018-11-30 21:47:59.711",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n##  Creating our Splice Machine Database\n\nBefore working with the MLlib, we need to create a Splice Machine database that contains the shipping data we\u0027re using. We:\n\n1. Connect to you database via JDBC\n2. Create the schema and tables\n2. Import the data\n3. Create our features table\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 06:59:23.482",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eCreating our Splice Machine Database\u003c/h2\u003e\n\u003cp\u003eBefore working with the MLlib, we need to create a Splice Machine database that contains the shipping data we\u0026rsquo;re using. We:\u003c/p\u003e\n\u003col\u003e\n  \u003cli\u003eConnect to you database via JDBC\u003c/li\u003e\n  \u003cli\u003eCreate the schema and tables\u003c/li\u003e\n  \u003cli\u003eImport the data\u003c/li\u003e\n  \u003cli\u003eCreate our features table\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542399640555_336761167",
      "id": "20180202-100849_1233744001",
      "dateCreated": "2018-11-16 12:20:40.000",
      "dateStarted": "2018-11-30 21:48:21.673",
      "dateFinished": "2018-11-30 21:48:21.687",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 1. Connect to Your Database via JDBC\n\nFirst we\u0027ll configure the URL we\u0027ll use in our JDBC connection to Splice Machine. \n\nFor this class, you can simply use the `defaultJDBCURL` assignment in the next paragraph. When running on a cluster, you can copy and paste the JDBC URL you\u0027ll find displayed at the bottom right of your cluster dashboard.",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 06:59:27.797",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003e1. Connect to Your Database via JDBC\u003c/h3\u003e\n\u003cp\u003eFirst we\u0026rsquo;ll configure the URL we\u0026rsquo;ll use in our JDBC connection to Splice Machine. \u003c/p\u003e\n\u003cp\u003eFor this class, you can simply use the \u003ccode\u003edefaultJDBCURL\u003c/code\u003e assignment in the next paragraph. When running on a cluster, you can copy and paste the JDBC URL you\u0026rsquo;ll find displayed at the bottom right of your cluster dashboard.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542763357881_671811763",
      "id": "20181120-172237_1731001469",
      "dateCreated": "2018-11-20 17:22:37.881",
      "dateStarted": "2018-11-30 21:49:21.151",
      "dateFinished": "2018-11-30 21:49:21.181",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n  val defaultJDBCURL \u003d \"\"\"jdbc:splice://localhost:1527/splicedb;user\u003dsplice;password\u003dadmin\"\"\"\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-21 17:55:05.915",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {
          "JDBCurl": "jdbc:splice://localhost:1527/splicedb;user\u003dsplice;password\u003dadmin",
          "JDBCurl Scala": "jdbc:splice://localhost:1527/splicedb;user\u003dsplice;password\u003dadmin;useSpark\u003dtrue",
          "JDBCURL Scala": "jdbc:splice:/localhost:1527/splicedb;user\u003dsplice;password\u003dadmin",
          "JDBCxurl": "jdbc:splice://localhost:1527/splicedb;user\u003dsplice;password\u003dadmin;useSpark\u003dtrue"
        },
        "forms": {
          "JDBCurl": {
            "type": "TextBox",
            "name": "JDBCurl",
            "displayName": "JDBCurl",
            "defaultValue": "jdbc:splice://localhost:1527/splicedb;user\u003dsplice;password\u003dadmin",
            "hidden": false
          }
        }
      },
      "apps": [],
      "jobName": "paragraph_1542399640557_929551616",
      "id": "20180215-062654_2077965041",
      "dateCreated": "2018-11-16 12:20:40.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n### 2. Create the Schema and Tables\n\nWe\u0027ll now create our new schema, make it our default schema, and then create the tables for the `shipment_in_transit` and `shipment_history` data that we will import.",
      "user": "anonymous",
      "dateUpdated": "2018-11-20 17:23:45.936",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003e2. Create the Schema and Tables\u003c/h3\u003e\n\u003cp\u003eWe\u0026rsquo;ll now create our new schema, make it our default schema, and then create the tables for the \u003ccode\u003eshipment_in_transit\u003c/code\u003e and \u003ccode\u003eshipment_history\u003c/code\u003e data that we will import.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542763375892_-53705224",
      "id": "20181120-172255_1826580030",
      "dateCreated": "2018-11-20 17:22:55.892",
      "dateStarted": "2018-11-20 17:23:45.937",
      "dateFinished": "2018-11-20 17:23:45.940",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\n\nCREATE SCHEMA DS_ASN;\nSET SCHEMA DS_ASN;\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 06:59:49.733",
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 4.0,
        "editorMode": "ace/mode/sql",
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Query executed successfully. Affected rows : 0"
          },
          {
            "type": "TEXT",
            "data": "Query executed successfully. Affected rows : 0"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542399640558_-325968537",
      "id": "20180202-101539_620068341",
      "dateCreated": "2018-11-16 12:20:40.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\n\nDROP TABLE IF EXISTS SHIPMENT_IN_TRANSIT;\nCREATE TABLE SHIPMENT_IN_TRANSIT(\n    SHIPMENTID VARCHAR(11) NOT NULL PRIMARY KEY,\n    STATUS VARCHAR(50),\n    SHIPMODE VARCHAR(30),\n    PRODUCT_DESCRIPTION VARCHAR(500),\n    CONSIGNEE VARCHAR(200),\n    SHIPPER VARCHAR(100),\n    ARRIVAL_DATE TIMESTAMP,\n    GROSS_WEIGHT_LB INTEGER,\n    GROSS_WEIGHT_KG INTEGER,\n    FOREIGN_PORT VARCHAR(50),\n    US_PORT VARCHAR(50),\n    VESSEL_NAME VARCHAR(40),\n    COUNTRY_OF_ORIGIN VARCHAR(40),\n    CONSIGNEE_ADDRESS VARCHAR(150),\n    SHIPPER_ADDRESS VARCHAR(150),\n    ZIPCODE VARCHAR(20),\n    NO_OF_CONTAINERS INTEGER,\n    CONTAINER_NUMBER VARCHAR(200),\n    CONTAINER_TYPE VARCHAR(80),\n    QUANTITY INTEGER,\n    QUANTITY_UNIT VARCHAR(10),\n    MEASUREMENT INTEGER,\n    MEASUREMENT_UNIT VARCHAR(5),\n    BILL_OF_LADING VARCHAR(20),\n    HOUSE_VS_MASTER CHAR(1),\n    DISTRIBUTION_PORT VARCHAR(40),\n    MASTER_BL VARCHAR(20),\n    VOYAGE_NUMBER VARCHAR(10),\n    SEAL VARCHAR(300),\n    SHIP_REGISTERED_IN VARCHAR(40),\n    INBOND_ENTRY_TYPE VARCHAR(30),\n    CARRIER_CODE VARCHAR(10),\n    CARRIER_NAME VARCHAR(40),\n    CARRIER_CITY VARCHAR(40),\n    CARRIER_STATE VARCHAR(10),\n    CARRIER_ZIP VARCHAR(10),\n    CARRIER_ADDRESS VARCHAR(200),\n    NOTIFY_PARTY VARCHAR(50),\n    NOTIFY_ADDRESS VARCHAR(200),\n    PLACE_OF_RECEIPT VARCHAR(50),\n    DATE_OF_RECEIPT TIMESTAMP\n    );\n\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 12:20:40.000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 4.0,
        "editorMode": "ace/mode/sql",
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542399640558_-1338917347",
      "id": "20180202-102107_567153190",
      "dateCreated": "2018-11-16 12:20:40.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\nDROP TABLE IF EXISTS SHIPMENT_HISTORY;\nCREATE TABLE SHIPMENT_HISTORY(\n    SHIPMENTID VARCHAR(11) NOT NULL PRIMARY KEY,\n    STATUS VARCHAR(50),\n    SHIPMODE VARCHAR(30),\n    PRODUCT_DESCRIPTION VARCHAR(500),\n    CONSIGNEE VARCHAR(200),\n    SHIPPER VARCHAR(100),\n    ARRIVAL_DATE TIMESTAMP,\n    GROSS_WEIGHT_LB INTEGER,\n    GROSS_WEIGHT_KG INTEGER,\n    FOREIGN_PORT VARCHAR(50),\n    US_PORT VARCHAR(50),\n    VESSEL_NAME VARCHAR(40),\n    COUNTRY_OF_ORIGIN VARCHAR(40),\n    CONSIGNEE_ADDRESS VARCHAR(150),\n    SHIPPER_ADDRESS VARCHAR(150),\n    ZIPCODE VARCHAR(20),\n    NO_OF_CONTAINERS INTEGER,\n    CONTAINER_NUMBER VARCHAR(200),\n    CONTAINER_TYPE VARCHAR(80),\n    QUANTITY INTEGER,\n    QUANTITY_UNIT VARCHAR(10),\n    MEASUREMENT INTEGER,\n    MEASUREMENT_UNIT VARCHAR(5),\n    BILL_OF_LADING VARCHAR(20),\n    HOUSE_VS_MASTER CHAR(1),\n    DISTRIBUTION_PORT VARCHAR(40),\n    MASTER_BL VARCHAR(20),\n    VOYAGE_NUMBER VARCHAR(10),\n    SEAL VARCHAR(300),\n    SHIP_REGISTERED_IN VARCHAR(40),\n    INBOND_ENTRY_TYPE VARCHAR(30),\n    CARRIER_CODE VARCHAR(10),\n    CARRIER_NAME VARCHAR(40),\n    CARRIER_CITY VARCHAR(40),\n    CARRIER_STATE VARCHAR(10),\n    CARRIER_ZIP VARCHAR(10),\n    CARRIER_ADDRESS VARCHAR(200),\n    NOTIFY_PARTY VARCHAR(50),\n    NOTIFY_ADDRESS VARCHAR(200),\n    PLACE_OF_RECEIPT VARCHAR(50),\n    DATE_OF_RECEIPT TIMESTAMP\n);\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 12:20:40.000",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 4.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542399640559_-681627794",
      "id": "20180202-102130_85303245",
      "dateCreated": "2018-11-16 12:20:40.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Import the Data\n\nNext we import the shipping data, which is in csv format, into our Splice Machine database.\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 12:20:40.000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eImport the Data\u003c/h3\u003e\n\u003cp\u003eNext we import the shipping data, which is in csv format, into our Splice Machine database.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542399640560_-57303257",
      "id": "20180202-104354_1962297047",
      "dateCreated": "2018-11-16 12:20:40.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\ncall SYSCS_UTIL.IMPORT_DATA (\n     \u0027DS_ASN\u0027,\n     \u0027SHIPMENT_IN_TRANSIT\u0027,\n     null,\n     \u0027s3a://splice-demo/shipment/shipment_in_transit.csv\u0027,\n     \u0027|\u0027,\n     null,\n     \u0027yyyy-MM-dd HH:mm:ss.SSSSSS\u0027,\n     \u0027yyyy-MM-dd\u0027,\n     null,\n     -1,\n     \u0027/tmp\u0027,\n     true, null);",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 07:00:09.983",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "rowsImported": "string",
                      "failedRows": "string",
                      "files": "string",
                      "dataSize": "string",
                      "failedLog": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542399640560_984478619",
      "id": "20180202-104501_1686524282",
      "dateCreated": "2018-11-16 12:20:40.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\ncall SYSCS_UTIL.IMPORT_DATA (\n     \u0027DS_ASN\u0027,\n     \u0027SHIPMENT_HISTORY\u0027,\n     null,\n     \u0027s3a://splice-demo/shipment/shipment_history.csv\u0027,\n     \u0027|\u0027,\n     null,\n     \u0027yyyy-MM-dd HH:mm:ss.SSSSSS\u0027,\n     \u0027yyyy-MM-dd\u0027,\n     null,\n     -1,\n     \u0027/tmp\u0027,\n     true, null);",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 07:00:24.428",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "rowsImported": "string",
                      "failedRows": "string",
                      "files": "string",
                      "dataSize": "string",
                      "failedLog": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            },
            "helium": {}
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542399640561_-98280948",
      "id": "20180202-104531_242494964",
      "dateCreated": "2018-11-16 12:20:40.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Create our Features Table\n\nWe create a features table in our database that we use with our learning model. We add three computed fields in the `features` table that are important to our model:\n\n* `quantity_bin` categorizes shipping quantities into bins, to improve learning accuracy \n* `lateness` computes how many days late a shipment was\n* `label` categorizes lateness into one of four values:\n\n\u003ctable class\u003d\"spliceZepNoBorder\" style\u003d\"margin: 0 0 100px 50px;\"\u003e\n    \u003ctbody\u003e\n            \u003ctr\u003e\u003ctd\u003e0\u003c/td\u003e\u003ctd\u003e0 days late\u003c/td\u003e\u003c/tr\u003e\n            \u003ctr\u003e\u003ctd\u003e1\u003c/td\u003e\u003ctd\u003e1-5 days late\u003c/td\u003e\u003c/tr\u003e\n            \u003ctr\u003e\u003ctd\u003e2\u003c/td\u003e\u003ctd\u003e5-10 days late\u003c/td\u003e\u003c/tr\u003e\n            \u003ctr\u003e\u003ctd\u003e3\u003c/td\u003e\u003ctd\u003e10 days or more late\u003c/td\u003e\u003c/tr\u003e\n    \u003c/tbody\u003e\n\u003c/table\u003e",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 07:00:31.227",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eCreate our Features Table\u003c/h3\u003e\n\u003cp\u003eWe create a features table in our database that we use with our learning model. We add three computed fields in the \u003ccode\u003efeatures\u003c/code\u003e table that are important to our model:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\u003ccode\u003equantity_bin\u003c/code\u003e categorizes shipping quantities into bins, to improve learning accuracy\u003c/li\u003e\n  \u003cli\u003e\u003ccode\u003elateness\u003c/code\u003e computes how many days late a shipment was\u003c/li\u003e\n  \u003cli\u003e\u003ccode\u003elabel\u003c/code\u003e categorizes lateness into one of four values:\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable class\u003d\"spliceZepNoBorder\" style\u003d\"margin: 0 0 100px 50px;\"\u003e\n    \u003ctbody\u003e\n            \u003ctr\u003e\u003ctd\u003e0\u003c/td\u003e\u003ctd\u003e0 days late\u003c/td\u003e\u003c/tr\u003e\n            \u003ctr\u003e\u003ctd\u003e1\u003c/td\u003e\u003ctd\u003e1-5 days late\u003c/td\u003e\u003c/tr\u003e\n            \u003ctr\u003e\u003ctd\u003e2\u003c/td\u003e\u003ctd\u003e5-10 days late\u003c/td\u003e\u003c/tr\u003e\n            \u003ctr\u003e\u003ctd\u003e3\u003c/td\u003e\u003ctd\u003e10 days or more late\u003c/td\u003e\u003c/tr\u003e\n    \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542399640561_-475162322",
      "id": "20180202-104618_659628734",
      "dateCreated": "2018-11-16 12:20:40.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\ndrop table IF EXISTS DS_ASN.FEATURES;\nCREATE table DS_ASN.FEATURES AS\n    SELECT\n        SHIPMENTID,\n        SHIPMODE,\n        CONSIGNEE,\n        SHIPPER,\n        ARRIVAL_DATE,\n        GROSS_WEIGHT_LB,\n        FOREIGN_PORT,\n        US_PORT,\n        VESSEL_NAME,\n        COUNTRY_OF_ORIGIN,\n        CONSIGNEE_ADDRESS,\n        SHIPPER_ADDRESS,\n        ZIPCODE,\n        NO_OF_CONTAINERS,\n        CONTAINER_NUMBER,\n        CONTAINER_TYPE,\n        QUANTITY,\n        QUANTITY_UNIT,\n        MEASUREMENT,\n        MEASUREMENT_UNIT,\n        BILL_OF_LADING,\n        HOUSE_VS_MASTER,\n        DISTRIBUTION_PORT,\n        MASTER_BL,\n        VOYAGE_NUMBER,\n        SEAL,\n        SHIP_REGISTERED_IN,\n        INBOND_ENTRY_TYPE,\n        CARRIER_CODE,\n        CARRIER_NAME,\n        CARRIER_CITY,\n        CARRIER_STATE,\n        CARRIER_ZIP,\n        CARRIER_ADDRESS,\n        NOTIFY_PARTY,\n        NOTIFY_ADDRESS,\n        PLACE_OF_RECEIPT,\n        DATE_OF_RECEIPT,\n        CASE\n        WHEN DS_ASN.SHIPMENT_HISTORY.QUANTITY \u003e 10\n        THEN\n            CASE\n                WHEN DS_ASN.SHIPMENT_HISTORY.QUANTITY \u003e 100\n                THEN\n                    CASE\n                        WHEN DS_ASN.SHIPMENT_HISTORY.QUANTITY \u003e 1000\n                        THEN 3\n                        ELSE 2\n                    END\n                ELSE 1\n    \tEND\n        ELSE 0\n        END AS QUANTITY_BIN,\n        DS_ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - DS_ASN.SHIPMENT_HISTORY.ARRIVAL_DATE as LATENESS,\n        CASE\n        WHEN   DS_ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - DS_ASN.SHIPMENT_HISTORY.ARRIVAL_DATE \u003e 0\n        THEN\n            CASE\n                WHEN   DS_ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - DS_ASN.SHIPMENT_HISTORY.ARRIVAL_DATE \u003e 5\n                THEN\n                    CASE\n                        WHEN   DS_ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - DS_ASN.SHIPMENT_HISTORY.ARRIVAL_DATE \u003e 10\n                        THEN 3\n                        ELSE 2\n                    END\n                ELSE 1\n    \tEND\n        ELSE 0\n        END AS LABEL\n    FROM DS_ASN.SHIPMENT_HISTORY",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 07:01:11.601",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542399640562_-1912167201",
      "id": "20180202-104614_1527675689",
      "dateCreated": "2018-11-16 12:20:40.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\ndrop table IF EXISTS DS_ASN.FEATURES;\nCREATE table DS_ASN.FEATURES AS\n    SELECT \n    SHIPMENTID,\n    STATUS,\n    SHIPMODE,\n    PRODUCT_DESCRIPTION,\n    CONSIGNEE,\n    SHIPPER,\n    ARRIVAL_DATE,\n    GROSS_WEIGHT_LB,\n    GROSS_WEIGHT_KG,\n    FOREIGN_PORT,\n    US_PORT,\n    VESSEL_NAME,\n    COUNTRY_OF_ORIGIN,\n    CONSIGNEE_ADDRESS,\n    SHIPPER_ADDRESS,\n    ZIPCODE,\n    NO_OF_CONTAINERS,\n    CONTAINER_NUMBER,\n    CONTAINER_TYPE,\n    QUANTITY,\n    QUANTITY_UNIT,\n    MEASUREMENT,\n    MEASUREMENT_UNIT,\n    BILL_OF_LADING,\n    HOUSE_VS_MASTER,\n    DISTRIBUTION_PORT,\n    MASTER_BL,\n    VOYAGE_NUMBER,\n    SEAL,\n    SHIP_REGISTERED_IN,\n    INBOND_ENTRY_TYPE,\n    CARRIER_CODE,\n    CARRIER_NAME,\n    CARRIER_CITY,\n    CARRIER_STATE,\n    CARRIER_ZIP,\n    CARRIER_ADDRESS,\n    NOTIFY_PARTY,\n    NOTIFY_ADDRESS,\n    PLACE_OF_RECEIPT,\n    DATE_OF_RECEIPT,\n    CASE\n    WHEN DS_ASN.SHIPMENT_HISTORY.QUANTITY \u003e 10\n    THEN\n        CASE\n            WHEN DS_ASN.SHIPMENT_HISTORY.QUANTITY \u003e 100\n            THEN\n                CASE\n                    WHEN DS_ASN.SHIPMENT_HISTORY.QUANTITY \u003e 1000\n                    THEN 3\n                    ELSE 2\n                END\n            ELSE 1\n    END\n    ELSE 0\n    END AS QUANTITY_BIN,\n    DS_ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - DS_ASN.SHIPMENT_HISTORY.ARRIVAL_DATE as LATENESS,\n    CASE\n    WHEN  DS_ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - DS_ASN.SHIPMENT_HISTORY.ARRIVAL_DATE \u003e 0\n    THEN\n        CASE\n            WHEN  DS_ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - DS_ASN.SHIPMENT_HISTORY.ARRIVAL_DATE \u003e 5\n            THEN\n                CASE\n                    WHEN  DS_ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - DS_ASN.SHIPMENT_HISTORY.ARRIVAL_DATE \u003e 10\n                    THEN 3\n                    ELSE 2\n                END\n            ELSE 1\n    END\n    ELSE 0\n    END AS LABEL\nFROM DS_ASN.SHIPMENT_HISTORY ",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 07:01:49.804",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542399640562_-955240672",
      "id": "20180205-022847_1435938411",
      "dateCreated": "2018-11-16 12:20:40.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Create, Train, and Deploy our Learning Model\n\nThe remainder of this notebook walks you through the code we use to create, train, and deploy our learning model, in these steps:\n\n1. *Perform Spark+MLlib Setup Tasks*\n2. *Create our DataFrame*\n3. *Create Pipeline Stages*\n4. *Assemble the Pipeline\u003eTrain our Model*\n5. *Deploy our Model*\n\nWe include the entire program at the end of this notebook.",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 07:01:51.881",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eCreate, Train, and Deploy our Learning Model\u003c/h2\u003e\n\u003cp\u003eThe remainder of this notebook walks you through the code we use to create, train, and deploy our learning model, in these steps:\u003c/p\u003e\n\u003col\u003e\n  \u003cli\u003e\u003cem\u003ePerform Spark+MLlib Setup Tasks\u003c/em\u003e\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eCreate our DataFrame\u003c/em\u003e\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eCreate Pipeline Stages\u003c/em\u003e\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eAssemble the Pipeline\u0026gt;Train our Model\u003c/em\u003e\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eDeploy our Model\u003c/em\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eWe include the entire program at the end of this notebook.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542399640563_-659613398",
      "id": "20180131-172852_644197695",
      "dateCreated": "2018-11-16 12:20:40.000",
      "dateStarted": "2018-11-30 21:52:22.586",
      "dateFinished": "2018-11-30 21:52:22.595",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 1. Perform Spark+MLlib Setup Tasks\n\nFirst we set up our resources and initialize the Splice Machine Spark Adapter (`spliceMachineContext`). \n\n```\nimport org.apache.spark.ml.feature.VectorAssembler\nimport java.sql.{Connection,Timestamp}\nimport com.splicemachine.spark.splicemachine._\nimport org.apache.spark.sql.execution.datasources.jdbc.{JDBCOptions, JdbcUtils}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature.StringIndexer\nimport spark.implicits._\n\nval splicemachineContext \u003d new SplicemachineContext(defaultJDBCURL)\n```",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 07:01:56.200",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003e1. Perform Spark+MLlib Setup Tasks\u003c/h3\u003e\n\u003cp\u003eFirst we set up our resources and initialize the Splice Machine Spark Adapter (\u003ccode\u003espliceMachineContext\u003c/code\u003e). \u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport org.apache.spark.ml.feature.VectorAssembler\nimport java.sql.{Connection,Timestamp}\nimport com.splicemachine.spark.splicemachine._\nimport org.apache.spark.sql.execution.datasources.jdbc.{JDBCOptions, JdbcUtils}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature.StringIndexer\nimport spark.implicits._\n\nval splicemachineContext \u003d new SplicemachineContext(defaultJDBCURL)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542399640563_1014208095",
      "id": "20180130-185957_1517048437",
      "dateCreated": "2018-11-16 12:20:40.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n#### Initialize our Context\n\nNow we initialize our context:\n\n```\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\n\nsplice \u003d PySpliceContext(defaultJDBCURL, sqlContext)\n```",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 07:01:59.630",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch4\u003eInitialize our Context\u003c/h4\u003e\n\u003cp\u003eNow we initialize our context:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\n\nsplice \u003d PySpliceContext(defaultJDBCURL, sqlContext)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542399640564_-568881306",
      "id": "20180611-203526_1071983380",
      "dateCreated": "2018-11-16 12:20:40.000",
      "dateStarted": "2018-11-20 17:25:16.212",
      "dateFinished": "2018-11-20 17:25:16.216",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 2. Create our DataFrame: Scala\n\nWe need to pull the schema from our shipping database, `DS_ASN.Features`, and convert it into a Spark DataFrame. We then create a sequential list (a Scala `seq` object) of the features (fields) from that table that we want to include in our model, and concatenate that onto our DataFrame. `MLlib` expects the schema to contain uppercase field names, so we convert our sequence to uppercase with a built-in function. \n\n```\nval df_with_uppercase_schema \u003d splicemachineContext.df(\"select * from ASN.Features\")\nval newNames \u003d Seq(\n    \"consignee\",\n    \"shipper\",\n    \"shipmode\",\n    \"gross_weight_lb\",\n    \"foreign_port\",\n    \"us_port\",\n    \"vessel_name\",\n    \"country_of_origin\",\n    \"container_number\",\n    \"container_type\",\n    \"quantity\",\n    \"ship_registered_in\",\n    \"carrier_code\",\n    \"carrier_city\",\n    \"notify_party\",\n    \"place_of_receipt\",\n    \"zipcode\",\n    \"quantity_bin\"\n    )\nval df \u003d df_with_uppercase_schema.toDF(newNames: _*)\n```",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 07:02:22.525",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003e2. Create our DataFrame: Scala\u003c/h3\u003e\n\u003cp\u003eWe need to pull the schema from our shipping database, \u003ccode\u003eDS_ASN.Features\u003c/code\u003e, and convert it into a Spark DataFrame. We then create a sequential list (a Scala \u003ccode\u003eseq\u003c/code\u003e object) of the features (fields) from that table that we want to include in our model, and concatenate that onto our DataFrame. \u003ccode\u003eMLlib\u003c/code\u003e expects the schema to contain uppercase field names, so we convert our sequence to uppercase with a built-in function. \u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval df_with_uppercase_schema \u003d splicemachineContext.df(\u0026quot;select * from ASN.Features\u0026quot;)\nval newNames \u003d Seq(\n    \u0026quot;consignee\u0026quot;,\n    \u0026quot;shipper\u0026quot;,\n    \u0026quot;shipmode\u0026quot;,\n    \u0026quot;gross_weight_lb\u0026quot;,\n    \u0026quot;foreign_port\u0026quot;,\n    \u0026quot;us_port\u0026quot;,\n    \u0026quot;vessel_name\u0026quot;,\n    \u0026quot;country_of_origin\u0026quot;,\n    \u0026quot;container_number\u0026quot;,\n    \u0026quot;container_type\u0026quot;,\n    \u0026quot;quantity\u0026quot;,\n    \u0026quot;ship_registered_in\u0026quot;,\n    \u0026quot;carrier_code\u0026quot;,\n    \u0026quot;carrier_city\u0026quot;,\n    \u0026quot;notify_party\u0026quot;,\n    \u0026quot;place_of_receipt\u0026quot;,\n    \u0026quot;zipcode\u0026quot;,\n    \u0026quot;quantity_bin\u0026quot;\n    )\nval df \u003d df_with_uppercase_schema.toDF(newNames: _*)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542399640564_-1104025187",
      "id": "20180130-190437_334148512",
      "dateCreated": "2018-11-16 12:20:40.000",
      "dateStarted": "2018-12-01 07:02:12.752",
      "dateFinished": "2018-12-01 07:02:12.760",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 3. Create Pipeline Stages\n\nOur pipeline stages are fairly simple:\n\n* Transform each row of data in the input dataset into an integer vector.\n* Assemble the vectors into a DataFrame\n* Use a Logistic Regression Estimator to create our model\n\n#### Transform each row of data into an integer vector\n\nThe Logistic Regression estimator operates on integer vectors, so we need to convert each row in our input dataframe into an integer vector. Remember that each row contains only the fields from our database that are of interest to our model: the fields that previously included in our sequence and concatenated onto our DataFrame.\n\nSpark includes a `StringIndexer` function that does exactly that, so we create a `StringIndexer` for each field, and we\u0027ll later use each of these as a stage in our learning pipeline. The `StringIndexer` transforms the data from a specified input column in our DataFrame and stores the output in a specified and new output column. By convention, we name each string indexer with the name of the field+`Indexer,` and name the output column the name of the field+`Index,` e.g. we create a transformer named `consigneeIndexer` to transform the input column `consignee` into the new output column `consigneeIndex.`\n\n```\n// Transform strings into numbers\nval consigneeIndexer \u003d new StringIndexer().setInputCol(\"consignee\").setOutputCol(\"consigneeIndex\").setHandleInvalid(\"skip\") \nval shipperIndexer \u003d new StringIndexer().setInputCol(\"shipper\").setOutputCol(\"shipperIndex\").setHandleInvalid(\"skip\")\nval shipmodeIndexer \u003d new StringIndexer().setInputCol(\"shipmode\").setOutputCol(\"shipmodeIndex\").setHandleInvalid(\"skip\") \nval gross_weight_lbIndexer \u003d new StringIndexer().setInputCol(\"gross_weight_lb\").setOutputCol(\"gross_weight_lbIndex\").setHandleInvalid(\"skip\") \nval foreign_portIndexer \u003d new StringIndexer().setInputCol(\"foreign_port\").setOutputCol(\"foreign_portIndex\").setHandleInvalid(\"skip\") \nval us_portIndexer \u003d new StringIndexer().setInputCol(\"us_port\").setOutputCol(\"us_portIndex\").setHandleInvalid(\"skip\") \nval vessel_nameIndexer \u003d new StringIndexer().setInputCol(\"vessel_name\").setOutputCol(\"vessel_nameIndex\").setHandleInvalid(\"skip\") \nval country_of_originIndexer \u003d new StringIndexer().setInputCol(\"country_of_origin\").setOutputCol(\"country_of_originIndex\").setHandleInvalid(\"skip\") \nval container_numberIndexer \u003d new StringIndexer().setInputCol(\"container_number\").setOutputCol(\"container_numberIndex\").setHandleInvalid(\"skip\")\nval container_typeIndexer \u003d new StringIndexer().setInputCol(\"container_type\").setOutputCol(\"container_typeIndex\").setHandleInvalid(\"skip\") \nval ship_registered_inIndexer \u003d new StringIndexer().setInputCol(\"ship_registered_in\").setOutputCol(\"ship_registered_inIndex\").setHandleInvalid(\"skip\") \nval carrier_codeIndexer \u003d new StringIndexer().setInputCol(\"carrier_code\").setOutputCol(\"carrier_codeIndex\").setHandleInvalid(\"skip\") \nval carrier_cityIndexer \u003d new StringIndexer().setInputCol(\"carrier_city\").setOutputCol(\"carrier_cityIndex\").setHandleInvalid(\"skip\") \nval notify_partyIndexer \u003d new StringIndexer().setInputCol(\"notify_party\").setOutputCol(\"notify_partyIndex\").setHandleInvalid(\"skip\") \nval place_of_receiptIndexer \u003d new StringIndexer().setInputCol(\"place_of_receipt\").setOutputCol(\"place_of_receiptIndex\").setHandleInvalid(\"skip\")\nval zipcodeIndexer \u003d new StringIndexer().setInputCol(\"zipcode\").setOutputCol(\"zipcodeIndex\").setHandleInvalid(\"skip\")\n```\n\n#### Assemble the Vectors\n\nAfter our pipeline has transformed data into numbers, we need to assemble those into vectors. Spark includes a `VectorAssembler` object that does just that, transforming a set of input columns into a vector that is stored in the `features` column in the DataFrame:\n\n```\n//assemble raw features\nval assembler \u003d new VectorAssembler()\n                .setInputCols(Array(\n                    \"shipmodeIndex\",\n                    \"consigneeIndex\",\n                    \"shipperIndex\",\n                    \"gross_weight_lbIndex\",\n                    \"foreign_portIndex\",\n                    \"us_portIndex\",\n                    \"vessel_nameIndex\",\n                    \"country_of_originIndex\",\n                    \"container_numberIndex\",\n                    \"container_typeIndex\",\n                    \"quantity_bin\",\n                    \"ship_registered_inIndex\",\n                    \"carrier_codeIndex\",\n                    \"carrier_cityIndex\",\n                    \"notify_partyIndex\",\n                    \"place_of_receiptIndex\",\n                    \"zipcodeIndex\",\n                    \"quantity_bin\"\n                    ))\n                .setOutputCol(\"features\")\n```\n\n#### Create the Estimator\n\nCreating the estimator is a simple matter of specifying a few parameters, including which column in the DataFrame is the label, and which column contains the feature set:\n\n```\n//Create ML analytic\nval lr \u003d new LogisticRegression()\n    .setMaxIter(30)\n    .setLabelCol(\"label\")\n    .setFeaturesCol(\"features\")\n    .set\n    RegParam(0.3)\n```",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 07:02:32.361",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003e3. Create Pipeline Stages\u003c/h3\u003e\n\u003cp\u003eOur pipeline stages are fairly simple:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eTransform each row of data in the input dataset into an integer vector.\u003c/li\u003e\n  \u003cli\u003eAssemble the vectors into a DataFrame\u003c/li\u003e\n  \u003cli\u003eUse a Logistic Regression Estimator to create our model\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eTransform each row of data into an integer vector\u003c/h4\u003e\n\u003cp\u003eThe Logistic Regression estimator operates on integer vectors, so we need to convert each row in our input dataframe into an integer vector. Remember that each row contains only the fields from our database that are of interest to our model: the fields that previously included in our sequence and concatenated onto our DataFrame.\u003c/p\u003e\n\u003cp\u003eSpark includes a \u003ccode\u003eStringIndexer\u003c/code\u003e function that does exactly that, so we create a \u003ccode\u003eStringIndexer\u003c/code\u003e for each field, and we\u0026rsquo;ll later use each of these as a stage in our learning pipeline. The \u003ccode\u003eStringIndexer\u003c/code\u003e transforms the data from a specified input column in our DataFrame and stores the output in a specified and new output column. By convention, we name each string indexer with the name of the field+\u003ccode\u003eIndexer,\u003c/code\u003e and name the output column the name of the field+\u003ccode\u003eIndex,\u003c/code\u003e e.g. we create a transformer named \u003ccode\u003econsigneeIndexer\u003c/code\u003e to transform the input column \u003ccode\u003econsignee\u003c/code\u003e into the new output column \u003ccode\u003econsigneeIndex.\u003c/code\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e// Transform strings into numbers\nval consigneeIndexer \u003d new StringIndexer().setInputCol(\u0026quot;consignee\u0026quot;).setOutputCol(\u0026quot;consigneeIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;) \nval shipperIndexer \u003d new StringIndexer().setInputCol(\u0026quot;shipper\u0026quot;).setOutputCol(\u0026quot;shipperIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;)\nval shipmodeIndexer \u003d new StringIndexer().setInputCol(\u0026quot;shipmode\u0026quot;).setOutputCol(\u0026quot;shipmodeIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;) \nval gross_weight_lbIndexer \u003d new StringIndexer().setInputCol(\u0026quot;gross_weight_lb\u0026quot;).setOutputCol(\u0026quot;gross_weight_lbIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;) \nval foreign_portIndexer \u003d new StringIndexer().setInputCol(\u0026quot;foreign_port\u0026quot;).setOutputCol(\u0026quot;foreign_portIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;) \nval us_portIndexer \u003d new StringIndexer().setInputCol(\u0026quot;us_port\u0026quot;).setOutputCol(\u0026quot;us_portIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;) \nval vessel_nameIndexer \u003d new StringIndexer().setInputCol(\u0026quot;vessel_name\u0026quot;).setOutputCol(\u0026quot;vessel_nameIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;) \nval country_of_originIndexer \u003d new StringIndexer().setInputCol(\u0026quot;country_of_origin\u0026quot;).setOutputCol(\u0026quot;country_of_originIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;) \nval container_numberIndexer \u003d new StringIndexer().setInputCol(\u0026quot;container_number\u0026quot;).setOutputCol(\u0026quot;container_numberIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;)\nval container_typeIndexer \u003d new StringIndexer().setInputCol(\u0026quot;container_type\u0026quot;).setOutputCol(\u0026quot;container_typeIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;) \nval ship_registered_inIndexer \u003d new StringIndexer().setInputCol(\u0026quot;ship_registered_in\u0026quot;).setOutputCol(\u0026quot;ship_registered_inIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;) \nval carrier_codeIndexer \u003d new StringIndexer().setInputCol(\u0026quot;carrier_code\u0026quot;).setOutputCol(\u0026quot;carrier_codeIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;) \nval carrier_cityIndexer \u003d new StringIndexer().setInputCol(\u0026quot;carrier_city\u0026quot;).setOutputCol(\u0026quot;carrier_cityIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;) \nval notify_partyIndexer \u003d new StringIndexer().setInputCol(\u0026quot;notify_party\u0026quot;).setOutputCol(\u0026quot;notify_partyIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;) \nval place_of_receiptIndexer \u003d new StringIndexer().setInputCol(\u0026quot;place_of_receipt\u0026quot;).setOutputCol(\u0026quot;place_of_receiptIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;)\nval zipcodeIndexer \u003d new StringIndexer().setInputCol(\u0026quot;zipcode\u0026quot;).setOutputCol(\u0026quot;zipcodeIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eAssemble the Vectors\u003c/h4\u003e\n\u003cp\u003eAfter our pipeline has transformed data into numbers, we need to assemble those into vectors. Spark includes a \u003ccode\u003eVectorAssembler\u003c/code\u003e object that does just that, transforming a set of input columns into a vector that is stored in the \u003ccode\u003efeatures\u003c/code\u003e column in the DataFrame:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e//assemble raw features\nval assembler \u003d new VectorAssembler()\n                .setInputCols(Array(\n                    \u0026quot;shipmodeIndex\u0026quot;,\n                    \u0026quot;consigneeIndex\u0026quot;,\n                    \u0026quot;shipperIndex\u0026quot;,\n                    \u0026quot;gross_weight_lbIndex\u0026quot;,\n                    \u0026quot;foreign_portIndex\u0026quot;,\n                    \u0026quot;us_portIndex\u0026quot;,\n                    \u0026quot;vessel_nameIndex\u0026quot;,\n                    \u0026quot;country_of_originIndex\u0026quot;,\n                    \u0026quot;container_numberIndex\u0026quot;,\n                    \u0026quot;container_typeIndex\u0026quot;,\n                    \u0026quot;quantity_bin\u0026quot;,\n                    \u0026quot;ship_registered_inIndex\u0026quot;,\n                    \u0026quot;carrier_codeIndex\u0026quot;,\n                    \u0026quot;carrier_cityIndex\u0026quot;,\n                    \u0026quot;notify_partyIndex\u0026quot;,\n                    \u0026quot;place_of_receiptIndex\u0026quot;,\n                    \u0026quot;zipcodeIndex\u0026quot;,\n                    \u0026quot;quantity_bin\u0026quot;\n                    ))\n                .setOutputCol(\u0026quot;features\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eCreate the Estimator\u003c/h4\u003e\n\u003cp\u003eCreating the estimator is a simple matter of specifying a few parameters, including which column in the DataFrame is the label, and which column contains the feature set:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e//Create ML analytic\nval lr \u003d new LogisticRegression()\n    .setMaxIter(30)\n    .setLabelCol(\u0026quot;label\u0026quot;)\n    .setFeaturesCol(\u0026quot;features\u0026quot;)\n    .set\n    RegParam(0.3)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542399640565_-1218541373",
      "id": "20180130-190241_1891417263",
      "dateCreated": "2018-11-16 12:20:40.000",
      "dateStarted": "2018-11-30 21:52:54.621",
      "dateFinished": "2018-11-30 21:52:54.653",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 4. Assemble our Pipeline\n\nNow that we\u0027ve got our stages set up, we\u0027re ready to assemble our Machine Learning pipeline, which chains together those stages in sequence:\n\n```\n// Chain indexers and tree in a Pipeline\nval lrPipeline \u003d new Pipeline().setStages(\n        Array(consigneeIndexer,\n                shipperIndexer,\n                shipmodeIndexer,\n                gross_weight_lbIndexer,\n                foreign_portIndexer,\n                us_portIndexer,\n                vessel_nameIndexer,\n                country_of_originIndexer,\n                container_numberIndexer,\n                container_typeIndexer,\n                ship_registered_inIndexer,\n                carrier_codeIndexer,\n                carrier_cityIndexer,\n                notify_partyIndexer,\n                place_of_receiptIndexer,\n                zipcodeIndexer,\n                assembler,\n                lr\n                ))\n```",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 07:02:49.213",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003e4. Assemble our Pipeline\u003c/h3\u003e\n\u003cp\u003eNow that we\u0026rsquo;ve got our stages set up, we\u0026rsquo;re ready to assemble our Machine Learning pipeline, which chains together those stages in sequence:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e// Chain indexers and tree in a Pipeline\nval lrPipeline \u003d new Pipeline().setStages(\n        Array(consigneeIndexer,\n                shipperIndexer,\n                shipmodeIndexer,\n                gross_weight_lbIndexer,\n                foreign_portIndexer,\n                us_portIndexer,\n                vessel_nameIndexer,\n                country_of_originIndexer,\n                container_numberIndexer,\n                container_typeIndexer,\n                ship_registered_inIndexer,\n                carrier_codeIndexer,\n                carrier_cityIndexer,\n                notify_partyIndexer,\n                place_of_receiptIndexer,\n                zipcodeIndexer,\n                assembler,\n                lr\n                ))\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542399640566_1120360496",
      "id": "20180130-201715_1588482510",
      "dateCreated": "2018-11-16 12:20:40.000",
      "dateStarted": "2018-11-20 17:26:12.707",
      "dateFinished": "2018-11-20 17:26:12.724",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 5. Train our Model\n\nNow that our pipeline is set up, all we need to do to train our model is feed our dataframe into the pipeline\u0027s `fit` method, which learns from the data. \n```\n// Train model. \nval lrModel \u003d lrPipeline.fit(df)\n```\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 07:02:53.389",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003e5. Train our Model\u003c/h3\u003e\n\u003cp\u003eNow that our pipeline is set up, all we need to do to train our model is feed our dataframe into the pipeline\u0026rsquo;s \u003ccode\u003efit\u003c/code\u003e method, which learns from the data. \u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e// Train model. \nval lrModel \u003d lrPipeline.fit(df)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542399640567_-1080930203",
      "id": "20180130-201949_1288501052",
      "dateCreated": "2018-11-16 12:20:40.000",
      "dateStarted": "2018-11-20 17:26:21.685",
      "dateFinished": "2018-11-20 17:26:21.689",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Using Spark MLlib",
      "text": "%md\n### 6. Materialize the Model\n\nNow that we\u0027ve trained our model, we can apply it to real data and display the results. For simplicity sake, we\u0027ll simply apply the model to our feature table itself.\n\n```\nlrModel.transform(df).select(\"prediction\", \"probability\", \"features\").show(100)\n```",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 07:02:56.070",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "title": false,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003e6. Materialize the Model\u003c/h3\u003e\n\u003cp\u003eNow that we\u0026rsquo;ve trained our model, we can apply it to real data and display the results. For simplicity sake, we\u0026rsquo;ll simply apply the model to our feature table itself.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003elrModel.transform(df).select(\u0026quot;prediction\u0026quot;, \u0026quot;probability\u0026quot;, \u0026quot;features\u0026quot;).show(100)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542399640567_571181625",
      "id": "20180118-020316_1913850778",
      "dateCreated": "2018-11-16 12:20:40.000",
      "dateStarted": "2018-11-20 17:26:33.171",
      "dateFinished": "2018-11-20 17:26:33.180",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\nselect *  from ASN.features { limit 100 }\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 12:20:40.000",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "SHIPMENTID": "string",
                      "STATUS": "string",
                      "SHIPMODE": "string",
                      "PRODUCT_DESCRIPTION": "string",
                      "CONSIGNEE": "string",
                      "SHIPPER": "string",
                      "ARRIVAL_DATE": "string",
                      "GROSS_WEIGHT_LB": "string",
                      "GROSS_WEIGHT_KG": "string",
                      "FOREIGN_PORT": "string",
                      "US_PORT": "string",
                      "VESSEL_NAME": "string",
                      "COUNTRY_OF_ORIGIN": "string",
                      "CONSIGNEE_ADDRESS": "string",
                      "SHIPPER_ADDRESS": "string",
                      "ZIPCODE": "string",
                      "NO_OF_CONTAINERS": "string",
                      "CONTAINER_NUMBER": "string",
                      "CONTAINER_TYPE": "string",
                      "QUANTITY": "string",
                      "QUANTITY_UNIT": "string",
                      "MEASUREMENT": "string",
                      "MEASUREMENT_UNIT": "string",
                      "BILL_OF_LADING": "string",
                      "HOUSE_VS_MASTER": "string",
                      "DISTRIBUTION_PORT": "string",
                      "MASTER_BL": "string",
                      "VOYAGE_NUMBER": "string",
                      "SEAL": "string",
                      "SHIP_REGISTERED_IN": "string",
                      "INBOND_ENTRY_TYPE": "string",
                      "CARRIER_CODE": "string",
                      "CARRIER_NAME": "string",
                      "CARRIER_CITY": "string",
                      "CARRIER_STATE": "string",
                      "CARRIER_ZIP": "string",
                      "CARRIER_ADDRESS": "string",
                      "NOTIFY_PARTY": "string",
                      "NOTIFY_ADDRESS": "string",
                      "PLACE_OF_RECEIPT": "string",
                      "DATE_OF_RECEIPT": "string",
                      "QUANTITY_BIN": "string",
                      "LATENESS": "string",
                      "LABEL": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542399640568_5063598",
      "id": "20180205-024307_1130627818",
      "dateCreated": "2018-11-16 12:20:40.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\nDROP TABLE IF EXISTS ASN.PREDICTIONS;\nCREATE TABLE ASN.PREDICTIONS (\n    SHIPMENTID VARCHAR(11) NOT NULL PRIMARY KEY,\n    PREDICTION DOUBLE\n    );",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 12:20:40.000",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542399640568_1637996786",
      "id": "20180205-024429_478433529",
      "dateCreated": "2018-11-16 12:20:40.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## The Scala Code\n\nThe Scala code is listed here:",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 07:03:07.926",
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eThe Scala Code\u003c/h2\u003e\n\u003cp\u003eThe Scala code is listed here:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542399640568_-248138807",
      "id": "20180611-220634_1810473505",
      "dateCreated": "2018-11-16 12:20:40.000",
      "dateStarted": "2018-11-20 17:27:09.105",
      "dateFinished": "2018-11-20 17:27:09.108",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nimport org.apache.spark.ml.feature.VectorAssembler\nimport java.sql.{Connection}\nimport com.splicemachine.spark.splicemachine._\nimport org.apache.spark.sql.execution.datasources.jdbc.{JDBCOptions, JdbcUtils}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature.StringIndexer\nimport spark.implicits._\n\n\nval optionMap \u003d Map(\n  JDBCOptions.JDBC_TABLE_NAME -\u003e \"DS_ASN.Features\",\n  JDBCOptions.JDBC_URL -\u003e defaultJDBCURL\n)\nval splicemachineContext \u003d new SplicemachineContext(defaultJDBCURL)\nval df_with_uppercase_schema \u003d splicemachineContext.df(\"select * from DS_ASN.Features\")\nval newNames \u003d Seq(\n    \"shipmentid\",\n    \"status\",\n    \"shipmode\",\n    \"product_description\",\n    \"consignee\",\n    \"shipper\",\n    \"arrival_date\",\n    \"gross_weight_lb\",\n    \"gross_weight_kg\",\n    \"foreign_port\",\n    \"us_port\",\n    \"vessel_name\",\n    \"country_of_origin\",\n    \"consignee_address\",\n    \"shipper_address\",\n    \"zipcode\",\n    \"no_of_containers\",\n    \"container_number\",\n    \"container_type\",\n    \"quantity\",\n    \"quantity_unit\",\n    \"measurement\",\n    \"measurement_unit\",\n    \"bill_of_lading\",\n    \"house_vs_master\",\n    \"distribution_port\",\n    \"master_bl\",\n    \"voyage_number\",\n    \"seal\",\n    \"ship_registered_in\",\n    \"inbond_entry_type\",\n    \"carrier_code\",\n    \"carrier_name\",\n    \"carrier_city\",\n    \"carrier_state\",\n    \"carrier_zip\",\n    \"carrier_address\",\n    \"notify_party\",\n    \"notify_address\",\n    \"place_of_receipt\",\n    \"date_of_receipt\",\n    \"quantity_bin\",\n    \"lateness\",\n    \"label\"\n)\nval df \u003d df_with_uppercase_schema.toDF(newNames: _*)\n\n//assemble raw features\nval assembler \u003d new VectorAssembler().\n                setInputCols(Array(\n                    \"consigneeIndex\",\n                    \"shipperIndex\",\n                    \"gross_weight_lbIndex\",\n                    \"foreign_portIndex\",\n                    \"us_portIndex\",\n                    \"vessel_nameIndex\",\n                    \"country_of_originIndex\",\n                    \"container_numberIndex\",\n                    \"container_typeIndex\",\n                    \"quantity_bin\",\n                    \"ship_registered_inIndex\",\n                    \"carrier_codeIndex\",\n                    \"carrier_cityIndex\",\n                    \"notify_partyIndex\",\n                    \"place_of_receiptIndex\",\n                    \"zipcodeIndex\"\n                    )).\n                setOutputCol(\"features\")\n\n// Transform strings into numbers\nval zipcodeIndexer \u003d new StringIndexer().setInputCol(\"zipcode\").setOutputCol(\"zipcodeIndex\").setHandleInvalid(\"skip\")\nval consigneeIndexer \u003d new StringIndexer().setInputCol(\"consignee\").setOutputCol(\"consigneeIndex\").setHandleInvalid(\"skip\") \nval shipperIndexer \u003d new StringIndexer().setInputCol(\"shipper\").setOutputCol(\"shipperIndex\").setHandleInvalid(\"skip\")\nval statusIndexer \u003d new StringIndexer().setInputCol(\"status\").setOutputCol(\"statusIndex\").setHandleInvalid(\"skip\") \nval shipmodeIndexer \u003d new StringIndexer().setInputCol(\"shipmode\").setOutputCol(\"shipmodeIndex\").setHandleInvalid(\"skip\") \nval gross_weight_lbIndexer \u003d new StringIndexer().setInputCol(\"gross_weight_lb\").setOutputCol(\"gross_weight_lbIndex\").setHandleInvalid(\"skip\") \nval foreign_portIndexer \u003d new StringIndexer().setInputCol(\"foreign_port\").setOutputCol(\"foreign_portIndex\").setHandleInvalid(\"skip\") \nval us_portIndexer \u003d new StringIndexer().setInputCol(\"us_port\").setOutputCol(\"us_portIndex\").setHandleInvalid(\"skip\") \nval vessel_nameIndexer \u003d new StringIndexer().setInputCol(\"vessel_name\").setOutputCol(\"vessel_nameIndex\").setHandleInvalid(\"skip\") \nval country_of_originIndexer \u003d new StringIndexer().setInputCol(\"country_of_origin\").setOutputCol(\"country_of_originIndex\").setHandleInvalid(\"skip\") \nval container_numberIndexer \u003d new StringIndexer().setInputCol(\"container_number\").setOutputCol(\"container_numberIndex\").setHandleInvalid(\"skip\")\nval container_typeIndexer \u003d new StringIndexer().setInputCol(\"container_type\").setOutputCol(\"container_typeIndex\").setHandleInvalid(\"skip\") \nval distribution_portIndexer \u003d new StringIndexer().setInputCol(\"distribution_port\").setOutputCol(\"distribution_portIndex\").setHandleInvalid(\"skip\") \nval ship_registered_inIndexer \u003d new StringIndexer().setInputCol(\"ship_registered_in\").setOutputCol(\"ship_registered_inIndex\").setHandleInvalid(\"skip\") \nval inbond_entry_typeIndexer \u003d new StringIndexer().setInputCol(\"inbond_entry_type\").setOutputCol(\"inbond_entry_typeIndex\").setHandleInvalid(\"skip\") \nval carrier_codeIndexer \u003d new StringIndexer().setInputCol(\"carrier_code\").setOutputCol(\"carrier_codeIndex\").setHandleInvalid(\"skip\") \nval carrier_cityIndexer \u003d new StringIndexer().setInputCol(\"carrier_city\").setOutputCol(\"carrier_cityIndex\").setHandleInvalid(\"skip\") \nval carrier_stateIndexer \u003d new StringIndexer().setInputCol(\"carrier_state\").setOutputCol(\"carrier_stateIndex\").setHandleInvalid(\"skip\") \nval carrier_zipIndexer \u003d new StringIndexer().setInputCol(\"carrier_zip\").setOutputCol(\"carrier_zipIndex\").setHandleInvalid(\"skip\") \nval notify_partyIndexer \u003d new StringIndexer().setInputCol(\"notify_party\").setOutputCol(\"notify_partyIndex\").setHandleInvalid(\"skip\") \nval place_of_receiptIndexer \u003d new StringIndexer().setInputCol(\"place_of_receipt\").setOutputCol(\"place_of_receiptIndex\").setHandleInvalid(\"skip\") \n\n//Create ML analytic\nval lr \u003d new LogisticRegression().\n    setMaxIter(30).\n    setLabelCol(\"label\").\n    setFeaturesCol(\"features\").\n    setRegParam(0.3)\n\n\n// Chain indexers and tree in a Pipeline\nval lrPipeline \u003d new Pipeline().setStages(\n        Array(consigneeIndexer,\n                shipperIndexer,\n                shipmodeIndexer,\n                gross_weight_lbIndexer,\n                foreign_portIndexer,\n                us_portIndexer,\n                vessel_nameIndexer,\n                country_of_originIndexer,\n                container_numberIndexer,\n                container_typeIndexer,\n                ship_registered_inIndexer,\n                carrier_codeIndexer,\n                carrier_cityIndexer,\n                notify_partyIndexer,\n                place_of_receiptIndexer,\n                zipcodeIndexer,\n                assembler,\n                lr\n                ))\n\n// Train model. \nval lrModel \u003d lrPipeline.fit(df)\n\nlrModel.transform(df).select(\"prediction\", \"probability\", \"features\").show(100)\n\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 07:03:52.845",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542399640569_-349309217",
      "id": "20180130-201612_556450692",
      "dateCreated": "2018-11-16 12:20:40.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\ndrop table IF EXISTS DS_ASN.TEST_FEATURES;\nCREATE table DS_ASN.TEST_FEATURES AS\n    SELECT \n    SHIPMENTID,\n    STATUS,\n    SHIPMODE,\n    PRODUCT_DESCRIPTION,\n    CONSIGNEE,\n    SHIPPER,\n    ARRIVAL_DATE,\n    GROSS_WEIGHT_LB,\n    GROSS_WEIGHT_KG,\n    FOREIGN_PORT,\n    US_PORT,\n    VESSEL_NAME,\n    COUNTRY_OF_ORIGIN,\n    CONSIGNEE_ADDRESS,\n    SHIPPER_ADDRESS,\n    ZIPCODE,\n    NO_OF_CONTAINERS,\n    CONTAINER_NUMBER,\n    CONTAINER_TYPE,\n    QUANTITY,\n    QUANTITY_UNIT,\n    MEASUREMENT,\n    MEASUREMENT_UNIT,\n    BILL_OF_LADING,\n    HOUSE_VS_MASTER,\n    DISTRIBUTION_PORT,\n    MASTER_BL,\n    VOYAGE_NUMBER,\n    SEAL,\n    SHIP_REGISTERED_IN,\n    INBOND_ENTRY_TYPE,\n    CARRIER_CODE,\n    CARRIER_NAME,\n    CARRIER_CITY,\n    CARRIER_STATE,\n    CARRIER_ZIP,\n    CARRIER_ADDRESS,\n    NOTIFY_PARTY,\n    NOTIFY_ADDRESS,\n    PLACE_OF_RECEIPT,\n    DATE_OF_RECEIPT,\n    CASE\n    WHEN DS_ASN.SHIPMENT_IN_TRANSIT.QUANTITY \u003e 10\n    THEN\n        CASE\n            WHEN DS_ASN.SHIPMENT_IN_TRANSIT.QUANTITY \u003e 100\n            THEN\n                CASE\n                    WHEN DS_ASN.SHIPMENT_IN_TRANSIT.QUANTITY \u003e 1000\n                    THEN 3\n                    ELSE 2\n                END\n            ELSE 1\n\tEND\n    ELSE 0\n    END AS QUANTITY_BIN\n    FROM DS_ASN.SHIPMENT_IN_TRANSIT;\n    \n",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 07:04:40.496",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542399640569_1581048916",
      "id": "20180304-185514_1167859763",
      "dateCreated": "2018-11-16 12:20:40.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n## Testing the Code\n\nNow we\u0027ll test our code on the `testing` features table:",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 07:04:46.288",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eTesting the Code\u003c/h2\u003e\n\u003cp\u003eNow we\u0026rsquo;ll test our code on the \u003ccode\u003etesting\u003c/code\u003e features table:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542399640570_-1871533433",
      "id": "20180612-031525_84277416",
      "dateCreated": "2018-11-16 12:20:40.000",
      "dateStarted": "2018-11-20 17:27:35.826",
      "dateFinished": "2018-11-20 17:27:35.829",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, LongType, BooleanType };\n\n    val test_data_with_uppercase_schema \u003d splicemachineContext.df(\"select * from DS_ASN.TEST_FEATURES\")\n    val newNames \u003d Seq(\n        \"shipmentid\",\n        \"status\",\n        \"shipmode\",\n        \"product_description\",\n        \"consignee\",\n        \"shipper\",\n        \"arrival_date\",\n        \"gross_weight_lb\",\n        \"gross_weight_kg\",\n        \"foreign_port\",\n        \"us_port\",\n        \"vessel_name\",\n        \"country_of_origin\",\n        \"consignee_address\",\n        \"shipper_address\",\n        \"zipcode\",\n        \"no_of_containers\",\n        \"container_number\",\n        \"container_type\",\n        \"quantity\",\n        \"quantity_unit\",\n        \"measurement\",\n        \"measurement_unit\",\n        \"bill_of_lading\",\n        \"house_vs_master\",\n        \"distribution_port\",\n        \"master_bl\",\n        \"voyage_number\",\n        \"seal\",\n        \"ship_registered_in\",\n        \"inbond_entry_type\",\n        \"carrier_code\",\n        \"carrier_name\",\n        \"carrier_city\",\n        \"carrier_state\",\n        \"carrier_zip\",\n        \"carrier_address\",\n        \"notify_party\",\n        \"notify_address\",\n        \"place_of_receipt\",\n        \"date_of_receipt\",\n        \"quantity_bin\"\n    )\n    val test_data \u003d test_data_with_uppercase_schema.toDF(newNames: _*)\n\n    // Make predictions.\n    val lrPredictions \u003d lrModel.transform(test_data)\n\n    var z \u003d Array(\"Zara\", \"Nuha\", \"Ayan\")\n\n    // Select example rows to display.\n     val innerStruct \u003d\n   StructType(\n     StructField(\"f1\", IntegerType, true) ::\n     StructField(\"f2\", LongType, false) ::\n     StructField(\"f3\", BooleanType, false) :: Nil)\n    lrPredictions.select(\"prediction\", \"probability\",\"features\").show(10)\n    splicemachineContext.createTable(\"tabletest\", innerStruct, z, null)",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 07:04:44.668",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542399640570_-2007863537",
      "id": "20180125-142959_1101825868",
      "dateCreated": "2018-11-16 12:20:40.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nimport org.apache.spark.sql.types.{StructType, StructField, StringType};\n\nval predictions \u003d lrPredictions.select(\"SHIPMENTID\", \"PREDICTION\")\n\npredictions.printSchema()\n \nsplicemachineContext.insert(predictions,\"DS_ASN.predictions\") \n\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 07:05:14.341",
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.types.{StructType, StructField, StringType}\npredictions: org.apache.spark.sql.DataFrame \u003d [SHIPMENTID: string, PREDICTION: double]\nroot\n |-- SHIPMENTID: string (nullable \u003d true)\n |-- PREDICTION: double (nullable \u003d true)\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542399640571_747453133",
      "id": "20180304-185743_1150073560",
      "dateCreated": "2018-11-16 12:20:40.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\nselect * from DS_ASN.predictions;",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 07:05:19.388",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 318.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "SHIPMENTID": "string",
                      "PREDICTION": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542399640572_-1258552307",
      "id": "20180304-190035_379792791",
      "dateCreated": "2018-11-16 12:20:40.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Where to Go Next\n\nNow we\u0027re ready to move onto exploring other examples of Machine Learning with Splice Machine, starting with our [*Predicting Supply Chain Shortages*](/#/notebook/2DY411M2A) program.\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 07:05:58.192",
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "results": {},
        "enabled": false,
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eWhere to Go Next\u003c/h2\u003e\n\u003cp\u003eNow we\u0026rsquo;re ready to move onto exploring other examples of Machine Learning with Splice Machine, starting with our \u003ca href\u003d\"/#/notebook/2DY411M2A\"\u003e\u003cem\u003ePredicting Supply Chain Shortages\u003c/em\u003e\u003c/a\u003e program.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542399640572_490874012",
      "id": "20180304-190115_1109571681",
      "dateCreated": "2018-11-16 12:20:40.000",
      "dateStarted": "2018-11-20 10:51:19.653",
      "dateFinished": "2018-11-20 10:51:19.657",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Splice Machine Training/For Data Scientists/h. ML with Spark MLlib Using Scala",
  "id": "2DXZ2JK4E",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "splicemachine:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}