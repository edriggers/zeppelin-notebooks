{
  "paragraphs": [
    {
      "text": "%md\n<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" />\n\n# Machine Learning with Spark MLlib\n\nThis notebook contains code that uses the Machine Learning (<em>ML</em>) Library embedded in Spark, *MLlib*, with the Splice Machine Spark Adapter to realize in-process machine learning. Specifically, the example in this notebook uses data that tracks international shipments to learn, and then predicts how late a shipment will be, based on various factors.\n\nIf you're not familiar with Machine Learning with Spark MLlib, you can learn more about this library here: <a href=\"https://spark.apache.org/docs/latest/ml-guide.html\" target=\"_blank\">https://spark.apache.org/docs/latest/ml-guide.html</a>.\n\nThe code in this project was written in the Scala programming language as well as the Python programming language.\n\nThe remainder of this notebook contains these sections:\n\n* <em>Basic Terminology</em> defines a few major ML terms used in this notebook.\n* <em>About Our Sample Data</em> introduces the shipping data that we use. \n* <em>About our Learning Model</em> describes the learning model method we're using.\n* <em>Creating our Splice Machine Database</em> walks you through setting up our database with our sample data.\n* <em>Creating, Training, and Deploying our Learning Model</em> walks you through our Machine Learning sample code.\n* <em>Program Listing</em> contains a listing of all of the code used in this notebook.\n\n## Basic Terminology\n\nHere's some basic terminology you need to be familiar with to understand the code in this notebook. These descriptions are paraphrased from the <a href=\"https://spark.apache.org/docs/latest/ml-guide.html\" target=\"_blank\">above-mentioned Spark MLlib guide.</a>\n\n<table class=\"splicezep\">\n    <col />\n    <col />\n    <thead>\n        <tr>\n            <th>Term</th>\n            <th>Description</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td class=\"ItalicFont\">DataFrame</td>\n            <td>A DataFrame is a basic Spark SQL concept. A DataFrame is similar to a table in a database: it contains rows of data with columns of varying types. The MLlib operates on datasets that are organized in DataFrames. </td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\">Pipeline</td>\n            <td>In MLlib, you chain together a sequence of algorithms, or <em>stages</em> that operate on your DataFrame into a <em>pipeline</em> that learns.</td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\">Transformer</td>\n            <td>An algorithm that transforms a DataFrame into another DataFrame. Each transformer implements a method named <code>transform</code> that converts the DataFrame, typically by appending additional columns to it. A <em>model</em> is a kind of transformer.</td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\">Estimator</td>\n            <td>A learning algorithm that trains or <em>fits</em> on a DataFrame and produces a <code>model</code>. Each estimator implements a method named <code>fit</code> that produces a model.</td>\n        </tr>\n    </tbody>\n</table>\n    \n\n## About our Sample Data\n\nWe've obtained some actual shipping data that tracks international shipments between ports, and have imported that data into a Splice Machine database that we've named `ASN.` The tables of interest are named `SHIPMENT_IN_TRANSIT` and `SHIPMENT_HISTORY;` you'll see these table used in the sample code below. We also create a database table named `Features` that forms the basis of the DataFrame we use for our learning model; this is the table you'll see featured in this notebook's code. The idea of this model is to predict, in real-time, how late a specific shipment will be, based on past data and other factors. Over time, as more data is processed by the model, the predictions become more accurate. \n\n## About our Learning Model\n\nWe use a Logistic Regression *estimator* as the final stage in our pipeline to produce a Logistic Regression Model of lateness from our data, and then deploy that model on a dataset to predict lateness.\n\nThe estimator operates on data that is formatted into vectors of integers. Since most of the fields in  our input dataset contain string values, we need to convert any data that will be used by the estimator into this format, as you'll see below. ",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:18:56.308",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" />\n<h1>Machine Learning with Spark MLlib</h1>\n<p>This notebook contains code that uses the Machine Learning (<em>ML</em>) Library embedded in Spark, <em>MLlib</em>, with the Splice Machine Spark Adapter to realize in-process machine learning. Specifically, the example in this notebook uses data that tracks international shipments to learn, and then predicts how late a shipment will be, based on various factors.</p>\n<p>If you&rsquo;re not familiar with Machine Learning with Spark MLlib, you can learn more about this library here: <a href=\"https://spark.apache.org/docs/latest/ml-guide.html\" target=\"_blank\"><a href=\"https://spark.apache.org/docs/latest/ml-guide.html\">https://spark.apache.org/docs/latest/ml-guide.html</a></a>.</p>\n<p>The code in this project was written in the Scala programming language as well as the Python programming language.</p>\n<p>The remainder of this notebook contains these sections:</p>\n<ul>\n  <li><em>Basic Terminology</em> defines a few major ML terms used in this notebook.</li>\n  <li><em>About Our Sample Data</em> introduces the shipping data that we use.</li>\n  <li><em>About our Learning Model</em> describes the learning model method we&rsquo;re using.</li>\n  <li><em>Creating our Splice Machine Database</em> walks you through setting up our database with our sample data.</li>\n  <li><em>Creating, Training, and Deploying our Learning Model</em> walks you through our Machine Learning sample code.</li>\n  <li><em>Program Listing</em> contains a listing of all of the code used in this notebook.</li>\n</ul>\n<h2>Basic Terminology</h2>\n<p>Here&rsquo;s some basic terminology you need to be familiar with to understand the code in this notebook. These descriptions are paraphrased from the <a href=\"https://spark.apache.org/docs/latest/ml-guide.html\" target=\"_blank\">above-mentioned Spark MLlib guide.</a></p>\n<table class=\"splicezep\">\n    <col />\n    <col />\n    <thead>\n        <tr>\n            <th>Term</th>\n            <th>Description</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td class=\"ItalicFont\">DataFrame</td>\n            <td>A DataFrame is a basic Spark SQL concept. A DataFrame is similar to a table in a database: it contains rows of data with columns of varying types. The MLlib operates on datasets that are organized in DataFrames. </td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\">Pipeline</td>\n            <td>In MLlib, you chain together a sequence of algorithms, or <em>stages</em> that operate on your DataFrame into a <em>pipeline</em> that learns.</td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\">Transformer</td>\n            <td>An algorithm that transforms a DataFrame into another DataFrame. Each transformer implements a method named <code>transform</code> that converts the DataFrame, typically by appending additional columns to it. A <em>model</em> is a kind of transformer.</td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\">Estimator</td>\n            <td>A learning algorithm that trains or <em>fits</em> on a DataFrame and produces a <code>model</code>. Each estimator implements a method named <code>fit</code> that produces a model.</td>\n        </tr>\n    </tbody>\n</table>\n<h2>About our Sample Data</h2>\n<p>We&rsquo;ve obtained some actual shipping data that tracks international shipments between ports, and have imported that data into a Splice Machine database that we&rsquo;ve named <code>ASN.</code> The tables of interest are named <code>SHIPMENT_IN_TRANSIT</code> and <code>SHIPMENT_HISTORY;</code> you&rsquo;ll see these table used in the sample code below. We also create a database table named <code>Features</code> that forms the basis of the DataFrame we use for our learning model; this is the table you&rsquo;ll see featured in this notebook&rsquo;s code. The idea of this model is to predict, in real-time, how late a specific shipment will be, based on past data and other factors. Over time, as more data is processed by the model, the predictions become more accurate. </p>\n<h2>About our Learning Model</h2>\n<p>We use a Logistic Regression <em>estimator</em> as the final stage in our pipeline to produce a Logistic Regression Model of lateness from our data, and then deploy that model on a dataset to predict lateness.</p>\n<p>The estimator operates on data that is formatted into vectors of integers. Since most of the fields in our input dataset contain string values, we need to convert any data that will be used by the estimator into this format, as you&rsquo;ll see below.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395787807_-1859852963",
      "id": "20180129-160012_924943773",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Create our Splice Machine Database\n\nBefore working with the MLlib, we need to create a Splice Machine database that contains the shipping data we're using. We:\n\n1. Connect to your database via JDBC\n2. Create the schema and tables\n3. Import the data\n4. Create our features table\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:19:01.848",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Create our Splice Machine Database</h2>\n<p>Before working with the MLlib, we need to create a Splice Machine database that contains the shipping data we&rsquo;re using. We:</p>\n<ol>\n  <li>Connect to your database via JDBC</li>\n  <li>Create the schema and tables</li>\n  <li>Import the data</li>\n  <li>Create our features table</li>\n</ol>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395787808_1407293526",
      "id": "20180202-100849_1233744001",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 1. Connect to Your Database via JDBC\n\nFirst we'll configure the URL we'll use in our JDBC connection to Splice Machine. \n\nFor this class, you can simply use the `defaultJDBCURL` assignment in the next paragraph. When running on a cluster, you can copy and paste the JDBC URL you'll find displayed at the bottom right of your cluster dashboard.",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:19:13.865",
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false,
        "fontSize": 9.0,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>1. Connect to Your Database via JDBC</h3>\n<p>First we&rsquo;ll configure the URL we&rsquo;ll use in our JDBC connection to Splice Machine. </p>\n<p>For this class, you can simply use the <code>defaultJDBCURL</code> assignment in the next paragraph. When running on a cluster, you can copy and paste the JDBC URL you&rsquo;ll find displayed at the bottom right of your cluster dashboard.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395787809_1910261322",
      "id": "20180613-033509_2022872512",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\ndefaultJDBCURL = \"\"\"jdbc:splice://localhost:1527/splicedb;user=splice;password=admin\"\"\"\n\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-02 17:55:57.891",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {
          "JDBCurl": "jdbc:splice://localhost:1527/splicedb;user=splice;password=admin",
          "JDBCurl Scala": "jdbc:splice://localhost:1527/splicedb;user=splice;password=admin;useSpark=true",
          "JDBCxurl": "jdbc:splice://localhost:1527/splicedb;user=splice;password=admin;useSpark=true",
          "JDBCURL Scala": "jdbc:splice://localhost:1527/splicedb;user=splice;password=admin",
          "Scala JDBCurl": "jdbc:splice://localhost:1527/splicedb;user=splice;password=admin"
        },
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1542395787809_-1189057698",
      "id": "20180215-062654_2077965041",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n### 2. Create the Schema and Tables\n\nWe'll now create our new schema, make it our default schema, and then create the tables for the `shipment_in_transit` and `shipment_history` data that we will import.",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:19:32.476",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>2. Create the Schema and Tables</h3>\n<p>We&rsquo;ll now create our new schema, make it our default schema, and then create the tables for the <code>shipment_in_transit</code> and <code>shipment_history</code> data that we will import.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542759443596_-1054064431",
      "id": "20181120-161723_2024551045",
      "dateCreated": "2018-11-20 16:17:23.596",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\n\nCREATE SCHEMA DEV2_ASN;\nSET SCHEMA DEV2_ASN;\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-02 18:08:21.516",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 4.0,
        "editorMode": "ace/mode/sql",
        "editorHide": false,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395787809_90897399",
      "id": "20180202-101539_620068341",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\n\nDROP TABLE IF EXISTS SHIPMENT_IN_TRANSIT;\nCREATE TABLE SHIPMENT_IN_TRANSIT(\n    SHIPMENTID VARCHAR(11) NOT NULL PRIMARY KEY,\n    STATUS VARCHAR(50),\n    SHIPMODE VARCHAR(30),\n    PRODUCT_DESCRIPTION VARCHAR(500),\n    CONSIGNEE VARCHAR(200),\n    SHIPPER VARCHAR(100),\n    ARRIVAL_DATE TIMESTAMP,\n    GROSS_WEIGHT_LB INTEGER,\n    GROSS_WEIGHT_KG INTEGER,\n    FOREIGN_PORT VARCHAR(50),\n    US_PORT VARCHAR(50),\n    VESSEL_NAME VARCHAR(40),\n    COUNTRY_OF_ORIGIN VARCHAR(40),\n    CONSIGNEE_ADDRESS VARCHAR(150),\n    SHIPPER_ADDRESS VARCHAR(150),\n    ZIPCODE VARCHAR(20),\n    NO_OF_CONTAINERS INTEGER,\n    CONTAINER_NUMBER VARCHAR(200),\n    CONTAINER_TYPE VARCHAR(80),\n    QUANTITY INTEGER,\n    QUANTITY_UNIT VARCHAR(10),\n    MEASUREMENT INTEGER,\n    MEASUREMENT_UNIT VARCHAR(5),\n    BILL_OF_LADING VARCHAR(20),\n    HOUSE_VS_MASTER CHAR(1),\n    DISTRIBUTION_PORT VARCHAR(40),\n    MASTER_BL VARCHAR(20),\n    VOYAGE_NUMBER VARCHAR(10),\n    SEAL VARCHAR(300),\n    SHIP_REGISTERED_IN VARCHAR(40),\n    INBOND_ENTRY_TYPE VARCHAR(30),\n    CARRIER_CODE VARCHAR(10),\n    CARRIER_NAME VARCHAR(40),\n    CARRIER_CITY VARCHAR(40),\n    CARRIER_STATE VARCHAR(10),\n    CARRIER_ZIP VARCHAR(10),\n    CARRIER_ADDRESS VARCHAR(200),\n    NOTIFY_PARTY VARCHAR(50),\n    NOTIFY_ADDRESS VARCHAR(200),\n    PLACE_OF_RECEIPT VARCHAR(50),\n    DATE_OF_RECEIPT TIMESTAMP\n    );\n\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-02 18:08:24.342",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 4.0,
        "editorMode": "ace/mode/sql",
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395787810_-28707119",
      "id": "20180202-102107_567153190",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\nDROP TABLE IF EXISTS SHIPMENT_HISTORY;\nCREATE TABLE SHIPMENT_HISTORY(\n    SHIPMENTID VARCHAR(11) NOT NULL PRIMARY KEY,\n    STATUS VARCHAR(50),\n    SHIPMODE VARCHAR(30),\n    PRODUCT_DESCRIPTION VARCHAR(500),\n    CONSIGNEE VARCHAR(200),\n    SHIPPER VARCHAR(100),\n    ARRIVAL_DATE TIMESTAMP,\n    GROSS_WEIGHT_LB INTEGER,\n    GROSS_WEIGHT_KG INTEGER,\n    FOREIGN_PORT VARCHAR(50),\n    US_PORT VARCHAR(50),\n    VESSEL_NAME VARCHAR(40),\n    COUNTRY_OF_ORIGIN VARCHAR(40),\n    CONSIGNEE_ADDRESS VARCHAR(150),\n    SHIPPER_ADDRESS VARCHAR(150),\n    ZIPCODE VARCHAR(20),\n    NO_OF_CONTAINERS INTEGER,\n    CONTAINER_NUMBER VARCHAR(200),\n    CONTAINER_TYPE VARCHAR(80),\n    QUANTITY INTEGER,\n    QUANTITY_UNIT VARCHAR(10),\n    MEASUREMENT INTEGER,\n    MEASUREMENT_UNIT VARCHAR(5),\n    BILL_OF_LADING VARCHAR(20),\n    HOUSE_VS_MASTER CHAR(1),\n    DISTRIBUTION_PORT VARCHAR(40),\n    MASTER_BL VARCHAR(20),\n    VOYAGE_NUMBER VARCHAR(10),\n    SEAL VARCHAR(300),\n    SHIP_REGISTERED_IN VARCHAR(40),\n    INBOND_ENTRY_TYPE VARCHAR(30),\n    CARRIER_CODE VARCHAR(10),\n    CARRIER_NAME VARCHAR(40),\n    CARRIER_CITY VARCHAR(40),\n    CARRIER_STATE VARCHAR(10),\n    CARRIER_ZIP VARCHAR(10),\n    CARRIER_ADDRESS VARCHAR(200),\n    NOTIFY_PARTY VARCHAR(50),\n    NOTIFY_ADDRESS VARCHAR(200),\n    PLACE_OF_RECEIPT VARCHAR(50),\n    DATE_OF_RECEIPT TIMESTAMP\n);\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-02 18:08:33.181",
      "config": {
        "colWidth": 4.0,
        "editorMode": "ace/mode/sql",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "fontSize": 9.0,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395787810_2044720705",
      "id": "20180202-102130_85303245",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 3. Import the Data\n\nNext we import the shipping data, which is in csv format, into our Splice Machine database.\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:20:12.931",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>3. Import the Data</h3>\n<p>Next we import the shipping data, which is in csv format, into our Splice Machine database.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395787810_156567497",
      "id": "20180202-104354_1962297047",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\ncall SYSCS_UTIL.IMPORT_DATA (\n     'DEV2_ASN',\n     'SHIPMENT_IN_TRANSIT',\n     null,\n     's3a://splice-demo/shipment/shipment_in_transit.csv',\n     '|',\n     null,\n     'yyyy-MM-dd HH:mm:ss.SSSSSS',\n     'yyyy-MM-dd',\n     null,\n     -1,\n     '/tmp',\n     true, null);",
      "user": "anonymous",
      "dateUpdated": "2018-12-02 18:09:23.971",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/sql",
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "rowsImported": "string",
                      "failedRows": "string",
                      "files": "string",
                      "dataSize": "string",
                      "failedLog": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395787811_1399795537",
      "id": "20180202-104501_1686524282",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\ncall SYSCS_UTIL.IMPORT_DATA (\n     'DEV2_ASN',\n     'SHIPMENT_HISTORY',\n     null,\n     's3a://splice-demo/shipment/shipment_history.csv',\n     '|',\n     null,\n     'yyyy-MM-dd HH:mm:ss.SSSSSS',\n     'yyyy-MM-dd',\n     null,\n     -1,\n     '/tmp',\n     true, null);",
      "user": "anonymous",
      "dateUpdated": "2018-12-02 18:09:21.359",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/sql",
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "rowsImported": "string",
                      "failedRows": "string",
                      "files": "string",
                      "dataSize": "string",
                      "failedLog": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            },
            "helium": {}
          }
        },
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395787811_1348543062",
      "id": "20180202-104531_242494964",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 4. Create our Features Table\n\nWe create a features table in our database that we use with our learning model. We add three computed fields in the `features` table that are important to our model:\n\n* `quantity_bin` categorizes shipping quantities into bins, to improve learning accuracy \n* `lateness` computes how many days late a shipment was\n* `label` categorizes lateness into one of four values:\n\n<table class=\"spliceZepNoBorder\" style=\"margin: 0 0 100px 50px;\">\n    <tbody>\n            <tr><td>0</td><td>0 days late</td></tr>\n            <tr><td>1</td><td>1-5 days late</td></tr>\n            <tr><td>2</td><td>5-10 days late</td></tr>\n            <tr><td>3</td><td>10 days or more late</td></tr>\n    </tbody>\n</table>",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:20:34.367",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>4. Create our Features Table</h3>\n<p>We create a features table in our database that we use with our learning model. We add three computed fields in the <code>features</code> table that are important to our model:</p>\n<ul>\n  <li><code>quantity_bin</code> categorizes shipping quantities into bins, to improve learning accuracy</li>\n  <li><code>lateness</code> computes how many days late a shipment was</li>\n  <li><code>label</code> categorizes lateness into one of four values:</li>\n</ul>\n<table class=\"spliceZepNoBorder\" style=\"margin: 0 0 100px 50px;\">\n    <tbody>\n            <tr><td>0</td><td>0 days late</td></tr>\n            <tr><td>1</td><td>1-5 days late</td></tr>\n            <tr><td>2</td><td>5-10 days late</td></tr>\n            <tr><td>3</td><td>10 days or more late</td></tr>\n    </tbody>\n</table>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395787811_-342254676",
      "id": "20180202-104618_659628734",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\ndrop table IF EXISTS DEV2_ASN.FEATURES;\nCREATE table DEV2_ASN.FEATURES AS\n    SELECT \n    SHIPMENTID,\n    STATUS,\n    SHIPMODE,\n    PRODUCT_DESCRIPTION,\n    CONSIGNEE,\n    SHIPPER,\n    ARRIVAL_DATE,\n    GROSS_WEIGHT_LB,\n    GROSS_WEIGHT_KG,\n    FOREIGN_PORT,\n    US_PORT,\n    VESSEL_NAME,\n    COUNTRY_OF_ORIGIN,\n    CONSIGNEE_ADDRESS,\n    SHIPPER_ADDRESS,\n    ZIPCODE,\n    NO_OF_CONTAINERS,\n    CONTAINER_NUMBER,\n    CONTAINER_TYPE,\n    QUANTITY,\n    QUANTITY_UNIT,\n    MEASUREMENT,\n    MEASUREMENT_UNIT,\n    BILL_OF_LADING,\n    HOUSE_VS_MASTER,\n    DISTRIBUTION_PORT,\n    MASTER_BL,\n    VOYAGE_NUMBER,\n    SEAL,\n    SHIP_REGISTERED_IN,\n    INBOND_ENTRY_TYPE,\n    CARRIER_CODE,\n    CARRIER_NAME,\n    CARRIER_CITY,\n    CARRIER_STATE,\n    CARRIER_ZIP,\n    CARRIER_ADDRESS,\n    NOTIFY_PARTY,\n    NOTIFY_ADDRESS,\n    PLACE_OF_RECEIPT,\n    DATE_OF_RECEIPT,\n    CASE\n    WHEN DEV2_ASN.SHIPMENT_HISTORY.QUANTITY > 10\n    THEN\n        CASE\n            WHEN DEV2_ASN.SHIPMENT_HISTORY.QUANTITY > 100\n            THEN\n                CASE\n                    WHEN DEV2_ASN.SHIPMENT_HISTORY.QUANTITY > 1000\n                    THEN 3\n                    ELSE 2\n                END\n            ELSE 1\n    END\n    ELSE 0\n    END AS QUANTITY_BIN,\n    DEV2_ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - DEV2_ASN.SHIPMENT_HISTORY.ARRIVAL_DATE as LATENESS,\n    CASE\n    WHEN  DEV2_ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - DEV2_ASN.SHIPMENT_HISTORY.ARRIVAL_DATE > 0\n    THEN\n        CASE\n            WHEN  DEV2_ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - DEV2_ASN.SHIPMENT_HISTORY.ARRIVAL_DATE > 5\n            THEN\n                CASE\n                    WHEN  DEV2_ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - DEV2_ASN.SHIPMENT_HISTORY.ARRIVAL_DATE > 10\n                    THEN 3\n                    ELSE 2\n                END\n            ELSE 1\n    END\n    ELSE 0\n    END AS LABEL\nFROM DEV2_ASN.SHIPMENT_HISTORY ",
      "user": "anonymous",
      "dateUpdated": "2018-12-02 18:09:48.001",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "fontSize": 9.0,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395787812_-713520919",
      "id": "20180205-022847_1435938411",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Create, Train, and Deploy our Learning Model\n\nThe remainder of this notebook walks you through the code we use to create, train, and deploy our learning model, in these steps:\n\n1. Perform Spark+MLlib Setup Tasks\n2. Create our DataFrame\n3. Create Pipeline Stages\n4. Assemble the Pipeline>Train our Model\n5. Deploy our Model\n\nWe include the entire program at the end of this notebook.",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:21:37.537",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Create, Train, and Deploy our Learning Model</h2>\n<p>The remainder of this notebook walks you through the code we use to create, train, and deploy our learning model, in these steps:</p>\n<ol>\n  <li>Perform Spark+MLlib Setup Tasks</li>\n  <li>Create our DataFrame</li>\n  <li>Create Pipeline Stages</li>\n  <li>Assemble the Pipeline&gt;Train our Model</li>\n  <li>Deploy our Model</li>\n</ol>\n<p>We include the entire program at the end of this notebook.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395787812_-451255579",
      "id": "20180131-172852_644197695",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 1. Perform Spark + MLlib Setup Tasks\n\nThe Python Splice Machine API communicates with your database through the `PySparkContext` class, which we created in the previous Notebook.\n\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-02 18:07:34.907",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>1. Perform Spark + MLlib Setup Tasks</h3>\n<p>The Python Splice Machine API communicates with your database through the <code>PySparkContext</code> class, which we created in the previous Notebook.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542759213126_684562962",
      "id": "20181120-161333_1536591529",
      "dateCreated": "2018-11-20 16:13:33.127",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \nWe use the following code to instantiate `PySpliceContext` and import our modules:\n\n```\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\n\nsplice = PySpliceContext(defaultJDBCURL, sqlContext)\n```",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:22:19.245",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>We use the following code to instantiate <code>PySpliceContext</code> and import our modules:</p>\n<pre><code>from pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\n\nsplice = PySpliceContext(defaultJDBCURL, sqlContext)\n</code></pre>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395787814_1617706465",
      "id": "20180611-203526_1071983380",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 2. Create our DataFrame\n\nNext we create a Python DataFrame from the results of a SQL `SELECT` query from the database. This allows us to manipulate our Splice Machine data as a Spark DataFrame\n\n```\ndf_with_uppercase_schema = PySpliceContext.df(\"select * from DEV2_ASN.Features\")\nnewNames = [\n    \"consignee\",\n    \"shipper\",\n    \"shipmode\",\n    \"gross_weight_lb\",\n    \"foreign_port\",\n    \"us_port\",\n    \"vessel_name\",\n    \"country_of_origin\",\n    \"container_number\",\n    \"container_type\",\n    \"quantity\",\n    \"ship_registered_in\",\n    \"carrier_code\",\n    \"carrier_city\",\n    \"notify_party\",\n    \"place_of_receipt\",\n    \"zipcode\",\n    \"quantity_bin\"\n    ]\ndf = df_with_uppercase_schema.toDF(newNames)\n```",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:23:24.910",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>2. Create our DataFrame</h3>\n<p>Next we create a Python DataFrame from the results of a SQL <code>SELECT</code> query from the database. This allows us to manipulate our Splice Machine data as a Spark DataFrame</p>\n<pre><code>df_with_uppercase_schema = PySpliceContext.df(&quot;select * from DEV2_ASN.Features&quot;)\nnewNames = [\n    &quot;consignee&quot;,\n    &quot;shipper&quot;,\n    &quot;shipmode&quot;,\n    &quot;gross_weight_lb&quot;,\n    &quot;foreign_port&quot;,\n    &quot;us_port&quot;,\n    &quot;vessel_name&quot;,\n    &quot;country_of_origin&quot;,\n    &quot;container_number&quot;,\n    &quot;container_type&quot;,\n    &quot;quantity&quot;,\n    &quot;ship_registered_in&quot;,\n    &quot;carrier_code&quot;,\n    &quot;carrier_city&quot;,\n    &quot;notify_party&quot;,\n    &quot;place_of_receipt&quot;,\n    &quot;zipcode&quot;,\n    &quot;quantity_bin&quot;\n    ]\ndf = df_with_uppercase_schema.toDF(newNames)\n</code></pre>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395787814_-782902311",
      "id": "20180611-211856_605292217",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 3. Create Pipeline Stages\n\nOur pipeline stages are fairly simple:\n\n* Transform each row of data in the input dataset into an integer vector.\n* Assemble the vectors into a DataFrame\n* Use a Logistic Regression Estimator to create our model\n\n#### Transform each row of data into an integer vector\n\nThe Logistic Regression estimator operates on integer vectors, so we need to convert each row in our input dataframe into an integer vector. Remember that each row contains only the fields from our database that are of interest to our model: the fields that previously included in our sequence and concatenated onto our DataFrame.\n\nSpark includes a `StringIndexer` function that does exactly that, so we create a `StringIndexer` for each field, and we'll later use each of these as a stage in our learning pipeline. The `StringIndexer` transforms the data from a specified input column in our DataFrame and stores the output in a specified and new output column. By convention, we name each string indexer with the name of the field+`Indexer,` and name the output column the name of the field+`Index,` e.g. we create a transformer named `consigneeIndexer` to transform the input column `consignee` into the new output column `consigneeIndex.`\n\n```\n// Transform strings into numbers\nconsigneeIndexer =  StringIndexer(inputCol=\"consignee\", outputCol=\"consigneeIndex\", handleInvalid=\"skip\")\nshipperIndexer = StringIndexer(inputCol=\"shipper\", outputCol=\"shipperIndex\", handleInvalid=\"skip\")\nshipmodeIndexer = StringIndexer(inputCol=\"shipmode\", outputCol=\"shipmodeIndex\", handleInvalid=\"skip\")\ngross_weight_lbIndexer = StringIndexer(inputCol=\"gross_weight_lb\", outputCol=\"gross_weight_lbIndex\", handleInvalid=\"skip\")\nforeign_portIndexer =  StringIndexer(inputCol=\"foreign_port\", outputCol=\"foreign_portIndex\", handleInvalid=\"skip\")\nus_portIndexer = StringIndexer(inputCol=\"us_port\", outputCol=\"us_portIndex\", handleInvalid=\"skip\")\nvessel_nameIndexer = StringIndexer(inputCol=\"vessel_name\", outputCol=\"vessel_nameIndex\",  handleInvalid=\"skip\")\ncountry_of_originIndexer = StringIndexer(inputCol=\"country_of_origin\", outputCol=\"country_of_originIndex\",  handleInvalid=\"skip\")\ncontainer_numberIndexer = StringIndexer(inputCol=\"container_number\", outputCol=\"container_numberIndex\", handleInvalid=\"skip\")\ncontainer_typeIndexer = StringIndexer(inputCol=\"container_type\", outputCol=\"container_typeIndex\", handleInvalid=\"skip\")\nship_registered_inIndexer = StringIndexer(inputCol=\"ship_registered_in\", outputCol=\"ship_registered_inIndex\", handleInvalid=\"skip\")\ncarrier_codeIndexer = StringIndexer(inputCol=\"carrier_code\", outputCol=\"carrier_codeIndex\", handleInvalid=\"skip\")\ncarrier_cityIndexer = StringIndexer(inputCol=\"carrier_city\", outputCol=\"carrier_cityIndex\", handleInvalid=\"skip\")\nnotify_partyIndexer = StringIndexer(inputCol=\"notify_party\", outputCol=\"notify_partyIndex\", handleInvalid=\"skip\")\nplace_of_receiptIndexer = StringIndexer(inputCol=\"place_of_receipt\", outputCol=\"place_of_receiptIndex\", handleInvalid=\"skip\")\nzipcodeIndexer = StringIndexer(inputCol=\"zipcode\", outputCol=\"zipcodeIndex\", handleInvalid=\"skip\")\n```\n#### Assemble the Vectors\n\nAfter our pipeline has transformed data into numbers, we need to assemble those into vectors. Spark includes a `VectorAssembler` object that does just that, transforming a set of input columns into a vector that is stored in the `features` column in the DataFrame:\n\n```\n//assemble raw features\nassembler = VectorAssembler(inputCols=[\n                    \"shipmodeIndex\",\n                    \"consigneeIndex\",\n                    \"shipperIndex\",\n                    \"gross_weight_lbIndex\",\n                    \"foreign_portIndex\",\n                    \"us_portIndex\",\n                    \"vessel_nameIndex\",\n                    \"country_of_originIndex\",\n                    \"container_numberIndex\",\n                    \"container_typeIndex\",\n                    \"quantity_bin\",\n                    \"ship_registered_inIndex\",\n                    \"carrier_codeIndex\",\n                    \"carrier_cityIndex\",\n                    \"notify_partyIndex\",\n                    \"place_of_receiptIndex\",\n                    \"zipcodeIndex\",\n                    \"quantity_bin\"\n                    ], outputCol='features')\n```\n\n#### Create the Estimator\n\nCreating the estimator is a simple matter of specifying a few parameters, including which column in the DataFrame is the label, and which column contains the feature set:\n\n```\n//Create ML analytic\nlr = LogisticRegression(maxIter=30, labelCol=\"label\", featuresCol=\"features\", regParam=0.3)\n\n```\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:23:31.149",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>3. Create Pipeline Stages</h3>\n<p>Our pipeline stages are fairly simple:</p>\n<ul>\n  <li>Transform each row of data in the input dataset into an integer vector.</li>\n  <li>Assemble the vectors into a DataFrame</li>\n  <li>Use a Logistic Regression Estimator to create our model</li>\n</ul>\n<h4>Transform each row of data into an integer vector</h4>\n<p>The Logistic Regression estimator operates on integer vectors, so we need to convert each row in our input dataframe into an integer vector. Remember that each row contains only the fields from our database that are of interest to our model: the fields that previously included in our sequence and concatenated onto our DataFrame.</p>\n<p>Spark includes a <code>StringIndexer</code> function that does exactly that, so we create a <code>StringIndexer</code> for each field, and we&rsquo;ll later use each of these as a stage in our learning pipeline. The <code>StringIndexer</code> transforms the data from a specified input column in our DataFrame and stores the output in a specified and new output column. By convention, we name each string indexer with the name of the field+<code>Indexer,</code> and name the output column the name of the field+<code>Index,</code> e.g. we create a transformer named <code>consigneeIndexer</code> to transform the input column <code>consignee</code> into the new output column <code>consigneeIndex.</code></p>\n<pre><code>// Transform strings into numbers\nconsigneeIndexer =  StringIndexer(inputCol=&quot;consignee&quot;, outputCol=&quot;consigneeIndex&quot;, handleInvalid=&quot;skip&quot;)\nshipperIndexer = StringIndexer(inputCol=&quot;shipper&quot;, outputCol=&quot;shipperIndex&quot;, handleInvalid=&quot;skip&quot;)\nshipmodeIndexer = StringIndexer(inputCol=&quot;shipmode&quot;, outputCol=&quot;shipmodeIndex&quot;, handleInvalid=&quot;skip&quot;)\ngross_weight_lbIndexer = StringIndexer(inputCol=&quot;gross_weight_lb&quot;, outputCol=&quot;gross_weight_lbIndex&quot;, handleInvalid=&quot;skip&quot;)\nforeign_portIndexer =  StringIndexer(inputCol=&quot;foreign_port&quot;, outputCol=&quot;foreign_portIndex&quot;, handleInvalid=&quot;skip&quot;)\nus_portIndexer = StringIndexer(inputCol=&quot;us_port&quot;, outputCol=&quot;us_portIndex&quot;, handleInvalid=&quot;skip&quot;)\nvessel_nameIndexer = StringIndexer(inputCol=&quot;vessel_name&quot;, outputCol=&quot;vessel_nameIndex&quot;,  handleInvalid=&quot;skip&quot;)\ncountry_of_originIndexer = StringIndexer(inputCol=&quot;country_of_origin&quot;, outputCol=&quot;country_of_originIndex&quot;,  handleInvalid=&quot;skip&quot;)\ncontainer_numberIndexer = StringIndexer(inputCol=&quot;container_number&quot;, outputCol=&quot;container_numberIndex&quot;, handleInvalid=&quot;skip&quot;)\ncontainer_typeIndexer = StringIndexer(inputCol=&quot;container_type&quot;, outputCol=&quot;container_typeIndex&quot;, handleInvalid=&quot;skip&quot;)\nship_registered_inIndexer = StringIndexer(inputCol=&quot;ship_registered_in&quot;, outputCol=&quot;ship_registered_inIndex&quot;, handleInvalid=&quot;skip&quot;)\ncarrier_codeIndexer = StringIndexer(inputCol=&quot;carrier_code&quot;, outputCol=&quot;carrier_codeIndex&quot;, handleInvalid=&quot;skip&quot;)\ncarrier_cityIndexer = StringIndexer(inputCol=&quot;carrier_city&quot;, outputCol=&quot;carrier_cityIndex&quot;, handleInvalid=&quot;skip&quot;)\nnotify_partyIndexer = StringIndexer(inputCol=&quot;notify_party&quot;, outputCol=&quot;notify_partyIndex&quot;, handleInvalid=&quot;skip&quot;)\nplace_of_receiptIndexer = StringIndexer(inputCol=&quot;place_of_receipt&quot;, outputCol=&quot;place_of_receiptIndex&quot;, handleInvalid=&quot;skip&quot;)\nzipcodeIndexer = StringIndexer(inputCol=&quot;zipcode&quot;, outputCol=&quot;zipcodeIndex&quot;, handleInvalid=&quot;skip&quot;)\n</code></pre>\n<h4>Assemble the Vectors</h4>\n<p>After our pipeline has transformed data into numbers, we need to assemble those into vectors. Spark includes a <code>VectorAssembler</code> object that does just that, transforming a set of input columns into a vector that is stored in the <code>features</code> column in the DataFrame:</p>\n<pre><code>//assemble raw features\nassembler = VectorAssembler(inputCols=[\n                    &quot;shipmodeIndex&quot;,\n                    &quot;consigneeIndex&quot;,\n                    &quot;shipperIndex&quot;,\n                    &quot;gross_weight_lbIndex&quot;,\n                    &quot;foreign_portIndex&quot;,\n                    &quot;us_portIndex&quot;,\n                    &quot;vessel_nameIndex&quot;,\n                    &quot;country_of_originIndex&quot;,\n                    &quot;container_numberIndex&quot;,\n                    &quot;container_typeIndex&quot;,\n                    &quot;quantity_bin&quot;,\n                    &quot;ship_registered_inIndex&quot;,\n                    &quot;carrier_codeIndex&quot;,\n                    &quot;carrier_cityIndex&quot;,\n                    &quot;notify_partyIndex&quot;,\n                    &quot;place_of_receiptIndex&quot;,\n                    &quot;zipcodeIndex&quot;,\n                    &quot;quantity_bin&quot;\n                    ], outputCol=&#39;features&#39;)\n</code></pre>\n<h4>Create the Estimator</h4>\n<p>Creating the estimator is a simple matter of specifying a few parameters, including which column in the DataFrame is the label, and which column contains the feature set:</p>\n<pre><code>//Create ML analytic\nlr = LogisticRegression(maxIter=30, labelCol=&quot;label&quot;, featuresCol=&quot;features&quot;, regParam=0.3)\n\n</code></pre>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395787814_-937797929",
      "id": "20180611-212921_295690204",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 4. Assemble our Pipeline\n\nNow we're ready to assemble our pipeline:\n\n```\n// Chain indexers and tree in a Pipeline\nlrPipeline = Pipeline(stages=\n        [consigneeIndexer,\n        shipperIndexer,\n        shipmodeIndexer,\n        gross_weight_lbIndexer,\n        foreign_portIndexer,\n        us_portIndexer,\n        vessel_nameIndexer,\n        country_of_originIndexer,\n        container_numberIndexer,\n        container_typeIndexer,\n        ship_registered_inIndexer,\n        carrier_codeIndexer,\n        carrier_cityIndexer,\n        notify_partyIndexer,\n        place_of_receiptIndexer,\n        zipcodeIndexer,\n        assembler,\n        lr]\n        )\n```",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:23:55.260",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>4. Assemble our Pipeline</h3>\n<p>Now we&rsquo;re ready to assemble our pipeline:</p>\n<pre><code>// Chain indexers and tree in a Pipeline\nlrPipeline = Pipeline(stages=\n        [consigneeIndexer,\n        shipperIndexer,\n        shipmodeIndexer,\n        gross_weight_lbIndexer,\n        foreign_portIndexer,\n        us_portIndexer,\n        vessel_nameIndexer,\n        country_of_originIndexer,\n        container_numberIndexer,\n        container_typeIndexer,\n        ship_registered_inIndexer,\n        carrier_codeIndexer,\n        carrier_cityIndexer,\n        notify_partyIndexer,\n        place_of_receiptIndexer,\n        zipcodeIndexer,\n        assembler,\n        lr]\n        )\n</code></pre>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395787815_-1002231981",
      "id": "20180611-215932_1084664308",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 5. Train our Model\nNow that our pipeline is set up, all we need to do to train our model by feeding our dataframe into the pipeline's `fit` method, which learns from the data. \n```\n// Train model. \nlrModel = lrPipeline.fit(df)\n```\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:24:00.998",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>5. Train our Model</h3>\n<p>Now that our pipeline is set up, all we need to do to train our model by feeding our dataframe into the pipeline&rsquo;s <code>fit</code> method, which learns from the data. </p>\n<pre><code>// Train model. \nlrModel = lrPipeline.fit(df)\n</code></pre>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395787815_1282966933",
      "id": "20180611-220247_1750575070",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Using Spark MLlib",
      "text": "%md\n### 6. Materialize the Model\n\nAfter training our model, we can apply it to real data and display the results. For simplicity sake, in this example, we'll apply the model to our feature table itself.\n\n```\nlrModel.transform(df).select(\"prediction\", \"probability\", \"features\").show(100)\n```",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:24:06.119",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "title": false,
        "results": {},
        "enabled": false,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>6. Materialize the Model</h3>\n<p>After training our model, we can apply it to real data and display the results. For simplicity sake, in this example, we&rsquo;ll apply the model to our feature table itself.</p>\n<pre><code>lrModel.transform(df).select(&quot;prediction&quot;, &quot;probability&quot;, &quot;features&quot;).show(100)\n</code></pre>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395787815_-1172896776",
      "id": "20180118-020316_1913850778",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\nselect *  from DEV2_ASN.features { limit 100 }\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-02 18:10:19.203",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "editorHide": false,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "SHIPMENTID": "string",
                      "STATUS": "string",
                      "SHIPMODE": "string",
                      "PRODUCT_DESCRIPTION": "string",
                      "CONSIGNEE": "string",
                      "SHIPPER": "string",
                      "ARRIVAL_DATE": "string",
                      "GROSS_WEIGHT_LB": "string",
                      "GROSS_WEIGHT_KG": "string",
                      "FOREIGN_PORT": "string",
                      "US_PORT": "string",
                      "VESSEL_NAME": "string",
                      "COUNTRY_OF_ORIGIN": "string",
                      "CONSIGNEE_ADDRESS": "string",
                      "SHIPPER_ADDRESS": "string",
                      "ZIPCODE": "string",
                      "NO_OF_CONTAINERS": "string",
                      "CONTAINER_NUMBER": "string",
                      "CONTAINER_TYPE": "string",
                      "QUANTITY": "string",
                      "QUANTITY_UNIT": "string",
                      "MEASUREMENT": "string",
                      "MEASUREMENT_UNIT": "string",
                      "BILL_OF_LADING": "string",
                      "HOUSE_VS_MASTER": "string",
                      "DISTRIBUTION_PORT": "string",
                      "MASTER_BL": "string",
                      "VOYAGE_NUMBER": "string",
                      "SEAL": "string",
                      "SHIP_REGISTERED_IN": "string",
                      "INBOND_ENTRY_TYPE": "string",
                      "CARRIER_CODE": "string",
                      "CARRIER_NAME": "string",
                      "CARRIER_CITY": "string",
                      "CARRIER_STATE": "string",
                      "CARRIER_ZIP": "string",
                      "CARRIER_ADDRESS": "string",
                      "NOTIFY_PARTY": "string",
                      "NOTIFY_ADDRESS": "string",
                      "PLACE_OF_RECEIPT": "string",
                      "DATE_OF_RECEIPT": "string",
                      "QUANTITY_BIN": "string",
                      "LATENESS": "string",
                      "LABEL": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            },
            "helium": {}
          }
        },
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395787816_-999139773",
      "id": "20180205-024307_1130627818",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## The Python Code\n\nOur Python code is listed here:",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:24:18.874",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>The Python Code</h2>\n<p>Our Python code is listed here:</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395787817_317619724",
      "id": "20180611-220822_612758436",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\nfrom __future__ import print_function\nimport string\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import Row\nfrom pyspark.sql import DataFrame\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nfrom __future__ import print_function\n\nfrom pyspark.sql import DataFrame\n\n# Setup our SpliceContext\nsplice = PySpliceContext(defaultJDBCURL, sqlContext)\n\n# Query Features\nquery_results = splice.df(\"select * from DEV2_ASN.Features\")\nnewNames = [\"shipmentid\",\n            \"status\",\n            \"shipmode\",\n            \"product_description\",\n            \"consignee\",\n            \"shipper\",\n            \"arrival_date\",\n            \"gross_weight_lb\",\n            \"gross_weight_kg\",\n            \"foreign_port\",\n            \"us_port\",\n            \"vessel_name\",\n            \"country_of_origin\",\n            \"consignee_address\",\n            \"shipper_address\",\n            \"zipcode\",\n            \"no_of_containers\",\n            \"container_number\",\n            \"container_type\",\n            \"quantity\",\n            \"quantity_unit\",\n            \"measurement\",\n            \"measurement_unit\",\n            \"bill_of_lading\",\n            \"house_vs_master\",\n            \"distribution_port\",\n            \"master_bl\",\n            \"voyage_number\",\n            \"seal\",\n            \"ship_registered_in\",\n            \"inbond_entry_type\",\n            \"carrier_code\",\n            \"carrier_name\",\n            \"carrier_city\",\n            \"carrier_state\",\n            \"carrier_zip\",\n            \"carrier_address\",\n            \"notify_party\",\n            \"notify_address\",\n            \"place_of_receipt\",\n            \"date_of_receipt\",\n            \"quantity_bin\",\n            \"lateness\",\n            \"label\"]\n\ndf = query_results.toDF(*newNames)\n\n# Assemble Vectors\nassembler = VectorAssembler(inputCols=[\n    \"shipmodeIndex\",\n    \"consigneeIndex\",\n    \"shipperIndex\",\n    \"gross_weight_lbIndex\",\n    \"foreign_portIndex\",\n    \"us_portIndex\",\n    \"vessel_nameIndex\",\n    \"country_of_originIndex\",\n    \"container_numberIndex\",\n    \"container_typeIndex\",\n    \"ship_registered_inIndex\",\n    \"carrier_codeIndex\",\n    \"carrier_cityIndex\",\n    \"notify_partyIndex\",\n    \"place_of_receiptIndex\",\n    \"zipcodeIndex\",\n    \"quantity_bin\"\n], outputCol='features')\n\n# Transform strings into numbers\nzipcodeIndexer = StringIndexer(inputCol=\"zipcode\", outputCol=\"zipcodeIndex\", handleInvalid=\"skip\")\nconsigneeIndexer = StringIndexer(inputCol=\"consignee\", outputCol=\"consigneeIndex\", handleInvalid=\"skip\")\nshipperIndexer = StringIndexer(inputCol=\"shipper\", outputCol=\"shipperIndex\", handleInvalid=\"skip\")\nstatusIndexer = StringIndexer(inputCol=\"status\", outputCol=\"statusIndex\", handleInvalid=\"skip\")\nshipmodeIndexer = StringIndexer(inputCol=\"shipmode\", outputCol=\"shipmodeIndex\", handleInvalid=\"skip\")\ngross_weight_lbIndexer = StringIndexer(inputCol=\"gross_weight_lb\", outputCol=\"gross_weight_lbIndex\",\n                                       handleInvalid=\"skip\")\nforeign_portIndexer = StringIndexer(inputCol=\"foreign_port\", outputCol=\"foreign_portIndex\", handleInvalid=\"skip\")\nus_portIndexer = StringIndexer(inputCol=\"us_port\", outputCol=\"us_portIndex\", handleInvalid=\"skip\")\nvessel_nameIndexer = StringIndexer(inputCol=\"vessel_name\", outputCol=\"vessel_nameIndex\", handleInvalid=\"skip\")\ncountry_of_originIndexer = StringIndexer(inputCol=\"country_of_origin\", outputCol=\"country_of_originIndex\",\n                                         handleInvalid=\"skip\")\ncontainer_numberIndexer = StringIndexer(inputCol=\"container_number\", outputCol=\"container_numberIndex\",\n                                        handleInvalid=\"skip\")\ncontainer_typeIndexer = StringIndexer(inputCol=\"container_type\", outputCol=\"container_typeIndex\",\n                                      handleInvalid=\"skip\")\ndistribution_portIndexer = StringIndexer(inputCol=\"distribution_port\", outputCol=\"distribution_portIndex\",\n                                         handleInvalid=\"skip\")\nship_registered_inIndexer = StringIndexer(inputCol=\"ship_registered_in\", outputCol=\"ship_registered_inIndex\",\n                                          handleInvalid=\"skip\")\ninbond_entry_typeIndexer = StringIndexer(inputCol=\"inbond_entry_type\", outputCol=\"inbond_entry_typeIndex\",\n                                         handleInvalid=\"skip\")\ncarrier_codeIndexer = StringIndexer(inputCol=\"carrier_code\", outputCol=\"carrier_codeIndex\", handleInvalid=\"skip\")\ncarrier_cityIndexer = StringIndexer(inputCol=\"carrier_city\", outputCol=\"carrier_cityIndex\", handleInvalid=\"skip\")\ncarrier_stateIndexer = StringIndexer(inputCol=\"carrier_state\", outputCol=\"carrier_stateIndex\", handleInvalid=\"skip\")\ncarrier_zipIndexer = StringIndexer(inputCol=\"carrier_zip\", outputCol=\"carrier_zipIndex\", handleInvalid=\"skip\")\nnotify_partyIndexer = StringIndexer(inputCol=\"notify_party\", outputCol=\"notify_partyIndex\", handleInvalid=\"skip\")\nplace_of_receiptIndexer = StringIndexer(inputCol=\"place_of_receipt\", outputCol=\"place_of_receiptIndex\",\n                                        handleInvalid=\"skip\")\n\nlr = LogisticRegression(maxIter=30, labelCol=\"label\", featuresCol=\"features\", regParam=0.3)\n\nlrPipeline = Pipeline(stages=\n                      [consigneeIndexer,\n                       shipperIndexer,\n                       shipmodeIndexer,\n                       gross_weight_lbIndexer,\n                       foreign_portIndexer,\n                       us_portIndexer,\n                       vessel_nameIndexer,\n                       country_of_originIndexer,\n                       container_numberIndexer,\n                       container_typeIndexer,\n                       ship_registered_inIndexer,\n                       carrier_codeIndexer,\n                       carrier_cityIndexer,\n                       notify_partyIndexer,\n                       place_of_receiptIndexer,\n                       zipcodeIndexer,\n                       assembler,\n                       lr]\n                      )\nparamGrid = ParamGridBuilder() \\\n    .addGrid(lr.regParam, [0.1, 0.01]) \\\n    .build()\n\ncrossval = CrossValidator(estimator=lrPipeline,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=BinaryClassificationEvaluator(),\n                          numFolds=2)  # use 3+ folds in practice\n\n# Run cross-validation, and choose the best parameters to demonstate GridSearch in Spark MLlib\nlrModel = crossval.fit(df)\n\ntransformed_df = lrModel.transform(df)\ntransformed_df.createOrReplaceTempView(\"res_view\")\nresults = sqlContext.sql('SELECT prediction, probability, features from res_view')\nresults.show(20)\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-02 18:10:34.525",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395787817_1342897950",
      "id": "20180611-220729_615649684",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\ndrop table IF EXISTS DEV2_ASN.TEST_FEATURES;\nCREATE table DEV2_ASN.TEST_FEATURES AS\n    SELECT \n    SHIPMENTID,\n    STATUS,\n    SHIPMODE,\n    PRODUCT_DESCRIPTION,\n    CONSIGNEE,\n    SHIPPER,\n    ARRIVAL_DATE,\n    GROSS_WEIGHT_LB,\n    GROSS_WEIGHT_KG,\n    FOREIGN_PORT,\n    US_PORT,\n    VESSEL_NAME,\n    COUNTRY_OF_ORIGIN,\n    CONSIGNEE_ADDRESS,\n    SHIPPER_ADDRESS,\n    ZIPCODE,\n    NO_OF_CONTAINERS,\n    CONTAINER_NUMBER,\n    CONTAINER_TYPE,\n    QUANTITY,\n    QUANTITY_UNIT,\n    MEASUREMENT,\n    MEASUREMENT_UNIT,\n    BILL_OF_LADING,\n    HOUSE_VS_MASTER,\n    DISTRIBUTION_PORT,\n    MASTER_BL,\n    VOYAGE_NUMBER,\n    SEAL,\n    SHIP_REGISTERED_IN,\n    INBOND_ENTRY_TYPE,\n    CARRIER_CODE,\n    CARRIER_NAME,\n    CARRIER_CITY,\n    CARRIER_STATE,\n    CARRIER_ZIP,\n    CARRIER_ADDRESS,\n    NOTIFY_PARTY,\n    NOTIFY_ADDRESS,\n    PLACE_OF_RECEIPT,\n    DATE_OF_RECEIPT,\n    CASE\n    WHEN DEV2_ASN.SHIPMENT_IN_TRANSIT.QUANTITY > 10\n    THEN\n        CASE\n            WHEN DEV2_ASN.SHIPMENT_IN_TRANSIT.QUANTITY > 100\n            THEN\n                CASE\n                    WHEN DEV2_ASN.SHIPMENT_IN_TRANSIT.QUANTITY > 1000\n                    THEN 3\n                    ELSE 2\n                END\n            ELSE 1\n\tEND\n    ELSE 0\n    END AS QUANTITY_BIN\n    FROM DEV2_ASN.SHIPMENT_IN_TRANSIT;\n    \n",
      "user": "anonymous",
      "dateUpdated": "2018-12-02 18:10:45.146",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "fontSize": 9.0,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395787818_-279223434",
      "id": "20180304-185514_1167859763",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n## Testing the Code\n\nNow we'll test our code on the `testing` features table:",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:27:08.901",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Testing the Code</h2>\n<p>Now we&rsquo;ll test our code on the <code>testing</code> features table:</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395787818_781413201",
      "id": "20180612-031608_1101973053",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\ntest_data_with_uppercase_schema = splice.df('SELECT * FROM DEV2_ASN.TEST_FEATURES')\n\nnewNames = [\"shipmentid\",\n            \"status\",\n            \"shipmode\",\n            \"product_description\",\n            \"consignee\",\n            \"shipper\",\n            \"arrival_date\",\n            \"gross_weight_lb\",\n            \"gross_weight_kg\",\n            \"foreign_port\",\n            \"us_port\",\n            \"vessel_name\",\n            \"country_of_origin\",\n            \"consignee_address\",\n            \"shipper_address\",\n            \"zipcode\",\n            \"no_of_containers\",\n            \"container_number\",\n            \"container_type\",\n            \"quantity\",\n            \"quantity_unit\",\n            \"measurement\",\n            \"measurement_unit\",\n            \"bill_of_lading\",\n            \"house_vs_master\",\n            \"distribution_port\",\n            \"master_bl\",\n            \"voyage_number\",\n            \"seal\",\n            \"ship_registered_in\",\n            \"inbond_entry_type\",\n            \"carrier_code\",\n            \"carrier_name\",\n            \"carrier_city\",\n            \"carrier_state\",\n            \"carrier_zip\",\n            \"carrier_address\",\n            \"notify_party\",\n            \"notify_address\",\n            \"place_of_receipt\",\n            \"date_of_receipt\",\n            \"quantity_bin\"]\n\ndf= test_data_with_uppercase_schema.toDF(*newNames)\nlrPredictions = lrModel.transform(df)\nlrPredictions.createOrReplaceTempView(\"pred_view\")\nresults = sqlContext.sql('SELECT prediction, probability, features FROM pred_view ORDER BY features')\nresults.show(20)",
      "user": "anonymous",
      "dateUpdated": "2018-12-02 18:10:55.876",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395787818_-162438799",
      "id": "20180612-031749_1939890694",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\nDROP TABLE IF EXISTS DEV2_ASN.PREDICTIONS;\nCREATE TABLE DEV2_ASN.PREDICTIONS (\n    SHIPMENTID VARCHAR(11) NOT NULL PRIMARY KEY,\n    PREDICTION DOUBLE\n    );",
      "user": "anonymous",
      "dateUpdated": "2018-12-02 18:11:03.812",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395787819_8682636",
      "id": "20180205-024429_478433529",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\npredictions = lrPredictions.select(\"SHIPMENTID\", \"PREDICTION\")\npredictions.printSchema()\nsplice.insert(predictions, 'DEV2_ASN.predictions')",
      "user": "anonymous",
      "dateUpdated": "2018-12-02 18:11:13.484",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395787819_-1654523393",
      "id": "20180612-161439_366937269",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\nselect * from DEV2_ASN.predictions;",
      "user": "anonymous",
      "dateUpdated": "2018-12-02 18:11:20.636",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 318.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "SHIPMENTID": "string",
                      "PREDICTION": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            },
            "helium": {}
          }
        },
        "enabled": true,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "fontSize": 9.0,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395787820_-1276369870",
      "id": "20180304-190035_379792791",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Where to Go Next\nThe next notebook in this class, [*Creating Custom Stored Procedures*](/#/notebook/2DWAGKSPM), shows you how to create and use custom stored procedures with Splice Machine.\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-02 20:10:51.105",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "results": {},
        "enabled": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": true,
          "completionSupport": false
        },
        "fontSize": 9.0,
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Where to Go Next</h2>\n<p>The next notebook in this class, <a href=\"/#/notebook/2DWAGKSPM\"><em>Creating Custom Stored Procedures</em></a>, shows you how to create and use custom stored procedures with Splice Machine.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395787820_-1701111727",
      "id": "20180726-013341_1392399267",
      "dateCreated": "2018-11-16 11:16:27.000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Splice Machine Training/For Developers, Part II - Intermediate/i. Machine Learning with Spark MLlib",
  "id": "2DWTYCFTY",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "splicemachine:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}