{
  "paragraphs": [
    {
      "title": "Spark Adapter",
      "text": "%md\n<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/zeppelin/css/zepstyles.css\" />\n\n# Using our Spark Adapter\n\nData Scientists have adopted Spark as the de facto data science platform, and Splice Machine provides an industry leading in-process integration to a Spark cluster. This means data scientists and data engineers can adopt the full power of Spark and manipulate dataframes but also get the power of full ANSI, ACID-compliant SQL.\n\nThe Splice Machine Spark adapter provides:\n\n* A durable, ACID compliant persistence model for Spark Dataframes.\n* Lazy result sets returned as Spark Dataframes.\n* Access to Spark libraries such as MLLib and GraphX.\n* Avoidance of expensive ETL of data from OLTP to OLAP.\n\nThis notebook demonstrates using the Spark Adapter with Python, in these steps:\n\n1. Create the `PySpliceContextClass` to interface with the Python API.\n2. Use the `spark.pyspark` Python interpreter in Zeppelin to create a Spark context.\n3. Create a simple table in Splice Machine.\n4. Create a Spark dataframe and insert that into Splice Machine\n5. Run a simple Splice Machine transaction using the Spark context.\n6. Rollback that transaction using the same context.\n\n<br />\n\n## 1. Create the PySpliceContext Class\n\nOur first step is to use the `PySpark` interpreter to create the `PySpliceContext` class:\n",
      "user": "splice",
      "dateUpdated": "2018-06-01T22:30:08+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "title": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/zeppelin/css/zepstyles.css\" />\n<h1>Using our Spark Adapter</h1>\n<p>Data Scientists have adopted Spark as the de facto data science platform, and Splice Machine provides an industry leading in-process integration to a Spark cluster. This means data scientists and data engineers can adopt the full power of Spark and manipulate dataframes but also get the power of full ANSI, ACID-compliant SQL.</p>\n<p>The Splice Machine Spark adapter provides:</p>\n<ul>\n  <li>A durable, ACID compliant persistence model for Spark Dataframes.</li>\n  <li>Lazy result sets returned as Spark Dataframes.</li>\n  <li>Access to Spark libraries such as MLLib and GraphX.</li>\n  <li>Avoidance of expensive ETL of data from OLTP to OLAP.</li>\n</ul>\n<p>This notebook demonstrates using the Spark Adapter with Python, in these steps:</p>\n<ol>\n  <li>Create the <code>PySpliceContextClass</code> to interface with the Python API.</li>\n  <li>Use the <code>spark.pyspark</code> Python interpreter in Zeppelin to create a Spark context.</li>\n  <li>Create a simple table in Splice Machine.</li>\n  <li>Create a Spark dataframe and insert that into Splice Machine</li>\n  <li>Run a simple Splice Machine transaction using the Spark context.</li>\n  <li>Rollback that transaction using the same context.</li>\n</ol>\n<br />\n<h2>1. Create the PySpliceContext Class</h2>\n<p>Our first step is to use the <code>PySpark</code> interpreter to create the <code>PySpliceContext</code> class:</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1520184315510_979775320",
      "id": "20180118-005945_1825119026",
      "dateCreated": "2018-03-04T17:25:15+0000",
      "dateStarted": "2018-06-01T22:30:08+0000",
      "dateFinished": "2018-06-01T22:30:08+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "focus": true,
      "$$hashKey": "object:17489"
    },
    {
      "title": "PySpliceContext Class",
      "text": "%spark.pyspark\nfrom __future__ import print_function\nimport string\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import Row\nfrom pyspark.sql import DataFrame\n\n#java imports                                                                                                                               \nfrom py4j.java_gateway import java_import\nfrom py4j.java_gateway import JavaGateway, GatewayParameters\n\nclass PySpliceContext:\n    def __init__(self,JDBC_URL,sparkSQLContext):\n        self.jdbcurl = JDBC_URL\n        self.sparkSQLContext = sparkSQLContext\n        self.jvm = self.sparkSQLContext._sc._jvm\n        java_import(self.jvm,\"com.splicemachine.spark.splicemachine.*\")\n        java_import(self.jvm, \"org.apache.spark.sql.execution.datasources.jdbc.{JDBCOptions, JdbcUtils}\")\n        java_import(self.jvm, \"scala.collection.JavaConverters._\")\n        self.context = self.jvm.com.splicemachine.spark.splicemachine.SplicemachineContext(self.jdbcurl)\n    def getConnection(self):\n        return self.context.getConnection()\n        def getConnection(): Connection \n    def tableExists(self, schemaTableName):\n        return self.context.tableExists(schemaTableName)\n    def dropTable(self,schemaName, tableName):\n        return self.context.dropTable(schemaName, tableName)\n    def dropTable(self,schemaTableName):\n        return self.context.dropTable(schemaTableName)\n    def df(self,sql):\n        return DataFrame(self.context.df(sql), self.sparkSQLContext)\n    def insert(self,dataFrame,schemaTableName): \n        return self.context.insert(dataFrame._jdf,schemaTableName)\n    def delete(self, dataFrame, schemaTableName):\n        return self.context.delete(dataFrame._jdf, schemaTableName)\n    def update(self, dataFrame, schemaTableName):\n        return self.context.update(dataFrame._jdf, schemaTableName) \n    def bulkImportHFile(self, dataFrame, schemaTableName, options):\n        return self.context.bulkImportHFile(dataFrame._jdf, schemaTableName, options)\n    def getSchema(self,schemaTableName):\n        return self.context.getSchema(schemaTableName)\n    def createTable(self, tableName, structType, keys, createTableOptions):\n        keyAsString = self.jvm.java.util.ArrayList()\n        for key in keys:\n            keyAsString.append(key)\n        return self.context.createTable(tableName, structType, self.jvm.scala.collection.JavaConversions.asScalaBuffer(keyAsString).toSeq(), createTableOptions)\n",
      "user": "splice",
      "dateUpdated": "2018-06-11T20:02:23+0000",
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "editorHide": false,
        "title": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1520184315510_979775320",
      "id": "20180118-015126_1752791454",
      "dateCreated": "2018-03-04T17:25:15+0000",
      "dateStarted": "2018-06-06T20:30:21+0000",
      "dateFinished": "2018-06-06T20:30:26+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:17490"
    },
    {
      "text": "%md\n\n## 2. Create a Spark context\n\nNext, we use the `PySpliceContext` to create a connection to Splice Machine:",
      "user": "splice",
      "dateUpdated": "2018-06-01T23:17:05+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>2. Create a Spark context</h2>\n<p>Next, we use the <code>PySpliceContext</code> to create a connection to Splice Machine:</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1520184315511_979390571",
      "id": "20180129-153516_1751825816",
      "dateCreated": "2018-03-04T17:25:15+0000",
      "dateStarted": "2018-06-01T23:17:05+0000",
      "dateFinished": "2018-06-01T23:17:05+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:17491"
    },
    {
      "title": "Create Context",
      "text": "%spark.pyspark\n\nsplice = PySpliceContext('jdbc:splice://{FRAMEWORKNAME}-proxy.marathon.mesos:1527/splicedb;user=splice;password=admin', sqlContext)",
      "user": "splice",
      "dateUpdated": "2018-06-11T20:49:19+0000",
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "title": false,
        "results": {},
        "enabled": true,
        "tableHide": false,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1520184315511_979390571",
      "id": "20180118-014113_2141603268",
      "dateCreated": "2018-03-04T17:25:15+0000",
      "dateStarted": "2018-06-11T20:49:20+0000",
      "dateFinished": "2018-06-11T20:49:20+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:17492"
    },
    {
      "text": "%md\n## 3. Create a Simple Table\n\nNow we create a simple table in Splice Machine that we'll subsequently populate:\n",
      "dateUpdated": "2018-03-04T17:25:15+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>3. Create a Simple Table</h2>\n<p>Now we create a simple table in Splice Machine that we&rsquo;ll subsequently populate:</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1520184315511_979390571",
      "id": "20180129-153654_674823995",
      "dateCreated": "2018-03-04T17:25:15+0000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:17493"
    },
    {
      "title": "Create a simple table",
      "text": "%splicemachine\ndrop table if exists foo;\ncreate table foo (I int, F float, V varchar(100), primary key (I));\n",
      "user": "splice",
      "dateUpdated": "2018-06-06T20:30:45+0000",
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "title": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Query executed successfully. Affected rows : 0"
          },
          {
            "type": "TEXT",
            "data": "Query executed successfully. Affected rows : 0"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1520184315511_979390571",
      "id": "20180118-015418_1427121084",
      "dateCreated": "2018-03-04T17:25:15+0000",
      "dateStarted": "2018-06-06T20:30:45+0000",
      "dateFinished": "2018-06-06T20:30:47+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:17494"
    },
    {
      "text": "%md\n## 4. Create a Spark Dataframe and Insert into Splice Machine\n\nThen we use `spark.pyspark` to create a Spark dataframe from some sample data, and insert that into our Splice Machine table.\n\nAfter inserting the data, we do a `select *` to display the contents of the Splice Machine table. ",
      "user": "splice",
      "dateUpdated": "2018-06-01T23:17:37+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>4. Create a Spark Dataframe and Insert into Splice Machine</h2>\n<p>Then we use <code>spark.pyspark</code> to create a Spark dataframe from some sample data, and insert that into our Splice Machine table.</p>\n<p>After inserting the data, we do a <code>select *</code> to display the contents of the Splice Machine table.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1520184315512_977466826",
      "id": "20180129-153818_1709035487",
      "dateCreated": "2018-03-04T17:25:15+0000",
      "dateStarted": "2018-06-01T23:17:37+0000",
      "dateFinished": "2018-06-01T23:17:37+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:17495"
    },
    {
      "title": "Create a dataframe and insert into Splice",
      "text": "%spark.pyspark\nfrom pyspark.sql import Row\nl = [(0,3.14,'Turing'), (1,4.14,'Newell'), (2,5.14,'Simon'), (3,6.14,'Minsky')]\nrdd = sc.parallelize(l)\nrows = rdd.map(lambda x: Row(I=x[0], F=float(x[1]), V=str(x[2])))\nschemaRows = sqlContext.createDataFrame(rows)\nsplice.insert(schemaRows,'foo')\n",
      "user": "splice",
      "dateUpdated": "2018-06-06T20:30:56+0000",
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "title": false,
        "results": {},
        "enabled": true,
        "tableHide": false,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1520184315512_977466826",
      "id": "20180118-015449_13133342",
      "dateCreated": "2018-03-04T17:25:15+0000",
      "dateStarted": "2018-06-06T20:30:56+0000",
      "dateFinished": "2018-06-06T20:31:04+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:17496"
    },
    {
      "text": "%splicemachine\nselect * from foo;",
      "user": "splice",
      "dateUpdated": "2018-06-06T20:33:10+0000",
      "config": {
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 169.006,
              "optionOpen": false
            }
          }
        },
        "enabled": true,
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "I\tF\tV\n0\t3.14\tTuring\n1\t4.14\tNewell\n2\t5.14\tSimon\n3\t6.14\tMinsky\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1520184315512_977466826",
      "id": "20180118-015510_1158645327",
      "dateCreated": "2018-03-04T17:25:15+0000",
      "dateStarted": "2018-06-06T20:33:11+0000",
      "dateFinished": "2018-06-06T20:33:11+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:17497"
    },
    {
      "text": "%md\n\n## 5. Run a Simple Splice Machine Transaction\n\nNow we'll add more data to that table in a transactional context: ",
      "dateUpdated": "2018-03-04T17:25:15+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>5. Run a Simple Splice Machine Transaction</h2>\n<p>Now we&rsquo;ll add more data to that table in a transactional context:</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1520184315512_977466826",
      "id": "20180129-154333_772550654",
      "dateCreated": "2018-03-04T17:25:15+0000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:17498"
    },
    {
      "title": "Transactional Context",
      "text": "%spark.pyspark\nconn = splice.getConnection()\nconn.setAutoCommit(False)\nl = [(4,3.14,'Turing'), (5,4.14,'Newell'), (6,5.14,'Simon'), (7,6.14,'Minsky')]\nrdd = sc.parallelize(l)\nrows = rdd.map(lambda x: Row(I=x[0], F=float(x[1]), V=str(x[2])))\nschemaRows = sqlContext.createDataFrame(rows)\nsplice.insert(schemaRows,'foo')\ndf = splice.df(\"select * from foo\")\ndf.collect\nz.show(df)",
      "user": "splice",
      "dateUpdated": "2018-06-06T20:33:32+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "I\tF\tV\n0\t3.14\tTuring\n1\t4.14\tNewell\n2\t5.14\tSimon\n3\t6.14\tMinsky\n4\t3.14\tTuring\n5\t4.14\tNewell\n6\t5.14\tSimon\n7\t6.14\tMinsky\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1520184315513_977082078",
      "id": "20180118-015525_2011115580",
      "dateCreated": "2018-03-04T17:25:15+0000",
      "dateStarted": "2018-06-06T20:33:33+0000",
      "dateFinished": "2018-06-06T20:33:36+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:17499"
    },
    {
      "text": "%md\n\n## 6. Rollback the transaction\n\nFinally, we'll rollback the transaction we just ran:\n",
      "user": "splice",
      "dateUpdated": "2018-03-05T09:03:25+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>6. Rollback the transaction</h2>\n<p>Finally, we&rsquo;ll rollback the transaction we just ran:</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1520184315513_977082078",
      "id": "20180129-154912_774169413",
      "dateCreated": "2018-03-04T17:25:15+0000",
      "dateStarted": "2018-03-05T09:03:25+0000",
      "dateFinished": "2018-03-05T09:03:25+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:17500"
    },
    {
      "title": "Rollback",
      "text": "%spark.pyspark\nconn.rollback()\ndf = splice.df(\"select * from foo\")\ndf.collect\nz.show(df)",
      "user": "splice",
      "dateUpdated": "2018-06-06T20:34:34+0000",
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "title": false,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 159.006,
              "optionOpen": false
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "I\tF\tV\n0\t3.14\tTuring\n1\t4.14\tNewell\n2\t5.14\tSimon\n3\t6.14\tMinsky\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1520184315513_977082078",
      "id": "20180118-015552_1456308237",
      "dateCreated": "2018-03-04T17:25:15+0000",
      "dateStarted": "2018-06-06T20:34:34+0000",
      "dateFinished": "2018-06-06T20:34:34+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:17501"
    },
    {
      "text": "%md\n## Where to Go Next\n\nThe next notebook in this presentation shows an example of <a href=\"/#/notebook/2DFY9ZKYB\">Using the Spark Machine Learning Library (MLlib) with Splice Machine.</a>\n",
      "dateUpdated": "2018-03-04T17:25:15+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Where to Go Next</h2>\n<p>The next notebook in this presentation shows an example of <a href=\"/#/notebook/2DFY9ZKYB\">Using the Spark Machine Learning Library (MLlib) with Splice Machine.</a></p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1520184315513_977082078",
      "id": "20180122-173624_509126525",
      "dateCreated": "2018-03-04T17:25:15+0000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:17502"
    },
    {
      "text": "%md\n",
      "dateUpdated": "2018-03-04T17:25:15+0000",
      "config": {
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1520184315514_978236324",
      "id": "20180125-142959_1101825868",
      "dateCreated": "2018-03-04T17:25:15+0000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:17503"
    }
  ],
  "name": "3. Splice Deep Dive / 7. Using our Spark Adapter",
  "id": "2D96GBY2P",
  "angularObjects": {
    "2DFPPTC1G:shared_process": [],
    "2DFGVWAYH:shared_process": [],
    "2DH3N72MC:shared_process": [],
    "2DHHZNYTH:shared_process": [],
    "2DFJGZXBJ:shared_process": [],
    "2DGCHUQ86:shared_process": []
  },
  "config": {
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}