{
  "paragraphs": [{
    "title": "Spark Adapter",
    "text": "%md\n<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/zeppelin/css/zepstyles.css\" />\n\n# Using our Spark Adapter\n\nData Scientists have adopted Spark as the de facto data science platform, and Splice Machine provides an industry leading in-process integration to a Spark cluster. This means data scientists and data engineers can adopt the full power of Spark and manipulate dataframes but also get the power of full ANSI, ACID-compliant SQL.\n<sub>Learn more about ANSI [here](https://share.ansi.org/Shared%20Documents/News%20and%20Publications/Brochures/WhatIsANSI_brochure.pdf) and ACID compliance [here](https://www.clustrix.com/bettersql/acid-compliance-means-care/)\n\nThe Splice Machine Spark adapter provides:\n\n* A durable, ACID compliant persistence model for Spark Dataframes.\n* Lazy result sets returned as Spark Dataframes.\n* Access to Spark libraries such as MLLib and GraphX.\n* Avoidance of expensive ETL of data from OLTP to OLAP.\n\nThis notebook will be our first look at writing code with <b>Python</b> using a Spark library called <b>PySpark.</b> \nLearn more about <b>Python</b> [here](https://docs.python.org/3/tutorial/index.html)\nLearn more about <b>Spark</b> and <b>PySpark</b> [here](https://spark.apache.org/docs/latest/api/python/index.html)\n\nThis notebook demonstrates using the Spark Adapter with Python, in these steps:\n\n1. Import the `PySpliceContextClass` to interface with the Python API.\n2. Use the `spark.pyspark` Python interpreter in Zeppelin to create a Spark context.\n3. Create a simple table in Splice Machine.\n4. Create a Spark dataframe and insert that into Splice Machine\n5. Run a simple Splice Machine transaction using the Spark context.\n6. View results in the table\n\n<br />\n\n## 1. Import the PySpliceContext Class\n\nOur first step is to use the `Spark.PySpark` interpreter to import the `PySpliceContext` class:\n",
    "user": "splice",
    "dateUpdated": "2019-03-28T21:03:58+0000",
    "config": {
      "tableHide": false,
      "editorSetting": {
        "language": "markdown",
        "editOnDblClick": true,
        "completionSupport": false
      },
      "colWidth": 12,
      "editorMode": "ace/mode/markdown",
      "editorHide": true,
      "fontSize": 9,
      "title": false,
      "results": {},
      "enabled": true
    },
    "settings": {
      "params": {},
      "forms": {}
    },
    "results": {
      "code": "SUCCESS",
      "msg": [{
        "type": "HTML",
        "data": "<div class=\"markdown-body\">\n<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/zeppelin/css/zepstyles.css\" />\n<h1>Using our Spark Adapter</h1>\n<p>Data Scientists have adopted Spark as the de facto data science platform, and Splice Machine provides an industry leading in-process integration to a Spark cluster. This means data scientists and data engineers can adopt the full power of Spark and manipulate dataframes but also get the power of full ANSI, ACID-compliant SQL.<br/><sub>Learn more about ANSI <a href=\"https://share.ansi.org/Shared%20Documents/News%20and%20Publications/Brochures/WhatIsANSI_brochure.pdf\">here</a> and ACID compliance <a href=\"https://www.clustrix.com/bettersql/acid-compliance-means-care/\">here</a></p>\n<p>The Splice Machine Spark adapter provides:</p>\n<ul>\n  <li>A durable, ACID compliant persistence model for Spark Dataframes.</li>\n  <li>Lazy result sets returned as Spark Dataframes.</li>\n  <li>Access to Spark libraries such as MLLib and GraphX.</li>\n  <li>Avoidance of expensive ETL of data from OLTP to OLAP.</li>\n</ul>\n<p>This notebook will be our first look at writing code with <b>Python</b> using a Spark library called <b>PySpark.</b><br/>Learn more about <b>Python</b> <a href=\"https://docs.python.org/3/tutorial/index.html\">here</a><br/>Learn more about <b>Spark</b> and <b>PySpark</b> <a href=\"https://spark.apache.org/docs/latest/api/python/index.html\">here</a></p>\n<p>This notebook demonstrates using the Spark Adapter with Python, in these steps:</p>\n<ol>\n  <li>Import the <code>PySpliceContextClass</code> to interface with the Python API.</li>\n  <li>Use the <code>spark.pyspark</code> Python interpreter in Zeppelin to create a Spark context.</li>\n  <li>Create a simple table in Splice Machine.</li>\n  <li>Create a Spark dataframe and insert that into Splice Machine</li>\n  <li>Run a simple Splice Machine transaction using the Spark context.</li>\n  <li>View results in the table</li>\n</ol>\n<br />\n<h2>1. Import the PySpliceContext Class</h2>\n<p>Our first step is to use the <code>Spark.PySpark</code> interpreter to import the <code>PySpliceContext</code> class:</p>\n</div>"
      }]
    },
    "apps": [],
    "jobName": "paragraph_1553807038133_-35879446",
    "id": "20180118-005945_1825119026",
    "dateCreated": "2019-03-28T21:03:58+0000",
    "status": "READY",
    "errorMessage": "",
    "progressUpdateIntervalMs": 500,
    "focus": true,
    "$$hashKey": "object:63229"
  }, {
    "title": "PySpliceContext Class",
    "text": "%spark.pyspark\nfrom splicemachine.spark.context import PySpliceContext",
    "user": "splice",
    "dateUpdated": "2019-03-28T21:03:58+0000",
    "config": {
      "tableHide": true,
      "editorSetting": {
        "language": "python",
        "editOnDblClick": false,
        "completionKey": "TAB",
        "completionSupport": true
      },
      "colWidth": 12,
      "editorMode": "ace/mode/python",
      "editorHide": false,
      "fontSize": 9,
      "title": false,
      "results": {},
      "enabled": true
    },
    "settings": {
      "params": {},
      "forms": {}
    },
    "results": {
      "code": "SUCCESS",
      "msg": []
    },
    "apps": [],
    "jobName": "paragraph_1553807038134_1711497931",
    "id": "20180118-015126_1752791454",
    "dateCreated": "2019-03-28T21:03:58+0000",
    "status": "READY",
    "errorMessage": "",
    "progressUpdateIntervalMs": 500,
    "$$hashKey": "object:63230"
  }, {
    "text": "%md\n\n## 2. Create a Spark context\n\nNext, we use the `PySpliceContext` to create a connection to Splice Machine:\n<sub>We can also use `inspect` to see more about what makes the PySpliceContext class",
    "user": "splice",
    "dateUpdated": "2019-03-28T21:03:58+0000",
    "config": {
      "tableHide": false,
      "editorSetting": {
        "language": "markdown",
        "editOnDblClick": true,
        "completionSupport": false
      },
      "colWidth": 12,
      "editorMode": "ace/mode/markdown",
      "editorHide": true,
      "fontSize": 9,
      "results": {},
      "enabled": true
    },
    "settings": {
      "params": {},
      "forms": {}
    },
    "results": {
      "code": "SUCCESS",
      "msg": [{
        "type": "HTML",
        "data": "<div class=\"markdown-body\">\n<h2>2. Create a Spark context</h2>\n<p>Next, we use the <code>PySpliceContext</code> to create a connection to Splice Machine:<br/><sub>We can also use <code>inspect</code> to see more about what makes the PySpliceContext class</p>\n</div>"
      }]
    },
    "apps": [],
    "jobName": "paragraph_1553807038135_-2084584418",
    "id": "20180129-153516_1751825816",
    "dateCreated": "2019-03-28T21:03:58+0000",
    "status": "READY",
    "errorMessage": "",
    "progressUpdateIntervalMs": 500,
    "$$hashKey": "object:63231"
  }, {
    "text": "%spark.pyspark\n%spark.pyspark\nimport inspect\njdbcURL = z.input('Enter your jdbcURL here, which you can find on the bottom of the cloud UI for your cluster')\nsplice = PySpliceContext(jdbcURL, spark)\nprint(inspect.getsource(PySpliceContext))\nprint(splice.jdbcurl)",
    "user": "splice",
    "dateUpdated": "2019-03-28T21:04:46+0000",
    "config": {
      "colWidth": 12,
      "fontSize": 9,
      "enabled": true,
      "results": {},
      "editorSetting": {
        "language": "python",
        "editOnDblClick": false,
        "completionKey": "TAB",
        "completionSupport": true
      },
      "editorMode": "ace/mode/python"
    },
    "settings": {
      "params": {},
      "forms": {}
    },
    "apps": [],
    "jobName": "paragraph_1553807081310_1225367469",
    "id": "20190328-210441_1424044473",
    "dateCreated": "2019-03-28T21:04:41+0000",
    "status": "READY",
    "progressUpdateIntervalMs": 500,
    "focus": true,
    "$$hashKey": "object:65690"
  }, {
    "text": "%md\n## 3. Create a Simple Table\n\nNow we create a simple table in Splice Machine that we'll subsequently populate:\n",
    "user": "splice",
    "dateUpdated": "2019-03-28T21:03:58+0000",
    "config": {
      "tableHide": false,
      "editorSetting": {
        "language": "markdown",
        "editOnDblClick": true
      },
      "colWidth": 12,
      "editorMode": "ace/mode/markdown",
      "editorHide": true,
      "fontSize": 9,
      "results": {},
      "enabled": true
    },
    "settings": {
      "params": {},
      "forms": {}
    },
    "results": {
      "code": "SUCCESS",
      "msg": [{
        "type": "HTML",
        "data": "<div class=\"markdown-body\">\n<h2>3. Create a Simple Table</h2>\n<p>Now we create a simple table in Splice Machine that we&rsquo;ll subsequently populate:</p>\n</div>"
      }]
    },
    "apps": [],
    "jobName": "paragraph_1553807038135_-1844154560",
    "id": "20180129-153654_674823995",
    "dateCreated": "2019-03-28T21:03:58+0000",
    "status": "READY",
    "errorMessage": "",
    "progressUpdateIntervalMs": 500,
    "$$hashKey": "object:63233"
  }, {
    "title": "Create a simple table",
    "text": "%splicemachine\ndrop table if exists foo;\ncreate table foo (I int, F float, V varchar(100), primary key (I));\n",
    "user": "splice",
    "dateUpdated": "2019-03-28T21:03:58+0000",
    "config": {
      "editorSetting": {
        "language": "text",
        "editOnDblClick": false
      },
      "colWidth": 12,
      "editorMode": "ace/mode/text",
      "fontSize": 9,
      "title": false,
      "results": {},
      "enabled": true
    },
    "settings": {
      "params": {},
      "forms": {}
    },
    "apps": [],
    "jobName": "paragraph_1553807038136_-489003221",
    "id": "20180118-015418_1427121084",
    "dateCreated": "2019-03-28T21:03:58+0000",
    "status": "READY",
    "errorMessage": "",
    "progressUpdateIntervalMs": 500,
    "$$hashKey": "object:63234"
  }, {
    "text": "%md\n## 4. Create a Spark Dataframe and Insert into Splice Machine\n\nThen we use `spark.pyspark` to create a Spark dataframe from some sample data, and insert that into our Splice Machine table.\n\nAfter inserting the data, we do a `select *` to display the contents of the Splice Machine table. ",
    "user": "splice",
    "dateUpdated": "2019-03-28T21:03:58+0000",
    "config": {
      "tableHide": false,
      "editorSetting": {
        "language": "markdown",
        "editOnDblClick": true
      },
      "colWidth": 12,
      "editorMode": "ace/mode/markdown",
      "editorHide": true,
      "fontSize": 9,
      "results": {},
      "enabled": true
    },
    "settings": {
      "params": {},
      "forms": {}
    },
    "results": {
      "code": "SUCCESS",
      "msg": [{
        "type": "HTML",
        "data": "<div class=\"markdown-body\">\n<h2>4. Create a Spark Dataframe and Insert into Splice Machine</h2>\n<p>Then we use <code>spark.pyspark</code> to create a Spark dataframe from some sample data, and insert that into our Splice Machine table.</p>\n<p>After inserting the data, we do a <code>select *</code> to display the contents of the Splice Machine table.</p>\n</div>"
      }]
    },
    "apps": [],
    "jobName": "paragraph_1553807038136_-1189735147",
    "id": "20180129-153818_1709035487",
    "dateCreated": "2019-03-28T21:03:58+0000",
    "status": "READY",
    "errorMessage": "",
    "progressUpdateIntervalMs": 500,
    "$$hashKey": "object:63235"
  }, {
    "title": "Create a dataframe and insert into Splice",
    "text": "%spark.pyspark\nfrom pyspark.sql import Row\nl = [(0,3.14,'Turing'), (1,4.14,'Newell'), (2,5.14,'Simon'), (3,6.14,'Minsky')]\nrdd = sc.parallelize(l)\nrows = rdd.map(lambda x: Row(I=x[0], F=float(x[1]), V=str(x[2])))\nschemaRows = sqlContext.createDataFrame(rows)\nsplice.insert(schemaRows,'foo')\n",
    "user": "splice",
    "dateUpdated": "2019-03-28T21:03:58+0000",
    "config": {
      "tableHide": false,
      "editorSetting": {
        "language": "python",
        "editOnDblClick": false,
        "completionKey": "TAB",
        "completionSupport": true
      },
      "colWidth": 12,
      "editorMode": "ace/mode/python",
      "editorHide": false,
      "fontSize": 9,
      "title": false,
      "results": {},
      "enabled": true
    },
    "settings": {
      "params": {},
      "forms": {}
    },
    "results": {
      "code": "SUCCESS",
      "msg": []
    },
    "apps": [],
    "jobName": "paragraph_1553807038137_-701254719",
    "id": "20180118-015449_13133342",
    "dateCreated": "2019-03-28T21:03:58+0000",
    "status": "READY",
    "errorMessage": "",
    "progressUpdateIntervalMs": 500,
    "$$hashKey": "object:63236"
  }, {
    "text": "%splicemachine\nselect * from foo;",
    "user": "splice",
    "dateUpdated": "2019-03-28T21:03:58+0000",
    "config": {
      "editorSetting": {
        "language": "text",
        "editOnDblClick": false
      },
      "colWidth": 12,
      "editorMode": "ace/mode/text",
      "fontSize": 9,
      "results": {
        "0": {
          "graph": {
            "mode": "table",
            "height": 169.006,
            "optionOpen": false,
            "setting": {
              "table": {
                "tableGridState": {},
                "tableColumnTypeState": {
                  "names": {
                    "I": "string",
                    "F": "string",
                    "V": "string"
                  },
                  "updated": false
                },
                "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                "tableOptionValue": {
                  "useFilter": false,
                  "showPagination": false,
                  "showAggregationFooter": false
                },
                "updated": false,
                "initialized": false
              }
            },
            "commonSetting": {}
          }
        }
      },
      "enabled": true
    },
    "settings": {
      "params": {},
      "forms": {}
    },
    "apps": [],
    "jobName": "paragraph_1553807038137_-1985986079",
    "id": "20180118-015510_1158645327",
    "dateCreated": "2019-03-28T21:03:58+0000",
    "status": "READY",
    "errorMessage": "",
    "progressUpdateIntervalMs": 500,
    "$$hashKey": "object:63237"
  }, {
    "text": "%md\n\n## 5. Run a Simple Splice Machine Transaction\n\nNow we'll add more data to that table in a transactional context: ",
    "user": "splice",
    "dateUpdated": "2019-03-28T21:03:58+0000",
    "config": {
      "tableHide": false,
      "editorSetting": {
        "language": "markdown",
        "editOnDblClick": true
      },
      "colWidth": 12,
      "editorMode": "ace/mode/markdown",
      "editorHide": true,
      "fontSize": 9,
      "results": {},
      "enabled": true
    },
    "settings": {
      "params": {},
      "forms": {}
    },
    "results": {
      "code": "SUCCESS",
      "msg": [{
        "type": "HTML",
        "data": "<div class=\"markdown-body\">\n<h2>5. Run a Simple Splice Machine Transaction</h2>\n<p>Now we&rsquo;ll add more data to that table in a transactional context:</p>\n</div>"
      }]
    },
    "apps": [],
    "jobName": "paragraph_1553807038137_1169560007",
    "id": "20180129-154333_772550654",
    "dateCreated": "2019-03-28T21:03:58+0000",
    "status": "READY",
    "errorMessage": "",
    "progressUpdateIntervalMs": 500,
    "$$hashKey": "object:63238"
  }, {
    "title": "Transactional Context",
    "text": "%spark.pyspark\nconn = splice.getConnection()\nconn.setAutoCommit(False)\nl = [(4,3.14,'Turing'), (5,4.14,'Newell'), (6,5.14,'Simon'), (7,6.14,'Minsky')]\nrdd = sc.parallelize(l)\nrows = rdd.map(lambda x: Row(I=x[0], F=float(x[1]), V=str(x[2])))\nschemaRows = sqlContext.createDataFrame(rows)\nsplice.insert(schemaRows,'foo')\ndf = splice.df(\"select * from foo\")\ndf.collect\nz.show(df)",
    "user": "splice",
    "dateUpdated": "2019-03-28T21:03:58+0000",
    "config": {
      "tableHide": false,
      "editorSetting": {
        "language": "text",
        "editOnDblClick": false
      },
      "colWidth": 12,
      "editorMode": "ace/mode/text",
      "editorHide": false,
      "fontSize": 9,
      "title": true,
      "results": {
        "0": {
          "graph": {
            "mode": "table",
            "height": 300,
            "optionOpen": false,
            "setting": {
              "table": {
                "tableGridState": {},
                "tableColumnTypeState": {
                  "names": {
                    "I": "string",
                    "F": "string",
                    "V": "string"
                  },
                  "updated": false
                },
                "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                "tableOptionValue": {
                  "useFilter": false,
                  "showPagination": false,
                  "showAggregationFooter": false
                },
                "updated": false,
                "initialized": false
              }
            },
            "commonSetting": {}
          }
        }
      },
      "enabled": true
    },
    "settings": {
      "params": {},
      "forms": {}
    },
    "apps": [],
    "jobName": "paragraph_1553807038138_1972164092",
    "id": "20180118-015525_2011115580",
    "dateCreated": "2019-03-28T21:03:58+0000",
    "status": "READY",
    "errorMessage": "",
    "progressUpdateIntervalMs": 500,
    "$$hashKey": "object:63239"
  }, {
    "text": "%md\n\n## 6. Rollback the transaction\n\nFinally, we'll rollback the transaction we just ran:\n",
    "user": "splice",
    "dateUpdated": "2019-03-28T21:03:58+0000",
    "config": {
      "tableHide": false,
      "editorSetting": {
        "language": "markdown",
        "editOnDblClick": true
      },
      "colWidth": 12,
      "editorMode": "ace/mode/markdown",
      "editorHide": true,
      "fontSize": 9,
      "results": {},
      "enabled": true
    },
    "settings": {
      "params": {},
      "forms": {}
    },
    "results": {
      "code": "SUCCESS",
      "msg": [{
        "type": "HTML",
        "data": "<div class=\"markdown-body\">\n<h2>6. Rollback the transaction</h2>\n<p>Finally, we&rsquo;ll rollback the transaction we just ran:</p>\n</div>"
      }]
    },
    "apps": [],
    "jobName": "paragraph_1553807038138_-1316307382",
    "id": "20180129-154912_774169413",
    "dateCreated": "2019-03-28T21:03:58+0000",
    "status": "READY",
    "errorMessage": "",
    "progressUpdateIntervalMs": 500,
    "$$hashKey": "object:63240"
  }, {
    "title": "Rollback",
    "text": "%spark.pyspark\nconn.rollback()\ndf = splice.df(\"select * from foo\")\ndf.collect\nz.show(df)",
    "user": "splice",
    "dateUpdated": "2019-03-28T21:03:58+0000",
    "config": {
      "editorSetting": {
        "language": "text",
        "editOnDblClick": false
      },
      "colWidth": 12,
      "editorMode": "ace/mode/text",
      "fontSize": 9,
      "title": false,
      "results": {
        "0": {
          "graph": {
            "mode": "table",
            "height": 159.006,
            "optionOpen": false,
            "setting": {
              "table": {
                "tableGridState": {},
                "tableColumnTypeState": {
                  "names": {
                    "I": "string",
                    "F": "string",
                    "V": "string"
                  },
                  "updated": false
                },
                "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                "tableOptionValue": {
                  "useFilter": false,
                  "showPagination": false,
                  "showAggregationFooter": false
                },
                "updated": false,
                "initialized": false
              }
            },
            "commonSetting": {}
          }
        }
      },
      "enabled": true
    },
    "settings": {
      "params": {},
      "forms": {}
    },
    "apps": [],
    "jobName": "paragraph_1553807038138_-837231635",
    "id": "20180118-015552_1456308237",
    "dateCreated": "2019-03-28T21:03:58+0000",
    "status": "READY",
    "errorMessage": "",
    "progressUpdateIntervalMs": 500,
    "$$hashKey": "object:63241"
  }, {
    "text": "%md\n## Where to Go Next\n\nThe next notebook in this presentation shows an example of <a href=\"/#/notebook/2DFY9ZKYB\">Using the Spark Machine Learning Library (MLlib) with Splice Machine.</a>\n",
    "user": "splice",
    "dateUpdated": "2019-03-28T21:03:58+0000",
    "config": {
      "tableHide": false,
      "editorSetting": {
        "language": "markdown",
        "editOnDblClick": true
      },
      "colWidth": 12,
      "editorMode": "ace/mode/markdown",
      "editorHide": true,
      "fontSize": 9,
      "results": {},
      "enabled": true
    },
    "settings": {
      "params": {},
      "forms": {}
    },
    "results": {
      "code": "SUCCESS",
      "msg": [{
        "type": "HTML",
        "data": "<div class=\"markdown-body\">\n<h2>Where to Go Next</h2>\n<p>The next notebook in this presentation shows an example of <a href=\"/#/notebook/2DFY9ZKYB\">Using the Spark Machine Learning Library (MLlib) with Splice Machine.</a></p>\n</div>"
      }]
    },
    "apps": [],
    "jobName": "paragraph_1553807038139_289734223",
    "id": "20180122-173624_509126525",
    "dateCreated": "2019-03-28T21:03:58+0000",
    "status": "READY",
    "errorMessage": "",
    "progressUpdateIntervalMs": 500,
    "$$hashKey": "object:63242"
  }],
  "name": "3. Splice Deep Dive / 7. Using our Spark Adapter",
  "id": "2E9PTKWMZ",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "splicemachine:shared_process": [],
    "angular:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}
