{
	"paragraphs": [{
		"text": "%md\n<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/zeppelin/css/zepstyles.css\" />\n\n# Machine Learning with Spark MLlib\n\nThis notebook contains code that uses the Machine Learning (<em>ML</em>) Library embedded in Spark, *MLlib*, with the Splice Machine Spark Adapter to realize in-process machine learning. Specifically, the example in this notebook uses data that tracks international shipments to learn, and then predicts how late a shipment will be, based on various factors.\n\nIf you're not familiar with Machine Learning with Spark MLlib, you can learn more about this library here: <a href=\"https://spark.apache.org/docs/latest/ml-guide.html\" target=\"_blank\">https://spark.apache.org/docs/latest/ml-guide.html</a>.\n\nThe remainder of this notebook contains these sections:\n\n* <em>Basic Terminology</em> defines a few major ML terms used in this notebook.\n* <em>About Our Sample Data</em> introduces the shipping data that we use. \n* <em>About our Learning Model</em> describes the learning model method we're using.\n* <em>Creating our Splice Machine Database</em> walks you through setting up our database with our sample data.\n* <em>Creating, Training, and Deploying our Learning Model</em> walks you through our Machine Learning sample code.\n* <em>Program Listing</em> contains a listing of all of the code used in this notebook.\n\n## Basic Terminology\n\nHere's some basic terminology you need to be familiar with to understand the code in this notebook. These descriptions are paraphrased from the <a href=\"https://spark.apache.org/docs/latest/ml-guide.html\" target=\"_blank\">above-mentioned Spark MLlib guide.</a>\n\n<table class=\"splicezep\">\n    <col />\n    <col />\n    <thead>\n        <tr>\n            <th>Term</th>\n            <th>Description</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td class=\"ItalicFont\">DataFrame</td>\n            <td>A DataFrame is a basic Spark SQL concept. A DataFrame is similar to a table in a database: it contains rows of data with columns of varying types. The MLlib operates on datasets that are organized in DataFrames. </td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\">Pipeline</td>\n            <td>In MLlib, you chain together a sequence of algorithms, or <em>stages</em> that operate on your DataFrame into a <em>pipeline</em> that learns.</td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\">Transformer</td>\n            <td>An algorithm that transforms a DataFrame into another DataFrame. Each transformer implements a method named <code>transform</code> that converts the DataFrame, typically by appending additional columns to it. A <em>model</em> is a kind of transformer.</td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\">Estimator</td>\n            <td>A learning algorithm that trains or <em>fits</em> on a DataFrame and produces a <code>model</code>. Each estimator implements a method named <code>fit</code> that produces a model.</td>\n        </tr>\n    </tbody>\n</table>\n    \n\n## About our Sample Data\n\nWe've obtained some actual shipping data that tracks international shipments between ports, and have imported that data into a Splice Machine database that we've named `ASN.` The tables of interest are named `SHIPMENT_IN_TRANSIT` and `SHIPMENT_HISTORY;` you'll see these table used in the sample code below. We also create a database table named `Features` that forms the basis of the DataFrame we use for our learning model; this is the table you'll see featured in this notebook's code. The idea of this model is to predict, in real-time, how late a specific shipment will be, based on past data and other factors. Over time, as more data is processed by the model, the predictions become more accurate. \n\n## About our Learning Model\n\nWe use a Logistic Regression *estimator* as the final stage in our pipeline to produce a Logistic Regression Model of lateness from our data, and then deploy that model on a dataset to predict lateness.\n\nThe estimator operates on data that is formatted into vectors of integers. Since most of the fields in  our input dataset contain string values, we need to convert any data that will be used by the estimator into this format, as you'll see below. ",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:24:51+0000",
		"config": {
			"tableHide": false,
			"editorSetting": {
				"language": "markdown",
				"editOnDblClick": true
			},
			"colWidth": 12,
			"editorMode": "ace/mode/markdown",
			"editorHide": true,
			"results": {},
			"enabled": true,
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"results": {
			"code": "SUCCESS",
			"msg": [{
				"type": "HTML",
				"data": "<div class=\"markdown-body\">\n<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/zeppelin/css/zepstyles.css\" />\n<h1>Machine Learning with Spark MLlib</h1>\n<p>This notebook contains code that uses the Machine Learning (<em>ML</em>) Library embedded in Spark, <em>MLlib</em>, with the Splice Machine Spark Adapter to realize in-process machine learning. Specifically, the example in this notebook uses data that tracks international shipments to learn, and then predicts how late a shipment will be, based on various factors.</p>\n<p>If you&rsquo;re not familiar with Machine Learning with Spark MLlib, you can learn more about this library here: <a href=\"https://spark.apache.org/docs/latest/ml-guide.html\" target=\"_blank\"><a href=\"https://spark.apache.org/docs/latest/ml-guide.html\">https://spark.apache.org/docs/latest/ml-guide.html</a></a>.</p>\n<p>The remainder of this notebook contains these sections:</p>\n<ul>\n  <li><em>Basic Terminology</em> defines a few major ML terms used in this notebook.</li>\n  <li><em>About Our Sample Data</em> introduces the shipping data that we use.</li>\n  <li><em>About our Learning Model</em> describes the learning model method we&rsquo;re using.</li>\n  <li><em>Creating our Splice Machine Database</em> walks you through setting up our database with our sample data.</li>\n  <li><em>Creating, Training, and Deploying our Learning Model</em> walks you through our Machine Learning sample code.</li>\n  <li><em>Program Listing</em> contains a listing of all of the code used in this notebook.</li>\n</ul>\n<h2>Basic Terminology</h2>\n<p>Here&rsquo;s some basic terminology you need to be familiar with to understand the code in this notebook. These descriptions are paraphrased from the <a href=\"https://spark.apache.org/docs/latest/ml-guide.html\" target=\"_blank\">above-mentioned Spark MLlib guide.</a></p>\n<table class=\"splicezep\">\n    <col />\n    <col />\n    <thead>\n        <tr>\n            <th>Term</th>\n            <th>Description</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td class=\"ItalicFont\">DataFrame</td>\n            <td>A DataFrame is a basic Spark SQL concept. A DataFrame is similar to a table in a database: it contains rows of data with columns of varying types. The MLlib operates on datasets that are organized in DataFrames. </td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\">Pipeline</td>\n            <td>In MLlib, you chain together a sequence of algorithms, or <em>stages</em> that operate on your DataFrame into a <em>pipeline</em> that learns.</td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\">Transformer</td>\n            <td>An algorithm that transforms a DataFrame into another DataFrame. Each transformer implements a method named <code>transform</code> that converts the DataFrame, typically by appending additional columns to it. A <em>model</em> is a kind of transformer.</td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\">Estimator</td>\n            <td>A learning algorithm that trains or <em>fits</em> on a DataFrame and produces a <code>model</code>. Each estimator implements a method named <code>fit</code> that produces a model.</td>\n        </tr>\n    </tbody>\n</table>\n<h2>About our Sample Data</h2>\n<p>We&rsquo;ve obtained some actual shipping data that tracks international shipments between ports, and have imported that data into a Splice Machine database that we&rsquo;ve named <code>ASN.</code> The tables of interest are named <code>SHIPMENT_IN_TRANSIT</code> and <code>SHIPMENT_HISTORY;</code> you&rsquo;ll see these table used in the sample code below. We also create a database table named <code>Features</code> that forms the basis of the DataFrame we use for our learning model; this is the table you&rsquo;ll see featured in this notebook&rsquo;s code. The idea of this model is to predict, in real-time, how late a specific shipment will be, based on past data and other factors. Over time, as more data is processed by the model, the predictions become more accurate. </p>\n<h2>About our Learning Model</h2>\n<p>We use a Logistic Regression <em>estimator</em> as the final stage in our pipeline to produce a Logistic Regression Model of lateness from our data, and then deploy that model on a dataset to predict lateness.</p>\n<p>The estimator operates on data that is formatted into vectors of integers. Since most of the fields in our input dataset contain string values, we need to convert any data that will be used by the estimator into this format, as you&rsquo;ll see below.</p>\n</div>"
			}]
		},
		"apps": [],
		"jobName": "paragraph_1520187001419_1111553724",
		"id": "20180129-160012_924943773",
		"dateCreated": "2018-03-04T18:10:01+0000",
		"dateStarted": "2018-10-08T22:24:51+0000",
		"dateFinished": "2018-10-08T22:24:54+0000",
		"status": "FINISHED",
		"progressUpdateIntervalMs": 500,
		"focus": true,
		"$$hashKey": "object:61602"
	}, {
		"text": "%md\n##  Create our Splice Machine Database\n\nBefore working with the MLlib, we need to create a Splice Machine database that contains the shipping data we're using. We:\n\n* Create the schema and tables\n* Import the data\n* Create our features table\n\n### Create the Schema and Tables\n\nFirst we create our new schema and make it our default schema, and then we create our tables for the shipment_in_transit and shipment_history data that we'll be importing.",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:24:54+0000",
		"config": {
			"tableHide": false,
			"editorSetting": {
				"language": "markdown",
				"editOnDblClick": true
			},
			"colWidth": 12,
			"editorMode": "ace/mode/markdown",
			"editorHide": true,
			"results": {},
			"enabled": true,
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"results": {
			"code": "SUCCESS",
			"msg": [{
				"type": "HTML",
				"data": "<div class=\"markdown-body\">\n<h2>Create our Splice Machine Database</h2>\n<p>Before working with the MLlib, we need to create a Splice Machine database that contains the shipping data we&rsquo;re using. We:</p>\n<ul>\n  <li>Create the schema and tables</li>\n  <li>Import the data</li>\n  <li>Create our features table</li>\n</ul>\n<h3>Create the Schema and Tables</h3>\n<p>First we create our new schema and make it our default schema, and then we create our tables for the shipment_in_transit and shipment_history data that we&rsquo;ll be importing.</p>\n</div>"
			}]
		},
		"apps": [],
		"jobName": "paragraph_1520187001420_1109629980",
		"id": "20180202-100849_1233744001",
		"dateCreated": "2018-03-04T18:10:01+0000",
		"dateStarted": "2018-10-08T22:24:54+0000",
		"dateFinished": "2018-10-08T22:24:54+0000",
		"status": "FINISHED",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61603"
	}, {
		"text": "%spark\n  println(\"Please copy and paste your JDBC URL. You can find it at the bottom right of your cluster dashboard\")\n  val defaultJDBCURL = z.input(\"JDBCurl\",\"\"\"jdbc:splice://{FRAMEWORKNAME}-proxy.marathon.mesos:1527/splicedb;user=splice;password=admin\"\"\").toString\n  val localJDBCURL = \"\"\"jdbc:splice://localhost:1527/splicedb;user=splice;password=admin\"\"\"\n",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:39:06+0000",
		"config": {
			"tableHide": false,
			"editorSetting": {
				"language": "scala",
				"editOnDblClick": false,
				"completionKey": "TAB",
				"completionSupport": true
			},
			"colWidth": 12,
			"editorMode": "ace/mode/scala",
			"editorHide": true,
			"results": {},
			"enabled": true,
			"fontSize": 9
		},
		"settings": {
			"params": {
				"JDBCurl": "jdbc:splice://{FRAMEWORKNAME}-proxy.marathon.mesos:1527/splicedb;user=splice;password=admin",
				"JDBCurl Scala": "jdbc:splice://{FRAMEWORKNAME}-proxy.marathon.mesos:1527/splicedb;user=splice;password=admin;useSpark=true",
				"JDBCURL Scala": "jdbc:splice:/{FRAMEWORKNAME}-proxy.marathon.mesos:1527/splicedb;user=splice;password=admin",
				"JDBCxurl": "jdbc:splice://{FRAMEWORKNAME}-proxy.marathon.mesos:1527/splicedb;user=splice;password=admin;useSpark=true"
			},
			"forms": {
				"JDBCurl": {
					"type": "TextBox",
					"name": "JDBCurl",
					"displayName": "JDBCurl",
					"defaultValue": "jdbc:splice://{FRAMEWORKNAME}-proxy.marathon.mesos:1527/splicedb;user=splice;password=admin",
					"hidden": false,
					"$$hashKey": "object:61809"
				}
			}
		},
		"results": {
			"code": "SUCCESS",
			"msg": [{
				"type": "TEXT",
				"data": "Please copy and paste your JDBC URL. You can find it at the bottom right of your cluster dashboard\nwarning: there was one deprecation warning; re-run with -deprecation for details\ndefaultJDBCURL: String = jdbc:splice://{FRAMEWORKNAME}-proxy.marathon.mesos:1527/splicedb;user=splice;password=admin\nlocalJDBCURL: String = jdbc:splice://localhost:1527/splicedb;user=splice;password=admin\n"
			}]
		},
		"apps": [],
		"jobName": "paragraph_1520187001420_1109629980",
		"id": "20180215-062654_2077965041",
		"dateCreated": "2018-03-04T18:10:01+0000",
		"dateStarted": "2018-10-08T22:38:50+0000",
		"dateFinished": "2018-10-08T22:38:52+0000",
		"status": "FINISHED",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61604"
	}, {
		"text": "%splicemachine\n\nCREATE SCHEMA ASN;\nSET SCHEMA ASN;\n",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:24:56+0000",
		"config": {
			"colWidth": 4,
			"editorMode": "ace/mode/sql",
			"results": {},
			"enabled": true,
			"editorSetting": {
				"language": "sql",
				"editOnDblClick": false
			},
			"tableHide": true,
			"editorHide": false,
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"results": {
			"code": "SUCCESS",
			"msg": [{
				"type": "TEXT",
				"data": "Query executed successfully. Affected rows : 0"
			}, {
				"type": "TEXT",
				"data": "Query executed successfully. Affected rows : 0"
			}]
		},
		"apps": [],
		"jobName": "paragraph_1520187001420_1109629980",
		"id": "20180202-101539_620068341",
		"dateCreated": "2018-03-04T18:10:01+0000",
		"dateStarted": "2018-10-08T22:24:56+0000",
		"dateFinished": "2018-10-08T22:24:59+0000",
		"status": "FINISHED",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61605"
	}, {
		"text": "%splicemachine\n\nDROP TABLE IF EXISTS SHIPMENT_IN_TRANSIT;\nCREATE TABLE SHIPMENT_IN_TRANSIT(\n    SHIPMENTID VARCHAR(11) NOT NULL PRIMARY KEY,\n    STATUS VARCHAR(50),\n    SHIPMODE VARCHAR(30),\n    PRODUCT_DESCRIPTION VARCHAR(500),\n    CONSIGNEE VARCHAR(200),\n    SHIPPER VARCHAR(100),\n    ARRIVAL_DATE TIMESTAMP,\n    GROSS_WEIGHT_LB INTEGER,\n    GROSS_WEIGHT_KG INTEGER,\n    FOREIGN_PORT VARCHAR(50),\n    US_PORT VARCHAR(50),\n    VESSEL_NAME VARCHAR(40),\n    COUNTRY_OF_ORIGIN VARCHAR(40),\n    CONSIGNEE_ADDRESS VARCHAR(150),\n    SHIPPER_ADDRESS VARCHAR(150),\n    ZIPCODE VARCHAR(20),\n    NO_OF_CONTAINERS INTEGER,\n    CONTAINER_NUMBER VARCHAR(200),\n    CONTAINER_TYPE VARCHAR(80),\n    QUANTITY INTEGER,\n    QUANTITY_UNIT VARCHAR(10),\n    MEASUREMENT INTEGER,\n    MEASUREMENT_UNIT VARCHAR(5),\n    BILL_OF_LADING VARCHAR(20),\n    HOUSE_VS_MASTER CHAR(1),\n    DISTRIBUTION_PORT VARCHAR(40),\n    MASTER_BL VARCHAR(20),\n    VOYAGE_NUMBER VARCHAR(10),\n    SEAL VARCHAR(300),\n    SHIP_REGISTERED_IN VARCHAR(40),\n    INBOND_ENTRY_TYPE VARCHAR(30),\n    CARRIER_CODE VARCHAR(10),\n    CARRIER_NAME VARCHAR(40),\n    CARRIER_CITY VARCHAR(40),\n    CARRIER_STATE VARCHAR(10),\n    CARRIER_ZIP VARCHAR(10),\n    CARRIER_ADDRESS VARCHAR(200),\n    NOTIFY_PARTY VARCHAR(50),\n    NOTIFY_ADDRESS VARCHAR(200),\n    PLACE_OF_RECEIPT VARCHAR(50),\n    DATE_OF_RECEIPT TIMESTAMP\n    );\n\n",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:24:59+0000",
		"config": {
			"colWidth": 4,
			"editorMode": "ace/mode/sql",
			"results": {},
			"enabled": true,
			"editorSetting": {
				"language": "sql",
				"editOnDblClick": false
			},
			"tableHide": false,
			"editorHide": false,
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"apps": [],
		"jobName": "paragraph_1520187001421_1109245231",
		"id": "20180202-102107_567153190",
		"dateCreated": "2018-03-04T18:10:01+0000",
		"dateStarted": "2018-10-08T22:24:59+0000",
		"dateFinished": "2018-10-08T22:25:02+0000",
		"status": "FINISHED",
		"errorMessage": "",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61606"
	}, {
		"text": "%splicemachine\nDROP TABLE IF EXISTS SHIPMENT_HISTORY;\nCREATE TABLE SHIPMENT_HISTORY(\n    SHIPMENTID VARCHAR(11) NOT NULL PRIMARY KEY,\n    STATUS VARCHAR(50),\n    SHIPMODE VARCHAR(30),\n    PRODUCT_DESCRIPTION VARCHAR(500),\n    CONSIGNEE VARCHAR(200),\n    SHIPPER VARCHAR(100),\n    ARRIVAL_DATE TIMESTAMP,\n    GROSS_WEIGHT_LB INTEGER,\n    GROSS_WEIGHT_KG INTEGER,\n    FOREIGN_PORT VARCHAR(50),\n    US_PORT VARCHAR(50),\n    VESSEL_NAME VARCHAR(40),\n    COUNTRY_OF_ORIGIN VARCHAR(40),\n    CONSIGNEE_ADDRESS VARCHAR(150),\n    SHIPPER_ADDRESS VARCHAR(150),\n    ZIPCODE VARCHAR(20),\n    NO_OF_CONTAINERS INTEGER,\n    CONTAINER_NUMBER VARCHAR(200),\n    CONTAINER_TYPE VARCHAR(80),\n    QUANTITY INTEGER,\n    QUANTITY_UNIT VARCHAR(10),\n    MEASUREMENT INTEGER,\n    MEASUREMENT_UNIT VARCHAR(5),\n    BILL_OF_LADING VARCHAR(20),\n    HOUSE_VS_MASTER CHAR(1),\n    DISTRIBUTION_PORT VARCHAR(40),\n    MASTER_BL VARCHAR(20),\n    VOYAGE_NUMBER VARCHAR(10),\n    SEAL VARCHAR(300),\n    SHIP_REGISTERED_IN VARCHAR(40),\n    INBOND_ENTRY_TYPE VARCHAR(30),\n    CARRIER_CODE VARCHAR(10),\n    CARRIER_NAME VARCHAR(40),\n    CARRIER_CITY VARCHAR(40),\n    CARRIER_STATE VARCHAR(10),\n    CARRIER_ZIP VARCHAR(10),\n    CARRIER_ADDRESS VARCHAR(200),\n    NOTIFY_PARTY VARCHAR(50),\n    NOTIFY_ADDRESS VARCHAR(200),\n    PLACE_OF_RECEIPT VARCHAR(50),\n    DATE_OF_RECEIPT TIMESTAMP\n);\n",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:25:02+0000",
		"config": {
			"colWidth": 4,
			"editorMode": "ace/mode/sql",
			"results": {},
			"enabled": true,
			"editorSetting": {
				"language": "sql",
				"editOnDblClick": false
			},
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"apps": [],
		"jobName": "paragraph_1520187001421_1109245231",
		"id": "20180202-102130_85303245",
		"dateCreated": "2018-03-04T18:10:01+0000",
		"dateStarted": "2018-10-08T22:25:03+0000",
		"dateFinished": "2018-10-08T22:25:05+0000",
		"status": "FINISHED",
		"errorMessage": "",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61607"
	}, {
		"text": "%md\n### Import the Data\n\nNext we import the shipping data, which is in csv format, into our Splice Machine database.\n",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:25:05+0000",
		"config": {
			"tableHide": false,
			"editorSetting": {
				"language": "markdown",
				"editOnDblClick": true
			},
			"colWidth": 12,
			"editorMode": "ace/mode/markdown",
			"editorHide": true,
			"results": {},
			"enabled": true,
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"results": {
			"code": "SUCCESS",
			"msg": [{
				"type": "HTML",
				"data": "<div class=\"markdown-body\">\n<h3>Import the Data</h3>\n<p>Next we import the shipping data, which is in csv format, into our Splice Machine database.</p>\n</div>"
			}]
		},
		"apps": [],
		"jobName": "paragraph_1520187001421_1109245231",
		"id": "20180202-104354_1962297047",
		"dateCreated": "2018-03-04T18:10:01+0000",
		"dateStarted": "2018-10-08T22:25:05+0000",
		"dateFinished": "2018-10-08T22:25:05+0000",
		"status": "FINISHED",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61608"
	}, {
		"text": "%splicemachine\ncall SYSCS_UTIL.IMPORT_DATA (\n     'ASN',\n     'SHIPMENT_IN_TRANSIT',\n     null,\n     's3a://splice-demo/shipment/shipment_in_transit.csv',\n     '|',\n     null,\n     'yyyy-MM-dd HH:mm:ss.SSSSSS',\n     'yyyy-MM-dd',\n     null,\n     -1,\n     '/tmp',\n     true, null);",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:25:05+0000",
		"config": {
			"colWidth": 6,
			"editorMode": "ace/mode/sql",
			"results": {
				"0": {
					"graph": {
						"mode": "table",
						"height": 300,
						"optionOpen": false,
						"setting": {
							"table": {
								"tableGridState": {},
								"tableColumnTypeState": {
									"names": {
										"rowsImported": "string",
										"failedRows": "string",
										"files": "string",
										"dataSize": "string",
										"failedLog": "string"
									},
									"updated": false
								},
								"tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
								"tableOptionValue": {
									"useFilter": false,
									"showPagination": false,
									"showAggregationFooter": false
								},
								"updated": false,
								"initialized": false
							}
						},
						"commonSetting": {}
					}
				}
			},
			"enabled": true,
			"editorSetting": {
				"language": "sql",
				"editOnDblClick": false
			},
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"apps": [],
		"jobName": "paragraph_1520187001422_1110399478",
		"id": "20180202-104501_1686524282",
		"dateCreated": "2018-03-04T18:10:01+0000",
		"dateStarted": "2018-10-08T22:25:05+0000",
		"dateFinished": "2018-10-08T22:25:13+0000",
		"status": "FINISHED",
		"errorMessage": "",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61609"
	}, {
		"text": "%splicemachine\ncall SYSCS_UTIL.IMPORT_DATA (\n     'ASN',\n     'SHIPMENT_HISTORY',\n     null,\n     's3a://splice-demo/shipment/shipment_history.csv',\n     '|',\n     null,\n     'yyyy-MM-dd HH:mm:ss.SSSSSS',\n     'yyyy-MM-dd',\n     null,\n     -1,\n     '/tmp',\n     true, null);",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:25:13+0000",
		"config": {
			"colWidth": 6,
			"editorMode": "ace/mode/sql",
			"results": {
				"0": {
					"graph": {
						"mode": "table",
						"height": 300,
						"optionOpen": false,
						"setting": {
							"table": {
								"tableGridState": {},
								"tableColumnTypeState": {
									"names": {
										"rowsImported": "string",
										"failedRows": "string",
										"files": "string",
										"dataSize": "string",
										"failedLog": "string"
									},
									"updated": false
								},
								"tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
								"tableOptionValue": {
									"useFilter": false,
									"showPagination": false,
									"showAggregationFooter": false
								},
								"updated": false,
								"initialized": false
							}
						},
						"commonSetting": {}
					},
					"helium": {}
				}
			},
			"enabled": true,
			"editorSetting": {
				"language": "sql",
				"editOnDblClick": false
			},
			"tableHide": false,
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"apps": [],
		"jobName": "paragraph_1520187001422_1110399478",
		"id": "20180202-104531_242494964",
		"dateCreated": "2018-03-04T18:10:01+0000",
		"dateStarted": "2018-10-08T22:25:13+0000",
		"dateFinished": "2018-10-08T22:25:16+0000",
		"status": "FINISHED",
		"errorMessage": "",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61610"
	}, {
		"text": "%md\n### Create our Features Table\n\nWe create a features table in our database that we use with our learning model. We add three computed fields in the `features` table that are important to our model:\n\n* `quantity_bin` categorizes shipping quantities into bins, to improve learning accuracy \n* `lateness` computes how many days late a shipment was\n* `label` categorizes lateness into one of four values:\n\n<table class=\"spliceZepNoBorder\" style=\"margin: 0 0 100px 50px;\">\n    <tbody>\n            <tr><td>0</td><td>0 days late</td></tr>\n            <tr><td>1</td><td>1-5 days late</td></tr>\n            <tr><td>2</td><td>5-10 days late</td></tr>\n            <tr><td>3</td><td>10 days or more late</td></tr>\n    </tbody>\n</table>",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:25:16+0000",
		"config": {
			"tableHide": false,
			"editorSetting": {
				"language": "markdown",
				"editOnDblClick": true
			},
			"colWidth": 12,
			"editorMode": "ace/mode/markdown",
			"editorHide": true,
			"results": {},
			"enabled": true,
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"results": {
			"code": "SUCCESS",
			"msg": [{
				"type": "HTML",
				"data": "<div class=\"markdown-body\">\n<h3>Create our Features Table</h3>\n<p>We create a features table in our database that we use with our learning model. We add three computed fields in the <code>features</code> table that are important to our model:</p>\n<ul>\n  <li><code>quantity_bin</code> categorizes shipping quantities into bins, to improve learning accuracy</li>\n  <li><code>lateness</code> computes how many days late a shipment was</li>\n  <li><code>label</code> categorizes lateness into one of four values:</li>\n</ul>\n<table class=\"spliceZepNoBorder\" style=\"margin: 0 0 100px 50px;\">\n    <tbody>\n            <tr><td>0</td><td>0 days late</td></tr>\n            <tr><td>1</td><td>1-5 days late</td></tr>\n            <tr><td>2</td><td>5-10 days late</td></tr>\n            <tr><td>3</td><td>10 days or more late</td></tr>\n    </tbody>\n</table>\n</div>"
			}]
		},
		"apps": [],
		"jobName": "paragraph_1520187001422_1110399478",
		"id": "20180202-104618_659628734",
		"dateCreated": "2018-03-04T18:10:01+0000",
		"dateStarted": "2018-10-08T22:25:16+0000",
		"dateFinished": "2018-10-08T22:25:16+0000",
		"status": "FINISHED",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61611"
	}, {
		"text": "%splicemachine\ndrop table IF EXISTS ASN.FEATURES;\nCREATE table ASN.FEATURES AS\n    SELECT\n        SHIPMENTID,\n        SHIPMODE,\n        CONSIGNEE,\n        SHIPPER,\n        ARRIVAL_DATE,\n        GROSS_WEIGHT_LB,\n        FOREIGN_PORT,\n        US_PORT,\n        VESSEL_NAME,\n        COUNTRY_OF_ORIGIN,\n        CONSIGNEE_ADDRESS,\n        SHIPPER_ADDRESS,\n        ZIPCODE,\n        NO_OF_CONTAINERS,\n        CONTAINER_NUMBER,\n        CONTAINER_TYPE,\n        QUANTITY,\n        QUANTITY_UNIT,\n        MEASUREMENT,\n        MEASUREMENT_UNIT,\n        BILL_OF_LADING,\n        HOUSE_VS_MASTER,\n        DISTRIBUTION_PORT,\n        MASTER_BL,\n        VOYAGE_NUMBER,\n        SEAL,\n        SHIP_REGISTERED_IN,\n        INBOND_ENTRY_TYPE,\n        CARRIER_CODE,\n        CARRIER_NAME,\n        CARRIER_CITY,\n        CARRIER_STATE,\n        CARRIER_ZIP,\n        CARRIER_ADDRESS,\n        NOTIFY_PARTY,\n        NOTIFY_ADDRESS,\n        PLACE_OF_RECEIPT,\n        DATE_OF_RECEIPT,\n        CASE\n        WHEN ASN.SHIPMENT_HISTORY.QUANTITY > 10\n        THEN\n            CASE\n                WHEN ASN.SHIPMENT_HISTORY.QUANTITY > 100\n                THEN\n                    CASE\n                        WHEN ASN.SHIPMENT_HISTORY.QUANTITY > 1000\n                        THEN 3\n                        ELSE 2\n                    END\n                ELSE 1\n    \tEND\n        ELSE 0\n        END AS QUANTITY_BIN,\n        ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - ASN.SHIPMENT_HISTORY.ARRIVAL_DATE as LATENESS,\n        CASE\n        WHEN   ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - ASN.SHIPMENT_HISTORY.ARRIVAL_DATE > 0\n        THEN\n            CASE\n                WHEN   ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - ASN.SHIPMENT_HISTORY.ARRIVAL_DATE > 5\n                THEN\n                    CASE\n                        WHEN   ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - ASN.SHIPMENT_HISTORY.ARRIVAL_DATE > 10\n                        THEN 3\n                        ELSE 2\n                    END\n                ELSE 1\n    \tEND\n        ELSE 0\n        END AS LABEL\n    FROM ASN.SHIPMENT_HISTORY",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:25:16+0000",
		"config": {
			"tableHide": false,
			"editorSetting": {
				"language": "sql",
				"editOnDblClick": false
			},
			"colWidth": 12,
			"editorMode": "ace/mode/sql",
			"editorHide": false,
			"results": {},
			"enabled": true,
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"apps": [],
		"jobName": "paragraph_1520187001423_1110014729",
		"id": "20180202-104614_1527675689",
		"dateCreated": "2018-03-04T18:10:01+0000",
		"dateStarted": "2018-10-08T22:25:16+0000",
		"dateFinished": "2018-10-08T22:25:22+0000",
		"status": "FINISHED",
		"errorMessage": "",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61612"
	}, {
		"text": "%splicemachine\ndrop table IF EXISTS ASN.FEATURES;\nCREATE table ASN.FEATURES AS\n    SELECT \n    SHIPMENTID,\n    STATUS,\n    SHIPMODE,\n    PRODUCT_DESCRIPTION,\n    CONSIGNEE,\n    SHIPPER,\n    ARRIVAL_DATE,\n    GROSS_WEIGHT_LB,\n    GROSS_WEIGHT_KG,\n    FOREIGN_PORT,\n    US_PORT,\n    VESSEL_NAME,\n    COUNTRY_OF_ORIGIN,\n    CONSIGNEE_ADDRESS,\n    SHIPPER_ADDRESS,\n    ZIPCODE,\n    NO_OF_CONTAINERS,\n    CONTAINER_NUMBER,\n    CONTAINER_TYPE,\n    QUANTITY,\n    QUANTITY_UNIT,\n    MEASUREMENT,\n    MEASUREMENT_UNIT,\n    BILL_OF_LADING,\n    HOUSE_VS_MASTER,\n    DISTRIBUTION_PORT,\n    MASTER_BL,\n    VOYAGE_NUMBER,\n    SEAL,\n    SHIP_REGISTERED_IN,\n    INBOND_ENTRY_TYPE,\n    CARRIER_CODE,\n    CARRIER_NAME,\n    CARRIER_CITY,\n    CARRIER_STATE,\n    CARRIER_ZIP,\n    CARRIER_ADDRESS,\n    NOTIFY_PARTY,\n    NOTIFY_ADDRESS,\n    PLACE_OF_RECEIPT,\n    DATE_OF_RECEIPT,\n    CASE\n    WHEN ASN.SHIPMENT_HISTORY.QUANTITY > 10\n    THEN\n        CASE\n            WHEN ASN.SHIPMENT_HISTORY.QUANTITY > 100\n            THEN\n                CASE\n                    WHEN ASN.SHIPMENT_HISTORY.QUANTITY > 1000\n                    THEN 3\n                    ELSE 2\n                END\n            ELSE 1\n    END\n    ELSE 0\n    END AS QUANTITY_BIN,\n    ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - ASN.SHIPMENT_HISTORY.ARRIVAL_DATE as LATENESS,\n    CASE\n    WHEN  ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - ASN.SHIPMENT_HISTORY.ARRIVAL_DATE > 0\n    THEN\n        CASE\n            WHEN  ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - ASN.SHIPMENT_HISTORY.ARRIVAL_DATE > 5\n            THEN\n                CASE\n                    WHEN  ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - ASN.SHIPMENT_HISTORY.ARRIVAL_DATE > 10\n                    THEN 3\n                    ELSE 2\n                END\n            ELSE 1\n    END\n    ELSE 0\n    END AS LABEL\nFROM ASN.SHIPMENT_HISTORY ",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:25:22+0000",
		"config": {
			"colWidth": 12,
			"editorMode": "ace/mode/sql",
			"results": {},
			"enabled": true,
			"editorSetting": {
				"language": "sql",
				"editOnDblClick": false
			},
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"apps": [],
		"jobName": "paragraph_1520187001423_1110014729",
		"id": "20180205-022847_1435938411",
		"dateCreated": "2018-03-04T18:10:01+0000",
		"dateStarted": "2018-10-08T22:25:22+0000",
		"dateFinished": "2018-10-08T22:25:31+0000",
		"status": "FINISHED",
		"errorMessage": "",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61613"
	}, {
		"text": "%md\n## Creating, Training, and Deploying our Learning Model\n\nThe remainder of this notebook walks you through the code we use to create, train, and deploy our learning model, in these steps:\n\n<ol class=\"italic\">\n    <li>Perform Spark+MLlib Setup Tasks</li>\n    <li>Create our DataFrame</li>\n    <li>Create Pipeline Stages</li>\n    <li>Assemble the Pipeline</li>\n    <li>Train our Model</li>\n    <li>Deploy our Model</li>\n</ol>\n\nWe include the entire program at the end of this notebook.",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:25:31+0000",
		"config": {
			"tableHide": false,
			"editorSetting": {
				"language": "markdown",
				"editOnDblClick": true
			},
			"colWidth": 12,
			"editorMode": "ace/mode/markdown",
			"editorHide": true,
			"results": {},
			"enabled": true,
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"results": {
			"code": "SUCCESS",
			"msg": [{
				"type": "HTML",
				"data": "<div class=\"markdown-body\">\n<h2>Creating, Training, and Deploying our Learning Model</h2>\n<p>The remainder of this notebook walks you through the code we use to create, train, and deploy our learning model, in these steps:</p>\n<ol class=\"italic\">\n    <li>Perform Spark+MLlib Setup Tasks</li>\n    <li>Create our DataFrame</li>\n    <li>Create Pipeline Stages</li>\n    <li>Assemble the Pipeline</li>\n    <li>Train our Model</li>\n    <li>Deploy our Model</li>\n</ol>\n<p>We include the entire program at the end of this notebook.</p>\n</div>"
			}]
		},
		"apps": [],
		"jobName": "paragraph_1520187001423_1110014729",
		"id": "20180131-172852_644197695",
		"dateCreated": "2018-03-04T18:10:01+0000",
		"dateStarted": "2018-10-08T22:25:31+0000",
		"dateFinished": "2018-10-08T22:25:31+0000",
		"status": "FINISHED",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61614"
	}, {
		"text": "%md\n### 1. Perform Spark+MLlib Setup Tasks\n\nFirst we set up our resources and initialize the Splice Machine Spark Adapter (`spliceMachineContext`). \n\n```\nimport org.apache.spark.ml.feature.VectorAssembler\nimport java.sql.{Connection,Timestamp}\nimport com.splicemachine.spark.splicemachine._\nimport org.apache.spark.sql.execution.datasources.jdbc.{JDBCOptions, JdbcUtils}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature.StringIndexer\nimport spark.implicits._\n\nval splicemachineContext = new SplicemachineContext(defaultJDBCURL)\n```",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:25:31+0000",
		"config": {
			"tableHide": false,
			"editorSetting": {
				"language": "markdown",
				"editOnDblClick": true
			},
			"colWidth": 12,
			"editorMode": "ace/mode/markdown",
			"editorHide": true,
			"results": {},
			"enabled": true,
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"results": {
			"code": "SUCCESS",
			"msg": [{
				"type": "HTML",
				"data": "<div class=\"markdown-body\">\n<h3>1. Perform Spark+MLlib Setup Tasks</h3>\n<p>First we set up our resources and initialize the Splice Machine Spark Adapter (<code>spliceMachineContext</code>). </p>\n<pre><code>import org.apache.spark.ml.feature.VectorAssembler\nimport java.sql.{Connection,Timestamp}\nimport com.splicemachine.spark.splicemachine._\nimport org.apache.spark.sql.execution.datasources.jdbc.{JDBCOptions, JdbcUtils}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature.StringIndexer\nimport spark.implicits._\n\nval splicemachineContext = new SplicemachineContext(defaultJDBCURL)\n</code></pre>\n</div>"
			}]
		},
		"apps": [],
		"jobName": "paragraph_1520187001424_1120402949",
		"id": "20180130-185957_1517048437",
		"dateCreated": "2018-03-04T18:10:01+0000",
		"dateStarted": "2018-10-08T22:25:31+0000",
		"dateFinished": "2018-10-08T22:25:31+0000",
		"status": "FINISHED",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61615"
	}, {
		"text": "%md \n#### Now we can do the same initialization that we did in Scala, but using Python instead\n\n```\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\n\nsplice = PySpliceContext(defaultJDBCURL, sqlContext)\n```",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:25:32+0000",
		"config": {
			"colWidth": 12,
			"enabled": true,
			"results": {},
			"editorSetting": {
				"language": "markdown",
				"editOnDblClick": true
			},
			"editorMode": "ace/mode/markdown",
			"editorHide": true,
			"tableHide": false,
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"results": {
			"code": "SUCCESS",
			"msg": [{
				"type": "HTML",
				"data": "<div class=\"markdown-body\">\n<h4>Now we can do the same initialization that we did in Scala, but using Python instead</h4>\n<pre><code>from pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\n\nsplice = PySpliceContext(defaultJDBCURL, sqlContext)\n</code></pre>\n</div>"
			}]
		},
		"apps": [],
		"jobName": "paragraph_1528749326903_1352113010",
		"id": "20180611-203526_1071983380",
		"dateCreated": "2018-06-11T20:35:26+0000",
		"dateStarted": "2018-10-08T22:25:32+0000",
		"dateFinished": "2018-10-08T22:25:32+0000",
		"status": "FINISHED",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61616"
	}, {
		"text": "%md\n### 2. Create our DataFrame: Scala\n\nWe need to pull the schema from our shipping database, `ASN.Features`, and convert it into a Spark DataFrame. We then create a sequential list (a Scala `seq` object) of the features (fields) from that table that we want to include in our model, and concatenate that onto our DataFrame. `MLlib` expects the schema to contain uppercase field names, so we convert our sequence to uppercase with a built-in function. \n\n```\nval df_with_uppercase_schema = splicemachineContext.df(\"select * from ASN.Features\")\nval newNames = Seq(\n    \"consignee\",\n    \"shipper\",\n    \"shipmode\",\n    \"gross_weight_lb\",\n    \"foreign_port\",\n    \"us_port\",\n    \"vessel_name\",\n    \"country_of_origin\",\n    \"container_number\",\n    \"container_type\",\n    \"quantity\",\n    \"ship_registered_in\",\n    \"carrier_code\",\n    \"carrier_city\",\n    \"notify_party\",\n    \"place_of_receipt\",\n    \"zipcode\",\n    \"quantity_bin\"\n    )\nval df = df_with_uppercase_schema.toDF(newNames: _*)\n```",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:25:32+0000",
		"config": {
			"tableHide": false,
			"editorSetting": {
				"language": "markdown",
				"editOnDblClick": true
			},
			"colWidth": 12,
			"editorMode": "ace/mode/markdown",
			"editorHide": true,
			"results": {},
			"enabled": true,
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"results": {
			"code": "SUCCESS",
			"msg": [{
				"type": "HTML",
				"data": "<div class=\"markdown-body\">\n<h3>2. Create our DataFrame: Scala</h3>\n<p>We need to pull the schema from our shipping database, <code>ASN.Features</code>, and convert it into a Spark DataFrame. We then create a sequential list (a Scala <code>seq</code> object) of the features (fields) from that table that we want to include in our model, and concatenate that onto our DataFrame. <code>MLlib</code> expects the schema to contain uppercase field names, so we convert our sequence to uppercase with a built-in function. </p>\n<pre><code>val df_with_uppercase_schema = splicemachineContext.df(&quot;select * from ASN.Features&quot;)\nval newNames = Seq(\n    &quot;consignee&quot;,\n    &quot;shipper&quot;,\n    &quot;shipmode&quot;,\n    &quot;gross_weight_lb&quot;,\n    &quot;foreign_port&quot;,\n    &quot;us_port&quot;,\n    &quot;vessel_name&quot;,\n    &quot;country_of_origin&quot;,\n    &quot;container_number&quot;,\n    &quot;container_type&quot;,\n    &quot;quantity&quot;,\n    &quot;ship_registered_in&quot;,\n    &quot;carrier_code&quot;,\n    &quot;carrier_city&quot;,\n    &quot;notify_party&quot;,\n    &quot;place_of_receipt&quot;,\n    &quot;zipcode&quot;,\n    &quot;quantity_bin&quot;\n    )\nval df = df_with_uppercase_schema.toDF(newNames: _*)\n</code></pre>\n</div>"
			}]
		},
		"apps": [],
		"jobName": "paragraph_1520187001424_1120402949",
		"id": "20180130-190437_334148512",
		"dateCreated": "2018-03-04T18:10:01+0000",
		"dateStarted": "2018-10-08T22:25:32+0000",
		"dateFinished": "2018-10-08T22:25:32+0000",
		"status": "FINISHED",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61617"
	}, {
		"text": "%md\n### 2. Create our DataFrame: Python \n\n```\ndf_with_uppercase_schema = PySpliceContext.df(\"select * from ASN.Features\")\nnewNames = [\n    \"consignee\",\n    \"shipper\",\n    \"shipmode\",\n    \"gross_weight_lb\",\n    \"foreign_port\",\n    \"us_port\",\n    \"vessel_name\",\n    \"country_of_origin\",\n    \"container_number\",\n    \"container_type\",\n    \"quantity\",\n    \"ship_registered_in\",\n    \"carrier_code\",\n    \"carrier_city\",\n    \"notify_party\",\n    \"place_of_receipt\",\n    \"zipcode\",\n    \"quantity_bin\"\n    ]\ndf = df_with_uppercase_schema.toDF(newNames)\n```",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:25:32+0000",
		"config": {
			"colWidth": 12,
			"enabled": true,
			"results": {},
			"editorSetting": {
				"language": "markdown",
				"editOnDblClick": true
			},
			"editorMode": "ace/mode/markdown",
			"editorHide": true,
			"tableHide": false,
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"results": {
			"code": "SUCCESS",
			"msg": [{
				"type": "HTML",
				"data": "<div class=\"markdown-body\">\n<h3>2. Create our DataFrame: Python</h3>\n<pre><code>df_with_uppercase_schema = PySpliceContext.df(&quot;select * from ASN.Features&quot;)\nnewNames = [\n    &quot;consignee&quot;,\n    &quot;shipper&quot;,\n    &quot;shipmode&quot;,\n    &quot;gross_weight_lb&quot;,\n    &quot;foreign_port&quot;,\n    &quot;us_port&quot;,\n    &quot;vessel_name&quot;,\n    &quot;country_of_origin&quot;,\n    &quot;container_number&quot;,\n    &quot;container_type&quot;,\n    &quot;quantity&quot;,\n    &quot;ship_registered_in&quot;,\n    &quot;carrier_code&quot;,\n    &quot;carrier_city&quot;,\n    &quot;notify_party&quot;,\n    &quot;place_of_receipt&quot;,\n    &quot;zipcode&quot;,\n    &quot;quantity_bin&quot;\n    ]\ndf = df_with_uppercase_schema.toDF(newNames)\n</code></pre>\n</div>"
			}]
		},
		"apps": [],
		"jobName": "paragraph_1528751936712_617932199",
		"id": "20180611-211856_605292217",
		"dateCreated": "2018-06-11T21:18:56+0000",
		"dateStarted": "2018-10-08T22:25:32+0000",
		"dateFinished": "2018-10-08T22:25:32+0000",
		"status": "FINISHED",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61618"
	}, {
		"text": "%md\n### 3. Create Pipeline Stages: Scala\n\nOur pipeline stages are fairly simple:\n\n* Transform each row of data in the input dataset into an integer vector.\n* Assemble the vectors into a DataFrame\n* Use a Logistic Regression Estimator to create our model\n\n#### Transform each row of data into an integer vector\n\nThe Logistic Regression estimator operates on integer vectors, so we need to convert each row in our input dataframe into an integer vector. Remember that each row contains only the fields from our database that are of interest to our model: the fields that previously included in our sequence and concatenated onto our DataFrame.\n\nSpark includes a `StringIndexer` function that does exactly that, so we create a `StringIndexer` for each field, and we'll later use each of these as a stage in our learning pipeline. The `StringIndexer` transforms the data from a specified input column in our DataFrame and stores the output in a specified and new output column. By convention, we name each string indexer with the name of the field+`Indexer,` and name the output column the name of the field+`Index,` e.g. we create a transformer named `consigneeIndexer` to transform the input column `consignee` into the new output column `consigneeIndex.`\n\n```\n// Transform strings into numbers\nval consigneeIndexer = new StringIndexer().setInputCol(\"consignee\").setOutputCol(\"consigneeIndex\").setHandleInvalid(\"skip\") \nval shipperIndexer = new StringIndexer().setInputCol(\"shipper\").setOutputCol(\"shipperIndex\").setHandleInvalid(\"skip\")\nval shipmodeIndexer = new StringIndexer().setInputCol(\"shipmode\").setOutputCol(\"shipmodeIndex\").setHandleInvalid(\"skip\") \nval gross_weight_lbIndexer = new StringIndexer().setInputCol(\"gross_weight_lb\").setOutputCol(\"gross_weight_lbIndex\").setHandleInvalid(\"skip\") \nval foreign_portIndexer = new StringIndexer().setInputCol(\"foreign_port\").setOutputCol(\"foreign_portIndex\").setHandleInvalid(\"skip\") \nval us_portIndexer = new StringIndexer().setInputCol(\"us_port\").setOutputCol(\"us_portIndex\").setHandleInvalid(\"skip\") \nval vessel_nameIndexer = new StringIndexer().setInputCol(\"vessel_name\").setOutputCol(\"vessel_nameIndex\").setHandleInvalid(\"skip\") \nval country_of_originIndexer = new StringIndexer().setInputCol(\"country_of_origin\").setOutputCol(\"country_of_originIndex\").setHandleInvalid(\"skip\") \nval container_numberIndexer = new StringIndexer().setInputCol(\"container_number\").setOutputCol(\"container_numberIndex\").setHandleInvalid(\"skip\")\nval container_typeIndexer = new StringIndexer().setInputCol(\"container_type\").setOutputCol(\"container_typeIndex\").setHandleInvalid(\"skip\") \nval ship_registered_inIndexer = new StringIndexer().setInputCol(\"ship_registered_in\").setOutputCol(\"ship_registered_inIndex\").setHandleInvalid(\"skip\") \nval carrier_codeIndexer = new StringIndexer().setInputCol(\"carrier_code\").setOutputCol(\"carrier_codeIndex\").setHandleInvalid(\"skip\") \nval carrier_cityIndexer = new StringIndexer().setInputCol(\"carrier_city\").setOutputCol(\"carrier_cityIndex\").setHandleInvalid(\"skip\") \nval notify_partyIndexer = new StringIndexer().setInputCol(\"notify_party\").setOutputCol(\"notify_partyIndex\").setHandleInvalid(\"skip\") \nval place_of_receiptIndexer = new StringIndexer().setInputCol(\"place_of_receipt\").setOutputCol(\"place_of_receiptIndex\").setHandleInvalid(\"skip\")\nval zipcodeIndexer = new StringIndexer().setInputCol(\"zipcode\").setOutputCol(\"zipcodeIndex\").setHandleInvalid(\"skip\")\n```\n\n#### Assemble the Vectors\n\nAfter our pipeline has transformed data into numbers, we need to assemble those into vectors. Spark includes a `VectorAssembler` object that does just that, transforming a set of input columns into a vector that is stored in the `features` column in the DataFrame:\n\n```\n//assemble raw features\nval assembler = new VectorAssembler()\n                .setInputCols(Array(\n                    \"shipmodeIndex\",\n                    \"consigneeIndex\",\n                    \"shipperIndex\",\n                    \"gross_weight_lbIndex\",\n                    \"foreign_portIndex\",\n                    \"us_portIndex\",\n                    \"vessel_nameIndex\",\n                    \"country_of_originIndex\",\n                    \"container_numberIndex\",\n                    \"container_typeIndex\",\n                    \"quantity_bin\",\n                    \"ship_registered_inIndex\",\n                    \"carrier_codeIndex\",\n                    \"carrier_cityIndex\",\n                    \"notify_partyIndex\",\n                    \"place_of_receiptIndex\",\n                    \"zipcodeIndex\",\n                    \"quantity_bin\"\n                    ))\n                .setOutputCol(\"features\")\n```\n\n#### Create the Estimator\n\nCreating the estimator is a simple matter of specifying a few parameters, including which column in the DataFrame is the label, and which column contains the feature set:\n\n```\n//Create ML analytic\nval lr = new LogisticRegression()\n    .setMaxIter(30)\n    .setLabelCol(\"label\")\n    .setFeaturesCol(\"features\")\n    .set\n    RegParam(0.3)\n```\n",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:25:32+0000",
		"config": {
			"tableHide": false,
			"editorSetting": {
				"language": "markdown",
				"editOnDblClick": true
			},
			"colWidth": 12,
			"editorMode": "ace/mode/markdown",
			"editorHide": true,
			"results": {},
			"enabled": true,
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"results": {
			"code": "SUCCESS",
			"msg": [{
				"type": "HTML",
				"data": "<div class=\"markdown-body\">\n<h3>3. Create Pipeline Stages: Scala</h3>\n<p>Our pipeline stages are fairly simple:</p>\n<ul>\n  <li>Transform each row of data in the input dataset into an integer vector.</li>\n  <li>Assemble the vectors into a DataFrame</li>\n  <li>Use a Logistic Regression Estimator to create our model</li>\n</ul>\n<h4>Transform each row of data into an integer vector</h4>\n<p>The Logistic Regression estimator operates on integer vectors, so we need to convert each row in our input dataframe into an integer vector. Remember that each row contains only the fields from our database that are of interest to our model: the fields that previously included in our sequence and concatenated onto our DataFrame.</p>\n<p>Spark includes a <code>StringIndexer</code> function that does exactly that, so we create a <code>StringIndexer</code> for each field, and we&rsquo;ll later use each of these as a stage in our learning pipeline. The <code>StringIndexer</code> transforms the data from a specified input column in our DataFrame and stores the output in a specified and new output column. By convention, we name each string indexer with the name of the field+<code>Indexer,</code> and name the output column the name of the field+<code>Index,</code> e.g. we create a transformer named <code>consigneeIndexer</code> to transform the input column <code>consignee</code> into the new output column <code>consigneeIndex.</code></p>\n<pre><code>// Transform strings into numbers\nval consigneeIndexer = new StringIndexer().setInputCol(&quot;consignee&quot;).setOutputCol(&quot;consigneeIndex&quot;).setHandleInvalid(&quot;skip&quot;) \nval shipperIndexer = new StringIndexer().setInputCol(&quot;shipper&quot;).setOutputCol(&quot;shipperIndex&quot;).setHandleInvalid(&quot;skip&quot;)\nval shipmodeIndexer = new StringIndexer().setInputCol(&quot;shipmode&quot;).setOutputCol(&quot;shipmodeIndex&quot;).setHandleInvalid(&quot;skip&quot;) \nval gross_weight_lbIndexer = new StringIndexer().setInputCol(&quot;gross_weight_lb&quot;).setOutputCol(&quot;gross_weight_lbIndex&quot;).setHandleInvalid(&quot;skip&quot;) \nval foreign_portIndexer = new StringIndexer().setInputCol(&quot;foreign_port&quot;).setOutputCol(&quot;foreign_portIndex&quot;).setHandleInvalid(&quot;skip&quot;) \nval us_portIndexer = new StringIndexer().setInputCol(&quot;us_port&quot;).setOutputCol(&quot;us_portIndex&quot;).setHandleInvalid(&quot;skip&quot;) \nval vessel_nameIndexer = new StringIndexer().setInputCol(&quot;vessel_name&quot;).setOutputCol(&quot;vessel_nameIndex&quot;).setHandleInvalid(&quot;skip&quot;) \nval country_of_originIndexer = new StringIndexer().setInputCol(&quot;country_of_origin&quot;).setOutputCol(&quot;country_of_originIndex&quot;).setHandleInvalid(&quot;skip&quot;) \nval container_numberIndexer = new StringIndexer().setInputCol(&quot;container_number&quot;).setOutputCol(&quot;container_numberIndex&quot;).setHandleInvalid(&quot;skip&quot;)\nval container_typeIndexer = new StringIndexer().setInputCol(&quot;container_type&quot;).setOutputCol(&quot;container_typeIndex&quot;).setHandleInvalid(&quot;skip&quot;) \nval ship_registered_inIndexer = new StringIndexer().setInputCol(&quot;ship_registered_in&quot;).setOutputCol(&quot;ship_registered_inIndex&quot;).setHandleInvalid(&quot;skip&quot;) \nval carrier_codeIndexer = new StringIndexer().setInputCol(&quot;carrier_code&quot;).setOutputCol(&quot;carrier_codeIndex&quot;).setHandleInvalid(&quot;skip&quot;) \nval carrier_cityIndexer = new StringIndexer().setInputCol(&quot;carrier_city&quot;).setOutputCol(&quot;carrier_cityIndex&quot;).setHandleInvalid(&quot;skip&quot;) \nval notify_partyIndexer = new StringIndexer().setInputCol(&quot;notify_party&quot;).setOutputCol(&quot;notify_partyIndex&quot;).setHandleInvalid(&quot;skip&quot;) \nval place_of_receiptIndexer = new StringIndexer().setInputCol(&quot;place_of_receipt&quot;).setOutputCol(&quot;place_of_receiptIndex&quot;).setHandleInvalid(&quot;skip&quot;)\nval zipcodeIndexer = new StringIndexer().setInputCol(&quot;zipcode&quot;).setOutputCol(&quot;zipcodeIndex&quot;).setHandleInvalid(&quot;skip&quot;)\n</code></pre>\n<h4>Assemble the Vectors</h4>\n<p>After our pipeline has transformed data into numbers, we need to assemble those into vectors. Spark includes a <code>VectorAssembler</code> object that does just that, transforming a set of input columns into a vector that is stored in the <code>features</code> column in the DataFrame:</p>\n<pre><code>//assemble raw features\nval assembler = new VectorAssembler()\n                .setInputCols(Array(\n                    &quot;shipmodeIndex&quot;,\n                    &quot;consigneeIndex&quot;,\n                    &quot;shipperIndex&quot;,\n                    &quot;gross_weight_lbIndex&quot;,\n                    &quot;foreign_portIndex&quot;,\n                    &quot;us_portIndex&quot;,\n                    &quot;vessel_nameIndex&quot;,\n                    &quot;country_of_originIndex&quot;,\n                    &quot;container_numberIndex&quot;,\n                    &quot;container_typeIndex&quot;,\n                    &quot;quantity_bin&quot;,\n                    &quot;ship_registered_inIndex&quot;,\n                    &quot;carrier_codeIndex&quot;,\n                    &quot;carrier_cityIndex&quot;,\n                    &quot;notify_partyIndex&quot;,\n                    &quot;place_of_receiptIndex&quot;,\n                    &quot;zipcodeIndex&quot;,\n                    &quot;quantity_bin&quot;\n                    ))\n                .setOutputCol(&quot;features&quot;)\n</code></pre>\n<h4>Create the Estimator</h4>\n<p>Creating the estimator is a simple matter of specifying a few parameters, including which column in the DataFrame is the label, and which column contains the feature set:</p>\n<pre><code>//Create ML analytic\nval lr = new LogisticRegression()\n    .setMaxIter(30)\n    .setLabelCol(&quot;label&quot;)\n    .setFeaturesCol(&quot;features&quot;)\n    .set\n    RegParam(0.3)\n</code></pre>\n</div>"
			}]
		},
		"apps": [],
		"jobName": "paragraph_1520187001424_1120402949",
		"id": "20180130-190241_1891417263",
		"dateCreated": "2018-03-04T18:10:01+0000",
		"dateStarted": "2018-10-08T22:25:32+0000",
		"dateFinished": "2018-10-08T22:25:32+0000",
		"status": "FINISHED",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61619"
	}, {
		"text": "%md\n### 3. Create Pipeline Stages: Python\n\nOur pipeline stages are fairly simple:\n\n* Transform each row of data in the input dataset into an integer vector.\n* Assemble the vectors into a DataFrame\n* Use a Logistic Regression Estimator to create our model\n\n#### Transform each row of data into an integer vector\n\nThe Logistic Regression estimator operates on integer vectors, so we need to convert each row in our input dataframe into an integer vector. Remember that each row contains only the fields from our database that are of interest to our model: the fields that previously included in our sequence and concatenated onto our DataFrame.\n\nSpark includes a `StringIndexer` function that does exactly that, so we create a `StringIndexer` for each field, and we'll later use each of these as a stage in our learning pipeline. The `StringIndexer` transforms the data from a specified input column in our DataFrame and stores the output in a specified and new output column. By convention, we name each string indexer with the name of the field+`Indexer,` and name the output column the name of the field+`Index,` e.g. we create a transformer named `consigneeIndexer` to transform the input column `consignee` into the new output column `consigneeIndex.`\n\n```\n// Transform strings into numbers\nconsigneeIndexer =  StringIndexer(inputCol=\"consignee\", outputCol=\"consigneeIndex\", handleInvalid=\"skip\")\nshipperIndexer = StringIndexer(inputCol=\"shipper\", outputCol=\"shipperIndex\", handleInvalid=\"skip\")\nshipmodeIndexer = StringIndexer(inputCol=\"shipmode\", outputCol=\"shipmodeIndex\", handleInvalid=\"skip\")\ngross_weight_lbIndexer = StringIndexer(inputCol=\"gross_weight_lb\", outputCol=\"gross_weight_lbIndex\", handleInvalid=\"skip\")\nforeign_portIndexer =  StringIndexer(inputCol=\"foreign_port\", outputCol=\"foreign_portIndex\", handleInvalid=\"skip\")\nus_portIndexer = StringIndexer(inputCol=\"us_port\", outputCol=\"us_portIndex\", handleInvalid=\"skip\")\nvessel_nameIndexer = StringIndexer(inputCol=\"vessel_name\", outputCol=\"vessel_nameIndex\",  handleInvalid=\"skip\")\ncountry_of_originIndexer = StringIndexer(inputCol=\"country_of_origin\", outputCol=\"country_of_originIndex\",  handleInvalid=\"skip\")\ncontainer_numberIndexer = StringIndexer(inputCol=\"container_number\", outputCol=\"container_numberIndex\", handleInvalid=\"skip\")\ncontainer_typeIndexer = StringIndexer(inputCol=\"container_type\", outputCol=\"container_typeIndex\", handleInvalid=\"skip\")\nship_registered_inIndexer = StringIndexer(inputCol=\"ship_registered_in\", outputCol=\"ship_registered_inIndex\", handleInvalid=\"skip\")\ncarrier_codeIndexer = StringIndexer(inputCol=\"carrier_code\", outputCol=\"carrier_codeIndex\", handleInvalid=\"skip\")\ncarrier_cityIndexer = StringIndexer(inputCol=\"carrier_city\", outputCol=\"carrier_cityIndex\", handleInvalid=\"skip\")\nnotify_partyIndexer = StringIndexer(inputCol=\"notify_party\", outputCol=\"notify_partyIndex\", handleInvalid=\"skip\")\nplace_of_receiptIndexer = StringIndexer(inputCol=\"place_of_receipt\", outputCol=\"place_of_receiptIndex\", handleInvalid=\"skip\")\nzipcodeIndexer = StringIndexer(inputCol=\"zipcode\", outputCol=\"zipcodeIndex\", handleInvalid=\"skip\")\n```\n#### Assemble the Vectors\n\nAfter our pipeline has transformed data into numbers, we need to assemble those into vectors. Spark includes a `VectorAssembler` object that does just that, transforming a set of input columns into a vector that is stored in the `features` column in the DataFrame:\n\n```\n//assemble raw features\nassembler = VectorAssembler(inputCols=[\n                    \"shipmodeIndex\",\n                    \"consigneeIndex\",\n                    \"shipperIndex\",\n                    \"gross_weight_lbIndex\",\n                    \"foreign_portIndex\",\n                    \"us_portIndex\",\n                    \"vessel_nameIndex\",\n                    \"country_of_originIndex\",\n                    \"container_numberIndex\",\n                    \"container_typeIndex\",\n                    \"quantity_bin\",\n                    \"ship_registered_inIndex\",\n                    \"carrier_codeIndex\",\n                    \"carrier_cityIndex\",\n                    \"notify_partyIndex\",\n                    \"place_of_receiptIndex\",\n                    \"zipcodeIndex\",\n                    \"quantity_bin\"\n                    ], outputCol='features')\n```\n\n#### Create the Estimator\n\nCreating the estimator is a simple matter of specifying a few parameters, including which column in the DataFrame is the label, and which column contains the feature set:\n\n```\n//Create ML analytic\nlr = new LogisticRegression(maxIter=30, labelCol=\"label\", featuresCol=\"features\", regParam=0.3)\n\n```\n",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:25:32+0000",
		"config": {
			"colWidth": 12,
			"enabled": true,
			"results": {},
			"editorSetting": {
				"language": "markdown",
				"editOnDblClick": true
			},
			"editorMode": "ace/mode/markdown",
			"editorHide": true,
			"tableHide": false,
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"results": {
			"code": "SUCCESS",
			"msg": [{
				"type": "HTML",
				"data": "<div class=\"markdown-body\">\n<h3>3. Create Pipeline Stages: Python</h3>\n<p>Our pipeline stages are fairly simple:</p>\n<ul>\n  <li>Transform each row of data in the input dataset into an integer vector.</li>\n  <li>Assemble the vectors into a DataFrame</li>\n  <li>Use a Logistic Regression Estimator to create our model</li>\n</ul>\n<h4>Transform each row of data into an integer vector</h4>\n<p>The Logistic Regression estimator operates on integer vectors, so we need to convert each row in our input dataframe into an integer vector. Remember that each row contains only the fields from our database that are of interest to our model: the fields that previously included in our sequence and concatenated onto our DataFrame.</p>\n<p>Spark includes a <code>StringIndexer</code> function that does exactly that, so we create a <code>StringIndexer</code> for each field, and we&rsquo;ll later use each of these as a stage in our learning pipeline. The <code>StringIndexer</code> transforms the data from a specified input column in our DataFrame and stores the output in a specified and new output column. By convention, we name each string indexer with the name of the field+<code>Indexer,</code> and name the output column the name of the field+<code>Index,</code> e.g. we create a transformer named <code>consigneeIndexer</code> to transform the input column <code>consignee</code> into the new output column <code>consigneeIndex.</code></p>\n<pre><code>// Transform strings into numbers\nconsigneeIndexer =  StringIndexer(inputCol=&quot;consignee&quot;, outputCol=&quot;consigneeIndex&quot;, handleInvalid=&quot;skip&quot;)\nshipperIndexer = StringIndexer(inputCol=&quot;shipper&quot;, outputCol=&quot;shipperIndex&quot;, handleInvalid=&quot;skip&quot;)\nshipmodeIndexer = StringIndexer(inputCol=&quot;shipmode&quot;, outputCol=&quot;shipmodeIndex&quot;, handleInvalid=&quot;skip&quot;)\ngross_weight_lbIndexer = StringIndexer(inputCol=&quot;gross_weight_lb&quot;, outputCol=&quot;gross_weight_lbIndex&quot;, handleInvalid=&quot;skip&quot;)\nforeign_portIndexer =  StringIndexer(inputCol=&quot;foreign_port&quot;, outputCol=&quot;foreign_portIndex&quot;, handleInvalid=&quot;skip&quot;)\nus_portIndexer = StringIndexer(inputCol=&quot;us_port&quot;, outputCol=&quot;us_portIndex&quot;, handleInvalid=&quot;skip&quot;)\nvessel_nameIndexer = StringIndexer(inputCol=&quot;vessel_name&quot;, outputCol=&quot;vessel_nameIndex&quot;,  handleInvalid=&quot;skip&quot;)\ncountry_of_originIndexer = StringIndexer(inputCol=&quot;country_of_origin&quot;, outputCol=&quot;country_of_originIndex&quot;,  handleInvalid=&quot;skip&quot;)\ncontainer_numberIndexer = StringIndexer(inputCol=&quot;container_number&quot;, outputCol=&quot;container_numberIndex&quot;, handleInvalid=&quot;skip&quot;)\ncontainer_typeIndexer = StringIndexer(inputCol=&quot;container_type&quot;, outputCol=&quot;container_typeIndex&quot;, handleInvalid=&quot;skip&quot;)\nship_registered_inIndexer = StringIndexer(inputCol=&quot;ship_registered_in&quot;, outputCol=&quot;ship_registered_inIndex&quot;, handleInvalid=&quot;skip&quot;)\ncarrier_codeIndexer = StringIndexer(inputCol=&quot;carrier_code&quot;, outputCol=&quot;carrier_codeIndex&quot;, handleInvalid=&quot;skip&quot;)\ncarrier_cityIndexer = StringIndexer(inputCol=&quot;carrier_city&quot;, outputCol=&quot;carrier_cityIndex&quot;, handleInvalid=&quot;skip&quot;)\nnotify_partyIndexer = StringIndexer(inputCol=&quot;notify_party&quot;, outputCol=&quot;notify_partyIndex&quot;, handleInvalid=&quot;skip&quot;)\nplace_of_receiptIndexer = StringIndexer(inputCol=&quot;place_of_receipt&quot;, outputCol=&quot;place_of_receiptIndex&quot;, handleInvalid=&quot;skip&quot;)\nzipcodeIndexer = StringIndexer(inputCol=&quot;zipcode&quot;, outputCol=&quot;zipcodeIndex&quot;, handleInvalid=&quot;skip&quot;)\n</code></pre>\n<h4>Assemble the Vectors</h4>\n<p>After our pipeline has transformed data into numbers, we need to assemble those into vectors. Spark includes a <code>VectorAssembler</code> object that does just that, transforming a set of input columns into a vector that is stored in the <code>features</code> column in the DataFrame:</p>\n<pre><code>//assemble raw features\nassembler = VectorAssembler(inputCols=[\n                    &quot;shipmodeIndex&quot;,\n                    &quot;consigneeIndex&quot;,\n                    &quot;shipperIndex&quot;,\n                    &quot;gross_weight_lbIndex&quot;,\n                    &quot;foreign_portIndex&quot;,\n                    &quot;us_portIndex&quot;,\n                    &quot;vessel_nameIndex&quot;,\n                    &quot;country_of_originIndex&quot;,\n                    &quot;container_numberIndex&quot;,\n                    &quot;container_typeIndex&quot;,\n                    &quot;quantity_bin&quot;,\n                    &quot;ship_registered_inIndex&quot;,\n                    &quot;carrier_codeIndex&quot;,\n                    &quot;carrier_cityIndex&quot;,\n                    &quot;notify_partyIndex&quot;,\n                    &quot;place_of_receiptIndex&quot;,\n                    &quot;zipcodeIndex&quot;,\n                    &quot;quantity_bin&quot;\n                    ], outputCol=&#39;features&#39;)\n</code></pre>\n<h4>Create the Estimator</h4>\n<p>Creating the estimator is a simple matter of specifying a few parameters, including which column in the DataFrame is the label, and which column contains the feature set:</p>\n<pre><code>//Create ML analytic\nlr = new LogisticRegression(maxIter=30, labelCol=&quot;label&quot;, featuresCol=&quot;features&quot;, regParam=0.3)\n\n</code></pre>\n</div>"
			}]
		},
		"apps": [],
		"jobName": "paragraph_1528752561631_2097334927",
		"id": "20180611-212921_295690204",
		"dateCreated": "2018-06-11T21:29:21+0000",
		"dateStarted": "2018-10-08T22:25:33+0000",
		"dateFinished": "2018-10-08T22:25:33+0000",
		"status": "FINISHED",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61620"
	}, {
		"text": "%md\n### 4. Assemble our Pipeline: Scala\n\nNow that we've got our stages set up, we're ready to assemble our Machine Learning pipeline, which chains together those stages in sequence:\n\n```\n// Chain indexers and tree in a Pipeline\nval lrPipeline = new Pipeline().setStages(\n        Array(consigneeIndexer,\n                shipperIndexer,\n                shipmodeIndexer,\n                gross_weight_lbIndexer,\n                foreign_portIndexer,\n                us_portIndexer,\n                vessel_nameIndexer,\n                country_of_originIndexer,\n                container_numberIndexer,\n                container_typeIndexer,\n                ship_registered_inIndexer,\n                carrier_codeIndexer,\n                carrier_cityIndexer,\n                notify_partyIndexer,\n                place_of_receiptIndexer,\n                zipcodeIndexer,\n                assembler,\n                lr\n                ))\n```",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:25:33+0000",
		"config": {
			"tableHide": false,
			"editorSetting": {
				"language": "markdown",
				"editOnDblClick": true
			},
			"colWidth": 12,
			"editorMode": "ace/mode/markdown",
			"editorHide": true,
			"results": {},
			"enabled": true,
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"results": {
			"code": "SUCCESS",
			"msg": [{
				"type": "HTML",
				"data": "<div class=\"markdown-body\">\n<h3>4. Assemble our Pipeline: Scala</h3>\n<p>Now that we&rsquo;ve got our stages set up, we&rsquo;re ready to assemble our Machine Learning pipeline, which chains together those stages in sequence:</p>\n<pre><code>// Chain indexers and tree in a Pipeline\nval lrPipeline = new Pipeline().setStages(\n        Array(consigneeIndexer,\n                shipperIndexer,\n                shipmodeIndexer,\n                gross_weight_lbIndexer,\n                foreign_portIndexer,\n                us_portIndexer,\n                vessel_nameIndexer,\n                country_of_originIndexer,\n                container_numberIndexer,\n                container_typeIndexer,\n                ship_registered_inIndexer,\n                carrier_codeIndexer,\n                carrier_cityIndexer,\n                notify_partyIndexer,\n                place_of_receiptIndexer,\n                zipcodeIndexer,\n                assembler,\n                lr\n                ))\n</code></pre>\n</div>"
			}]
		},
		"apps": [],
		"jobName": "paragraph_1520187001425_1120018200",
		"id": "20180130-201715_1588482510",
		"dateCreated": "2018-03-04T18:10:01+0000",
		"dateStarted": "2018-10-08T22:25:33+0000",
		"dateFinished": "2018-10-08T22:25:33+0000",
		"status": "FINISHED",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61621"
	}, {
		"text": "%md\n### 5. Train our Model: Scala\n\nNow that our pipeline is set up, all we need to do to train our model is feed our dataframe into the pipeline's `fit` method, which learns from the data. \n```\n// Train model. \nval lrModel = lrPipeline.fit(df)\n```\n",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:25:33+0000",
		"config": {
			"tableHide": false,
			"editorSetting": {
				"language": "markdown",
				"editOnDblClick": true
			},
			"colWidth": 12,
			"editorMode": "ace/mode/markdown",
			"editorHide": true,
			"results": {},
			"enabled": true,
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"results": {
			"code": "SUCCESS",
			"msg": [{
				"type": "HTML",
				"data": "<div class=\"markdown-body\">\n<h3>5. Train our Model: Scala</h3>\n<p>Now that our pipeline is set up, all we need to do to train our model is feed our dataframe into the pipeline&rsquo;s <code>fit</code> method, which learns from the data. </p>\n<pre><code>// Train model. \nval lrModel = lrPipeline.fit(df)\n</code></pre>\n</div>"
			}]
		},
		"apps": [],
		"jobName": "paragraph_1520187001425_1120018200",
		"id": "20180130-201949_1288501052",
		"dateCreated": "2018-03-04T18:10:01+0000",
		"dateStarted": "2018-10-08T22:25:33+0000",
		"dateFinished": "2018-10-08T22:25:33+0000",
		"status": "FINISHED",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61622"
	}, {
		"title": "Using Spark MLlib",
		"text": "%md\n### 6. Materialize the Model: Python & Scala (same code)\n\nNow that we've trained our model, we can apply it to real data and display the results. For simplicity sake, we'll simply apply the model to our feature table itself.\n\n```\nlrModel.transform(df).select(\"prediction\", \"probability\", \"features\").show(100)\n```",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:25:33+0000",
		"config": {
			"tableHide": false,
			"editorSetting": {
				"language": "markdown",
				"editOnDblClick": true
			},
			"colWidth": 12,
			"editorMode": "ace/mode/markdown",
			"editorHide": true,
			"title": false,
			"results": {},
			"enabled": true,
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"results": {
			"code": "SUCCESS",
			"msg": [{
				"type": "HTML",
				"data": "<div class=\"markdown-body\">\n<h3>6. Materialize the Model: Python &amp; Scala (same code)</h3>\n<p>Now that we&rsquo;ve trained our model, we can apply it to real data and display the results. For simplicity sake, we&rsquo;ll simply apply the model to our feature table itself.</p>\n<pre><code>lrModel.transform(df).select(&quot;prediction&quot;, &quot;probability&quot;, &quot;features&quot;).show(100)\n</code></pre>\n</div>"
			}]
		},
		"apps": [],
		"jobName": "paragraph_1520187001425_1120018200",
		"id": "20180118-020316_1913850778",
		"dateCreated": "2018-03-04T18:10:01+0000",
		"dateStarted": "2018-10-08T22:25:33+0000",
		"dateFinished": "2018-10-08T22:25:33+0000",
		"status": "FINISHED",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61623"
	}, {
		"text": "%splicemachine\nselect *  from ASN.features { limit 100 }\n",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:25:33+0000",
		"config": {
			"colWidth": 12,
			"editorMode": "ace/mode/sql",
			"results": {
				"0": {
					"graph": {
						"mode": "table",
						"height": 300,
						"optionOpen": false,
						"setting": {
							"table": {
								"tableGridState": {},
								"tableColumnTypeState": {
									"names": {
										"SHIPMENTID": "string",
										"STATUS": "string",
										"SHIPMODE": "string",
										"PRODUCT_DESCRIPTION": "string",
										"CONSIGNEE": "string",
										"SHIPPER": "string",
										"ARRIVAL_DATE": "string",
										"GROSS_WEIGHT_LB": "string",
										"GROSS_WEIGHT_KG": "string",
										"FOREIGN_PORT": "string",
										"US_PORT": "string",
										"VESSEL_NAME": "string",
										"COUNTRY_OF_ORIGIN": "string",
										"CONSIGNEE_ADDRESS": "string",
										"SHIPPER_ADDRESS": "string",
										"ZIPCODE": "string",
										"NO_OF_CONTAINERS": "string",
										"CONTAINER_NUMBER": "string",
										"CONTAINER_TYPE": "string",
										"QUANTITY": "string",
										"QUANTITY_UNIT": "string",
										"MEASUREMENT": "string",
										"MEASUREMENT_UNIT": "string",
										"BILL_OF_LADING": "string",
										"HOUSE_VS_MASTER": "string",
										"DISTRIBUTION_PORT": "string",
										"MASTER_BL": "string",
										"VOYAGE_NUMBER": "string",
										"SEAL": "string",
										"SHIP_REGISTERED_IN": "string",
										"INBOND_ENTRY_TYPE": "string",
										"CARRIER_CODE": "string",
										"CARRIER_NAME": "string",
										"CARRIER_CITY": "string",
										"CARRIER_STATE": "string",
										"CARRIER_ZIP": "string",
										"CARRIER_ADDRESS": "string",
										"NOTIFY_PARTY": "string",
										"NOTIFY_ADDRESS": "string",
										"PLACE_OF_RECEIPT": "string",
										"DATE_OF_RECEIPT": "string",
										"QUANTITY_BIN": "string",
										"LATENESS": "string",
										"LABEL": "string"
									},
									"updated": false
								},
								"tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
								"tableOptionValue": {
									"useFilter": false,
									"showPagination": false,
									"showAggregationFooter": false
								},
								"updated": false,
								"initialized": false
							}
						},
						"commonSetting": {}
					}
				}
			},
			"enabled": true,
			"editorSetting": {
				"language": "sql",
				"editOnDblClick": false
			},
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"apps": [],
		"jobName": "paragraph_1520187001426_1121172447",
		"id": "20180205-024307_1130627818",
		"dateCreated": "2018-03-04T18:10:01+0000",
		"dateStarted": "2018-10-08T22:25:33+0000",
		"dateFinished": "2018-10-08T22:25:34+0000",
		"status": "FINISHED",
		"errorMessage": "",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61624"
	}, {
		"text": "%splicemachine\nDROP TABLE IF EXISTS ASN.PREDICTIONS;\nCREATE TABLE ASN.PREDICTIONS (\n    SHIPMENTID VARCHAR(11) NOT NULL PRIMARY KEY,\n    PREDICTION DOUBLE\n    );",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:25:34+0000",
		"config": {
			"colWidth": 12,
			"editorMode": "ace/mode/sql",
			"results": {},
			"enabled": true,
			"editorSetting": {
				"language": "sql",
				"editOnDblClick": false
			},
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"apps": [],
		"jobName": "paragraph_1520187001427_1120787698",
		"id": "20180205-024429_478433529",
		"dateCreated": "2018-03-04T18:10:01+0000",
		"dateStarted": "2018-10-08T22:25:34+0000",
		"dateFinished": "2018-10-08T22:25:35+0000",
		"status": "FINISHED",
		"errorMessage": "",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61625"
	}, {
		"text": "%md\n### Final Scala Code \n##### (you made it!)\n",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:25:35+0000",
		"config": {
			"colWidth": 12,
			"enabled": true,
			"results": {},
			"editorSetting": {
				"language": "text",
				"editOnDblClick": false
			},
			"editorMode": "ace/mode/text",
			"editorHide": true,
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"results": {
			"code": "SUCCESS",
			"msg": [{
				"type": "HTML",
				"data": "<div class=\"markdown-body\">\n<h3>Final Scala Code</h3>\n<h5>(you made it!)</h5>\n</div>"
			}]
		},
		"apps": [],
		"jobName": "paragraph_1528754794618_-2139516148",
		"id": "20180611-220634_1810473505",
		"dateCreated": "2018-06-11T22:06:34+0000",
		"dateStarted": "2018-10-08T22:25:35+0000",
		"dateFinished": "2018-10-08T22:25:35+0000",
		"status": "FINISHED",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61626"
	}, {
		"text": "%spark\nimport org.apache.spark.ml.feature.VectorAssembler\nimport java.sql.{Connection}\nimport com.splicemachine.spark.splicemachine._\nimport org.apache.spark.sql.execution.datasources.jdbc.{JDBCOptions, JdbcUtils}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature.StringIndexer\nimport spark.implicits._\n\n\nval optionMap = Map(\n  JDBCOptions.JDBC_TABLE_NAME -> \"ASN.Features\",\n  JDBCOptions.JDBC_URL -> defaultJDBCURL\n)\nval splicemachineContext = new SplicemachineContext(defaultJDBCURL)\nval df_with_uppercase_schema = splicemachineContext.df(\"select * from ASN.Features\")\nval newNames = Seq(\n    \"shipmentid\",\n    \"status\",\n    \"shipmode\",\n    \"product_description\",\n    \"consignee\",\n    \"shipper\",\n    \"arrival_date\",\n    \"gross_weight_lb\",\n    \"gross_weight_kg\",\n    \"foreign_port\",\n    \"us_port\",\n    \"vessel_name\",\n    \"country_of_origin\",\n    \"consignee_address\",\n    \"shipper_address\",\n    \"zipcode\",\n    \"no_of_containers\",\n    \"container_number\",\n    \"container_type\",\n    \"quantity\",\n    \"quantity_unit\",\n    \"measurement\",\n    \"measurement_unit\",\n    \"bill_of_lading\",\n    \"house_vs_master\",\n    \"distribution_port\",\n    \"master_bl\",\n    \"voyage_number\",\n    \"seal\",\n    \"ship_registered_in\",\n    \"inbond_entry_type\",\n    \"carrier_code\",\n    \"carrier_name\",\n    \"carrier_city\",\n    \"carrier_state\",\n    \"carrier_zip\",\n    \"carrier_address\",\n    \"notify_party\",\n    \"notify_address\",\n    \"place_of_receipt\",\n    \"date_of_receipt\",\n    \"quantity_bin\",\n    \"lateness\",\n    \"label\"\n)\nval df = df_with_uppercase_schema.toDF(newNames: _*)\n\n//assemble raw features\nval assembler = new VectorAssembler().\n                setInputCols(Array(\n                    \"consigneeIndex\",\n                    \"shipperIndex\",\n                    \"gross_weight_lbIndex\",\n                    \"foreign_portIndex\",\n                    \"us_portIndex\",\n                    \"vessel_nameIndex\",\n                    \"country_of_originIndex\",\n                    \"container_numberIndex\",\n                    \"container_typeIndex\",\n                    \"quantity_bin\",\n                    \"ship_registered_inIndex\",\n                    \"carrier_codeIndex\",\n                    \"carrier_cityIndex\",\n                    \"notify_partyIndex\",\n                    \"place_of_receiptIndex\",\n                    \"zipcodeIndex\"\n                    )).\n                setOutputCol(\"features\")\n\n// Transform strings into numbers\nval zipcodeIndexer = new StringIndexer().setInputCol(\"zipcode\").setOutputCol(\"zipcodeIndex\").setHandleInvalid(\"skip\")\nval consigneeIndexer = new StringIndexer().setInputCol(\"consignee\").setOutputCol(\"consigneeIndex\").setHandleInvalid(\"skip\") \nval shipperIndexer = new StringIndexer().setInputCol(\"shipper\").setOutputCol(\"shipperIndex\").setHandleInvalid(\"skip\")\nval statusIndexer = new StringIndexer().setInputCol(\"status\").setOutputCol(\"statusIndex\").setHandleInvalid(\"skip\") \nval shipmodeIndexer = new StringIndexer().setInputCol(\"shipmode\").setOutputCol(\"shipmodeIndex\").setHandleInvalid(\"skip\") \nval gross_weight_lbIndexer = new StringIndexer().setInputCol(\"gross_weight_lb\").setOutputCol(\"gross_weight_lbIndex\").setHandleInvalid(\"skip\") \nval foreign_portIndexer = new StringIndexer().setInputCol(\"foreign_port\").setOutputCol(\"foreign_portIndex\").setHandleInvalid(\"skip\") \nval us_portIndexer = new StringIndexer().setInputCol(\"us_port\").setOutputCol(\"us_portIndex\").setHandleInvalid(\"skip\") \nval vessel_nameIndexer = new StringIndexer().setInputCol(\"vessel_name\").setOutputCol(\"vessel_nameIndex\").setHandleInvalid(\"skip\") \nval country_of_originIndexer = new StringIndexer().setInputCol(\"country_of_origin\").setOutputCol(\"country_of_originIndex\").setHandleInvalid(\"skip\") \nval container_numberIndexer = new StringIndexer().setInputCol(\"container_number\").setOutputCol(\"container_numberIndex\").setHandleInvalid(\"skip\")\nval container_typeIndexer = new StringIndexer().setInputCol(\"container_type\").setOutputCol(\"container_typeIndex\").setHandleInvalid(\"skip\") \nval distribution_portIndexer = new StringIndexer().setInputCol(\"distribution_port\").setOutputCol(\"distribution_portIndex\").setHandleInvalid(\"skip\") \nval ship_registered_inIndexer = new StringIndexer().setInputCol(\"ship_registered_in\").setOutputCol(\"ship_registered_inIndex\").setHandleInvalid(\"skip\") \nval inbond_entry_typeIndexer = new StringIndexer().setInputCol(\"inbond_entry_type\").setOutputCol(\"inbond_entry_typeIndex\").setHandleInvalid(\"skip\") \nval carrier_codeIndexer = new StringIndexer().setInputCol(\"carrier_code\").setOutputCol(\"carrier_codeIndex\").setHandleInvalid(\"skip\") \nval carrier_cityIndexer = new StringIndexer().setInputCol(\"carrier_city\").setOutputCol(\"carrier_cityIndex\").setHandleInvalid(\"skip\") \nval carrier_stateIndexer = new StringIndexer().setInputCol(\"carrier_state\").setOutputCol(\"carrier_stateIndex\").setHandleInvalid(\"skip\") \nval carrier_zipIndexer = new StringIndexer().setInputCol(\"carrier_zip\").setOutputCol(\"carrier_zipIndex\").setHandleInvalid(\"skip\") \nval notify_partyIndexer = new StringIndexer().setInputCol(\"notify_party\").setOutputCol(\"notify_partyIndex\").setHandleInvalid(\"skip\") \nval place_of_receiptIndexer = new StringIndexer().setInputCol(\"place_of_receipt\").setOutputCol(\"place_of_receiptIndex\").setHandleInvalid(\"skip\") \n\n//Create ML analytic\nval lr = new LogisticRegression().\n    setMaxIter(30).\n    setLabelCol(\"label\").\n    setFeaturesCol(\"features\").\n    setRegParam(0.3)\n\n\n// Chain indexers and tree in a Pipeline\nval lrPipeline = new Pipeline().setStages(\n        Array(consigneeIndexer,\n                shipperIndexer,\n                shipmodeIndexer,\n                gross_weight_lbIndexer,\n                foreign_portIndexer,\n                us_portIndexer,\n                vessel_nameIndexer,\n                country_of_originIndexer,\n                container_numberIndexer,\n                container_typeIndexer,\n                ship_registered_inIndexer,\n                carrier_codeIndexer,\n                carrier_cityIndexer,\n                notify_partyIndexer,\n                place_of_receiptIndexer,\n                zipcodeIndexer,\n                assembler,\n                lr\n                ))\n\n// Train model. \nval lrModel = lrPipeline.fit(df)\n\nlrModel.transform(df).select(\"prediction\", \"probability\", \"features\").show(100)\n\n\n\n",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:25:36+0000",
		"config": {
			"tableHide": false,
			"editorSetting": {
				"language": "scala",
				"editOnDblClick": false,
				"completionKey": "TAB",
				"completionSupport": true
			},
			"colWidth": 12,
			"editorMode": "ace/mode/scala",
			"editorHide": false,
			"results": {},
			"enabled": true,
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"apps": [],
		"jobName": "paragraph_1520187001427_1120787698",
		"id": "20180130-201612_556450692",
		"dateCreated": "2018-03-04T18:10:01+0000",
		"dateStarted": "2018-10-08T22:25:36+0000",
		"dateFinished": "2018-10-08T22:26:24+0000",
		"status": "FINISHED",
		"errorMessage": "",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61627"
	}, {
		"text": "%splicemachine\ndrop table IF EXISTS ASN.TEST_FEATURES;\nCREATE table ASN.TEST_FEATURES AS\n    SELECT \n    SHIPMENTID,\n    STATUS,\n    SHIPMODE,\n    PRODUCT_DESCRIPTION,\n    CONSIGNEE,\n    SHIPPER,\n    ARRIVAL_DATE,\n    GROSS_WEIGHT_LB,\n    GROSS_WEIGHT_KG,\n    FOREIGN_PORT,\n    US_PORT,\n    VESSEL_NAME,\n    COUNTRY_OF_ORIGIN,\n    CONSIGNEE_ADDRESS,\n    SHIPPER_ADDRESS,\n    ZIPCODE,\n    NO_OF_CONTAINERS,\n    CONTAINER_NUMBER,\n    CONTAINER_TYPE,\n    QUANTITY,\n    QUANTITY_UNIT,\n    MEASUREMENT,\n    MEASUREMENT_UNIT,\n    BILL_OF_LADING,\n    HOUSE_VS_MASTER,\n    DISTRIBUTION_PORT,\n    MASTER_BL,\n    VOYAGE_NUMBER,\n    SEAL,\n    SHIP_REGISTERED_IN,\n    INBOND_ENTRY_TYPE,\n    CARRIER_CODE,\n    CARRIER_NAME,\n    CARRIER_CITY,\n    CARRIER_STATE,\n    CARRIER_ZIP,\n    CARRIER_ADDRESS,\n    NOTIFY_PARTY,\n    NOTIFY_ADDRESS,\n    PLACE_OF_RECEIPT,\n    DATE_OF_RECEIPT,\n    CASE\n    WHEN ASN.SHIPMENT_IN_TRANSIT.QUANTITY > 10\n    THEN\n        CASE\n            WHEN ASN.SHIPMENT_IN_TRANSIT.QUANTITY > 100\n            THEN\n                CASE\n                    WHEN ASN.SHIPMENT_IN_TRANSIT.QUANTITY > 1000\n                    THEN 3\n                    ELSE 2\n                END\n            ELSE 1\n\tEND\n    ELSE 0\n    END AS QUANTITY_BIN\n    FROM ASN.SHIPMENT_IN_TRANSIT;\n    \n",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:26:24+0000",
		"config": {
			"colWidth": 12,
			"enabled": true,
			"results": {},
			"editorSetting": {
				"language": "sql",
				"editOnDblClick": false
			},
			"editorMode": "ace/mode/sql",
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"apps": [],
		"jobName": "paragraph_1520189714677_331176209",
		"id": "20180304-185514_1167859763",
		"dateCreated": "2018-03-04T18:55:14+0000",
		"dateStarted": "2018-10-08T22:26:25+0000",
		"dateFinished": "2018-10-08T22:26:27+0000",
		"status": "FINISHED",
		"errorMessage": "",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61628"
	}, {
		"text": "%md\n### Scala Test",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:26:27+0000",
		"config": {
			"colWidth": 12,
			"enabled": true,
			"results": {},
			"editorSetting": {
				"language": "markdown",
				"editOnDblClick": true
			},
			"editorMode": "ace/mode/markdown",
			"editorHide": true,
			"tableHide": false,
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"results": {
			"code": "SUCCESS",
			"msg": [{
				"type": "HTML",
				"data": "<div class=\"markdown-body\">\n<h3>Scala Test</h3>\n</div>"
			}]
		},
		"apps": [],
		"jobName": "paragraph_1528773325221_-1949303904",
		"id": "20180612-031525_84277416",
		"dateCreated": "2018-06-12T03:15:25+0000",
		"dateStarted": "2018-10-08T22:26:27+0000",
		"dateFinished": "2018-10-08T22:26:27+0000",
		"status": "FINISHED",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61629"
	}, {
		"text": "%spark\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, LongType, BooleanType };\n\n    val test_data_with_uppercase_schema = splicemachineContext.df(\"select * from ASN.TEST_FEATURES\")\n    val newNames = Seq(\n        \"shipmentid\",\n        \"status\",\n        \"shipmode\",\n        \"product_description\",\n        \"consignee\",\n        \"shipper\",\n        \"arrival_date\",\n        \"gross_weight_lb\",\n        \"gross_weight_kg\",\n        \"foreign_port\",\n        \"us_port\",\n        \"vessel_name\",\n        \"country_of_origin\",\n        \"consignee_address\",\n        \"shipper_address\",\n        \"zipcode\",\n        \"no_of_containers\",\n        \"container_number\",\n        \"container_type\",\n        \"quantity\",\n        \"quantity_unit\",\n        \"measurement\",\n        \"measurement_unit\",\n        \"bill_of_lading\",\n        \"house_vs_master\",\n        \"distribution_port\",\n        \"master_bl\",\n        \"voyage_number\",\n        \"seal\",\n        \"ship_registered_in\",\n        \"inbond_entry_type\",\n        \"carrier_code\",\n        \"carrier_name\",\n        \"carrier_city\",\n        \"carrier_state\",\n        \"carrier_zip\",\n        \"carrier_address\",\n        \"notify_party\",\n        \"notify_address\",\n        \"place_of_receipt\",\n        \"date_of_receipt\",\n        \"quantity_bin\"\n    )\n    val test_data = test_data_with_uppercase_schema.toDF(newNames: _*)\n\n    // Make predictions.\n    val lrPredictions = lrModel.transform(test_data)\n\n    var z = Array(\"Zara\", \"Nuha\", \"Ayan\")\n\n    // Select example rows to display.\n     val innerStruct =\n   StructType(\n     StructField(\"f1\", IntegerType, true) ::\n     StructField(\"f2\", LongType, false) ::\n     StructField(\"f3\", BooleanType, false) :: Nil)\n    lrPredictions.select(\"prediction\", \"probability\",\"features\").show(10)\n    splicemachineContext.createTable(\"tabletest\", innerStruct, z, null)",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:26:58+0000",
		"config": {
			"colWidth": 12,
			"editorMode": "ace/mode/scala",
			"results": {},
			"enabled": true,
			"editorSetting": {
				"language": "scala",
				"editOnDblClick": false,
				"completionKey": "TAB",
				"completionSupport": true
			},
			"editorHide": false,
			"tableHide": false,
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"apps": [],
		"jobName": "paragraph_1520187001427_1120787698",
		"id": "20180125-142959_1101825868",
		"dateCreated": "2018-03-04T18:10:01+0000",
		"dateStarted": "2018-10-08T22:26:58+0000",
		"dateFinished": "2018-10-08T22:27:08+0000",
		"status": "FINISHED",
		"errorMessage": "",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61630"
	}, {
		"text": "%md\n### Scala",
		"user": "splice",
		"dateUpdated": "2018-06-12T22:23:58+0000",
		"config": {
			"colWidth": 12,
			"enabled": true,
			"results": {},
			"editorSetting": {
				"language": "markdown",
				"editOnDblClick": true
			},
			"editorMode": "ace/mode/markdown",
			"editorHide": true,
			"tableHide": false,
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"results": {
			"code": "SUCCESS",
			"msg": [{
				"type": "HTML",
				"data": "<div class=\"markdown-body\">\n<h3>Scala</h3>\n</div>"
			}]
		},
		"apps": [],
		"jobName": "paragraph_1528825640671_2142794058",
		"id": "20180612-174720_2040536500",
		"dateCreated": "2018-06-12T17:47:20+0000",
		"dateStarted": "2018-06-12T17:47:30+0000",
		"dateFinished": "2018-06-12T17:47:30+0000",
		"status": "FINISHED",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61631"
	}, {
		"text": "%spark\nimport org.apache.spark.sql.types.{StructType, StructField, StringType};\n\nval predictions = lrPredictions.select(\"SHIPMENTID\", \"PREDICTION\")\n\npredictions.printSchema()\n \nsplicemachineContext.insert(predictions,\"ASN.predictions\") \n\n",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:27:29+0000",
		"config": {
			"colWidth": 12,
			"enabled": true,
			"results": {},
			"editorSetting": {
				"language": "scala",
				"editOnDblClick": false,
				"completionKey": "TAB",
				"completionSupport": true
			},
			"editorMode": "ace/mode/scala",
			"editorHide": false,
			"tableHide": true,
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"results": {
			"code": "SUCCESS",
			"msg": [{
				"type": "TEXT",
				"data": "import org.apache.spark.sql.types.{StructType, StructField, StringType}\npredictions: org.apache.spark.sql.DataFrame = [SHIPMENTID: string, PREDICTION: double]\nroot\n |-- SHIPMENTID: string (nullable = true)\n |-- PREDICTION: double (nullable = true)\n\n"
			}]
		},
		"apps": [],
		"jobName": "paragraph_1520189863458_1609682004",
		"id": "20180304-185743_1150073560",
		"dateCreated": "2018-03-04T18:57:43+0000",
		"dateStarted": "2018-10-08T22:27:29+0000",
		"dateFinished": "2018-10-08T22:27:32+0000",
		"status": "FINISHED",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61632"
	}, {
		"text": "%splicemachine\nselect * from ASN.predictions;",
		"user": "splice",
		"dateUpdated": "2018-10-08T22:27:36+0000",
		"config": {
			"colWidth": 12,
			"enabled": true,
			"results": {
				"0": {
					"graph": {
						"mode": "table",
						"height": 318,
						"optionOpen": false,
						"setting": {
							"table": {
								"tableGridState": {},
								"tableColumnTypeState": {
									"names": {
										"SHIPMENTID": "string",
										"PREDICTION": "string"
									},
									"updated": false
								},
								"tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
								"tableOptionValue": {
									"useFilter": false,
									"showPagination": false,
									"showAggregationFooter": false
								},
								"updated": false,
								"initialized": false
							}
						},
						"commonSetting": {}
					}
				}
			},
			"editorSetting": {
				"language": "sql",
				"editOnDblClick": false
			},
			"editorMode": "ace/mode/sql",
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"apps": [],
		"jobName": "paragraph_1520190035692_829874249",
		"id": "20180304-190035_379792791",
		"dateCreated": "2018-03-04T19:00:35+0000",
		"dateStarted": "2018-10-08T22:27:36+0000",
		"dateFinished": "2018-10-08T22:27:36+0000",
		"status": "FINISHED",
		"errorMessage": "",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61633"
	}, {
		"text": "\n",
		"user": "splice",
		"dateUpdated": "2018-06-12T22:26:44+0000",
		"config": {
			"colWidth": 12,
			"enabled": true,
			"results": {},
			"editorSetting": {
				"language": "sql",
				"editOnDblClick": false
			},
			"editorMode": "ace/mode/sql",
			"fontSize": 9
		},
		"settings": {
			"params": {},
			"forms": {}
		},
		"apps": [],
		"jobName": "paragraph_1520190075458_-1682105347",
		"id": "20180304-190115_1109571681",
		"dateCreated": "2018-03-04T19:01:15+0000",
		"status": "READY",
		"progressUpdateIntervalMs": 500,
		"$$hashKey": "object:61634"
	}],
	"name": "3. Splice Deep Dive / 9. Scala MLlib example",
	"id": "2D7P8BMMS",
	"noteParams": {},
	"noteForms": {},
	"angularObjects": {
		"md:shared_process": [],
		"splicemachine:shared_process": [],
		"spark:shared_process": []
	},
	"config": {
		"looknfeel": "simple",
		"personalizedMode": "false",
		"isZeppelinNotebookCronEnable": false
	},
	"info": {}
}