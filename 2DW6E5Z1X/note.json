{
  "paragraphs": [
    {
      "title": "Spark Adapter",
      "text": "%md\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\n# Using our Spark Adapter\n\nData Scientists have adopted Spark as the de facto data science platform, and Splice Machine provides an industry leading in-process integration to a Spark cluster. This means data scientists and data engineers can adopt the full power of Spark and manipulate dataframes but also get the power of full ANSI, ACID-compliant SQL.\n\nThe Splice Machine Spark adapter provides:\n\n* A durable, ACID compliant persistence model for Spark Dataframes.\n* Lazy result sets returned as Spark Dataframes.\n* Access to Spark libraries such as MLLib and GraphX.\n* Avoidance of expensive ETL of data from OLTP to OLAP.\n\nThis notebook demonstrates using the Spark Adapter with Python, in these steps:\n\n1. Create the `PySpliceContextClass` to interface with the Python API.\n2. Use the `spark.pyspark` Python interpreter in Zeppelin to create a Spark context.\n3. Create a simple table in Splice Machine.\n4. Create a Spark dataframe and insert that into Splice Machine\n5. Run a simple Splice Machine transaction using the Spark context.\n6. Rollback that transaction using the same context.\n\n\u003cbr /\u003e\n\n## 1. Create the PySpliceContext Class\n\nOur first step is to use the `PySpark` interpreter to create the `PySpliceContext` class:\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-20 10:58:44.639",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "title": false,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\u003ch1\u003eUsing our Spark Adapter\u003c/h1\u003e\n\u003cp\u003eData Scientists have adopted Spark as the de facto data science platform, and Splice Machine provides an industry leading in-process integration to a Spark cluster. This means data scientists and data engineers can adopt the full power of Spark and manipulate dataframes but also get the power of full ANSI, ACID-compliant SQL.\u003c/p\u003e\n\u003cp\u003eThe Splice Machine Spark adapter provides:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eA durable, ACID compliant persistence model for Spark Dataframes.\u003c/li\u003e\n  \u003cli\u003eLazy result sets returned as Spark Dataframes.\u003c/li\u003e\n  \u003cli\u003eAccess to Spark libraries such as MLLib and GraphX.\u003c/li\u003e\n  \u003cli\u003eAvoidance of expensive ETL of data from OLTP to OLAP.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis notebook demonstrates using the Spark Adapter with Python, in these steps:\u003c/p\u003e\n\u003col\u003e\n  \u003cli\u003eCreate the \u003ccode\u003ePySpliceContextClass\u003c/code\u003e to interface with the Python API.\u003c/li\u003e\n  \u003cli\u003eUse the \u003ccode\u003espark.pyspark\u003c/code\u003e Python interpreter in Zeppelin to create a Spark context.\u003c/li\u003e\n  \u003cli\u003eCreate a simple table in Splice Machine.\u003c/li\u003e\n  \u003cli\u003eCreate a Spark dataframe and insert that into Splice Machine\u003c/li\u003e\n  \u003cli\u003eRun a simple Splice Machine transaction using the Spark context.\u003c/li\u003e\n  \u003cli\u003eRollback that transaction using the same context.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cbr /\u003e\n\u003ch2\u003e1. Create the PySpliceContext Class\u003c/h2\u003e\n\u003cp\u003eOur first step is to use the \u003ccode\u003ePySpark\u003c/code\u003e interpreter to create the \u003ccode\u003ePySpliceContext\u003c/code\u003e class:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395654304_-247900930",
      "id": "20180118-005945_1825119026",
      "dateCreated": "2018-11-16 11:14:14.000",
      "dateStarted": "2018-11-20 10:58:44.640",
      "dateFinished": "2018-11-20 10:58:44.648",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "PySpliceContext Class",
      "text": "%spark.pyspark\nfrom __future__ import print_function\nimport string\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import Row\nfrom pyspark.sql import DataFrame\n\n#java imports                                                                                                                               \nfrom py4j.java_gateway import java_import\nfrom py4j.java_gateway import JavaGateway, GatewayParameters\n\nclass PySpliceContext:\n    def __init__(self,JDBC_URL,sparkSQLContext):\n        self.jdbcurl \u003d JDBC_URL\n        self.sparkSQLContext \u003d sparkSQLContext\n        self.jvm \u003d self.sparkSQLContext._sc._jvm\n        java_import(self.jvm,\"com.splicemachine.spark.splicemachine.*\")\n        java_import(self.jvm, \"org.apache.spark.sql.execution.datasources.jdbc.{JDBCOptions, JdbcUtils}\")\n        java_import(self.jvm, \"scala.collection.JavaConverters._\")\n        self.context \u003d self.jvm.com.splicemachine.spark.splicemachine.SplicemachineContext(self.jdbcurl)\n    def getConnection(self):\n        return self.context.getConnection()\n        def getConnection(): Connection \n    def tableExists(self, schemaTableName):\n        return self.context.tableExists(schemaTableName)\n    def dropTable(self,schemaName, tableName):\n        return self.context.dropTable(schemaName, tableName)\n    def dropTable(self,schemaTableName):\n        return self.context.dropTable(schemaTableName)\n    def df(self,sql):\n        return DataFrame(self.context.df(sql), self.sparkSQLContext)\n    def insert(self,dataFrame,schemaTableName): \n        return self.context.insert(dataFrame._jdf,schemaTableName)\n    def delete(self, dataFrame, schemaTableName):\n        return self.context.delete(dataFrame._jdf, schemaTableName)\n    def update(self, dataFrame, schemaTableName):\n        return self.context.update(dataFrame._jdf, schemaTableName) \n    def bulkImportHFile(self, dataFrame, schemaTableName, options):\n        return self.context.bulkImportHFile(dataFrame._jdf, schemaTableName, options)\n    def getSchema(self,schemaTableName):\n        return self.context.getSchema(schemaTableName)\n    def createTable(self, tableName, structType, keys, createTableOptions):\n        keyAsString \u003d self.jvm.java.util.ArrayList()\n        for key in keys:\n            keyAsString.append(key)\n        return self.context.createTable(tableName, structType, self.jvm.scala.collection.JavaConversions.asScalaBuffer(keyAsString).toSeq(), createTableOptions)\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:14:14.000",
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "title": false,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1542395654306_1053279453",
      "id": "20180118-015126_1752791454",
      "dateCreated": "2018-11-16 11:14:14.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n## 2. Create a Spark context\n\nNext, we use the `PySpliceContext` to create a connection to Splice Machine:",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:14:14.000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003e2. Create a Spark context\u003c/h2\u003e\n\u003cp\u003eNext, we use the \u003ccode\u003ePySpliceContext\u003c/code\u003e to create a connection to Splice Machine:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395654306_745071231",
      "id": "20180129-153516_1751825816",
      "dateCreated": "2018-11-16 11:14:14.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create Context",
      "text": "%spark.pyspark\n\nsplice \u003d PySpliceContext(\u0027jdbc:splice://{FRAMEWORKNAME}-proxy.marathon.mesos:1527/splicedb;user\u003dsplice;password\u003dadmin\u0027, sqlContext)",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:14:14.000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "title": false,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1542395654307_517477292",
      "id": "20180118-014113_2141603268",
      "dateCreated": "2018-11-16 11:14:14.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## 3. Create a Simple Table\n\nNow we create a simple table in Splice Machine that we\u0027ll subsequently populate:\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:14:14.000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003e3. Create a Simple Table\u003c/h2\u003e\n\u003cp\u003eNow we create a simple table in Splice Machine that we\u0026rsquo;ll subsequently populate:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395654308_-1855904705",
      "id": "20180129-153654_674823995",
      "dateCreated": "2018-11-16 11:14:14.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create a simple table",
      "text": "%splicemachine\ndrop table if exists foo;\ncreate table foo (I int, F float, V varchar(100), primary key (I));\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:14:14.000",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "title": false,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Query executed successfully. Affected rows : 0"
          },
          {
            "type": "TEXT",
            "data": "Query executed successfully. Affected rows : 0"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395654308_-66941472",
      "id": "20180118-015418_1427121084",
      "dateCreated": "2018-11-16 11:14:14.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## 4. Create a Spark Dataframe and Insert into Splice Machine\n\nThen we use `spark.pyspark` to create a Spark dataframe from some sample data, and insert that into our Splice Machine table.\n\nAfter inserting the data, we do a `select *` to display the contents of the Splice Machine table. ",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:14:14.000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003e4. Create a Spark Dataframe and Insert into Splice Machine\u003c/h2\u003e\n\u003cp\u003eThen we use \u003ccode\u003espark.pyspark\u003c/code\u003e to create a Spark dataframe from some sample data, and insert that into our Splice Machine table.\u003c/p\u003e\n\u003cp\u003eAfter inserting the data, we do a \u003ccode\u003eselect *\u003c/code\u003e to display the contents of the Splice Machine table.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395654309_1060518389",
      "id": "20180129-153818_1709035487",
      "dateCreated": "2018-11-16 11:14:14.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create a dataframe and insert into Splice",
      "text": "%spark.pyspark\nfrom pyspark.sql import Row\nl \u003d [(0,3.14,\u0027Turing\u0027), (1,4.14,\u0027Newell\u0027), (2,5.14,\u0027Simon\u0027), (3,6.14,\u0027Minsky\u0027)]\nrdd \u003d sc.parallelize(l)\nrows \u003d rdd.map(lambda x: Row(I\u003dx[0], F\u003dfloat(x[1]), V\u003dstr(x[2])))\nschemaRows \u003d sqlContext.createDataFrame(rows)\nsplice.insert(schemaRows,\u0027foo\u0027)\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:14:14.000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "title": false,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1542395654309_1802628017",
      "id": "20180118-015449_13133342",
      "dateCreated": "2018-11-16 11:14:14.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\nselect * from foo;",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:14:42.000",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 169.006,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "I": "string",
                      "F": "string",
                      "V": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "I\tF\tV\n0\t3.14\tTuring\n1\t4.14\tNewell\n2\t5.14\tSimon\n3\t6.14\tMinsky\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395654310_-1396800287",
      "id": "20180118-015510_1158645327",
      "dateCreated": "2018-11-16 11:14:14.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n## 5. Run a Simple Splice Machine Transaction\n\nNow we\u0027ll add more data to that table in a transactional context: ",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:14:14.000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003e5. Run a Simple Splice Machine Transaction\u003c/h2\u003e\n\u003cp\u003eNow we\u0026rsquo;ll add more data to that table in a transactional context:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395654310_1501089403",
      "id": "20180129-154333_772550654",
      "dateCreated": "2018-11-16 11:14:14.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Transactional Context",
      "text": "%spark.pyspark\nconn \u003d splice.getConnection()\nconn.setAutoCommit(False)\nl \u003d [(4,3.14,\u0027Turing\u0027), (5,4.14,\u0027Newell\u0027), (6,5.14,\u0027Simon\u0027), (7,6.14,\u0027Minsky\u0027)]\nrdd \u003d sc.parallelize(l)\nrows \u003d rdd.map(lambda x: Row(I\u003dx[0], F\u003dfloat(x[1]), V\u003dstr(x[2])))\nschemaRows \u003d sqlContext.createDataFrame(rows)\nsplice.insert(schemaRows,\u0027foo\u0027)\ndf \u003d splice.df(\"select * from foo\")\ndf.collect\nz.show(df)",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:14:42.000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "title": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "I": "string",
                      "F": "string",
                      "V": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "I\tF\tV\n0\t3.14\tTuring\n1\t4.14\tNewell\n2\t5.14\tSimon\n3\t6.14\tMinsky\n4\t3.14\tTuring\n5\t4.14\tNewell\n6\t5.14\tSimon\n7\t6.14\tMinsky\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395654311_-252240147",
      "id": "20180118-015525_2011115580",
      "dateCreated": "2018-11-16 11:14:14.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n## 6. Rollback the transaction\n\nFinally, we\u0027ll rollback the transaction we just ran:\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:14:14.000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003e6. Rollback the transaction\u003c/h2\u003e\n\u003cp\u003eFinally, we\u0026rsquo;ll rollback the transaction we just ran:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395654311_324284397",
      "id": "20180129-154912_774169413",
      "dateCreated": "2018-11-16 11:14:14.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Rollback",
      "text": "%spark.pyspark\nconn.rollback()\ndf \u003d splice.df(\"select * from foo\")\ndf.collect\nz.show(df)",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:14:42.000",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "title": false,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 159.006,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "I": "string",
                      "F": "string",
                      "V": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "I\tF\tV\n0\t3.14\tTuring\n1\t4.14\tNewell\n2\t5.14\tSimon\n3\t6.14\tMinsky\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395654313_1836815294",
      "id": "20180118-015552_1456308237",
      "dateCreated": "2018-11-16 11:14:14.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Where to Go Next\n\nNow let\u0027s explore Machine Learning with these Splice Machine, starting with our next notebook: [*ML with Spark MLib using Python*](/#/notebook/2DWTYCFTY).",
      "user": "anonymous",
      "dateUpdated": "2018-11-20 11:03:00.647",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eWhere to Go Next\u003c/h2\u003e\n\u003cp\u003eNow let\u0026rsquo;s explore Machine Learning with these Splice Machine, starting with our next notebook: \u003ca href\u003d\"/#/notebook/2DWTYCFTY\"\u003e\u003cem\u003eML with Spark MLib using Python\u003c/em\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395654314_71425482",
      "id": "20180122-173624_509126525",
      "dateCreated": "2018-11-16 11:14:14.000",
      "dateStarted": "2018-11-20 11:03:00.648",
      "dateFinished": "2018-11-20 11:03:00.652",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:14:14.000",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395654314_682197011",
      "id": "20180125-142959_1101825868",
      "dateCreated": "2018-11-16 11:14:14.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Training/Developers - Part II/h. Using the Native Spark DataSource",
  "id": "2DW6E5Z1X",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "angular:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}