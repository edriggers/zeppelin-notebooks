{
  "paragraphs": [
    {
      "title": "Spark Adapter",
      "text": "%md\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\n# Using our Native Spark DataSource\nThis notebook demonstrates using the Spark Adapter with Python, in these steps:\n\n1. *Create the `PySpliceContextClass` to interface with the Python API.*\n2. *Use the `spark.pyspark` Python interpreter in Zeppelin to create a Spark context.*\n3. *Create a simple table in Splice Machine.*\n4. *Create a Spark dataframe and insert that into Splice Machine.*\n5. *Run a simple Splice Machine transaction using the Spark context.*\n6. *Rollback that transaction using the same context.*\n\n## About the Native Spark DataSource\nData Scientists have adopted Spark as the de facto data science platform, and Splice Machine provides an industry leading in-process integration to a Spark cluster. This means data scientists and data engineers can adopt the full power of Spark and manipulate dataframes but also get the power of full ANSI, ACID-compliant SQL.\n\nThe Splice Machine Spark adapter provides:\n\n* A durable, ACID compliant persistence model for Spark Dataframes.\n* Lazy result sets returned as Spark Dataframes.\n* Access to Spark libraries such as MLLib and GraphX.\n* Avoidance of expensive ETL of data from OLTP to OLAP.",
      "user": "anonymous",
      "dateUpdated": "2018-12-03 23:38:53.394",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "title": false,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\u003ch1\u003eUsing our Native Spark DataSource\u003c/h1\u003e\n\u003cp\u003eThis notebook demonstrates using the Spark Adapter with Python, in these steps:\u003c/p\u003e\n\u003col\u003e\n  \u003cli\u003e\u003cem\u003eCreate the \u003ccode\u003ePySpliceContextClass\u003c/code\u003e to interface with the Python API.\u003c/em\u003e\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eUse the \u003ccode\u003espark.pyspark\u003c/code\u003e Python interpreter in Zeppelin to create a Spark context.\u003c/em\u003e\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eCreate a simple table in Splice Machine.\u003c/em\u003e\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eCreate a Spark dataframe and insert that into Splice Machine.\u003c/em\u003e\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eRun a simple Splice Machine transaction using the Spark context.\u003c/em\u003e\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eRollback that transaction using the same context.\u003c/em\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eAbout the Native Spark DataSource\u003c/h2\u003e\n\u003cp\u003eData Scientists have adopted Spark as the de facto data science platform, and Splice Machine provides an industry leading in-process integration to a Spark cluster. This means data scientists and data engineers can adopt the full power of Spark and manipulate dataframes but also get the power of full ANSI, ACID-compliant SQL.\u003c/p\u003e\n\u003cp\u003eThe Splice Machine Spark adapter provides:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eA durable, ACID compliant persistence model for Spark Dataframes.\u003c/li\u003e\n  \u003cli\u003eLazy result sets returned as Spark Dataframes.\u003c/li\u003e\n  \u003cli\u003eAccess to Spark libraries such as MLLib and GraphX.\u003c/li\u003e\n  \u003cli\u003eAvoidance of expensive ETL of data from OLTP to OLAP.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1543880333379_1036489161",
      "id": "20180118-005945_1825119026",
      "dateCreated": "2018-12-03 23:38:53.379",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## 1. Create the PySpliceContext Class\n\nYour first step is to use the `PySpark` interpreter in Zeppelin to create the `PySpliceContext` class:",
      "user": "anonymous",
      "dateUpdated": "2018-12-03 23:38:53.399",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003e1. Create the PySpliceContext Class\u003c/h2\u003e\n\u003cp\u003eYour first step is to use the \u003ccode\u003ePySpark\u003c/code\u003e interpreter in Zeppelin to create the \u003ccode\u003ePySpliceContext\u003c/code\u003e class:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1543880333398_1348461029",
      "id": "20181130-051253_1322450311",
      "dateCreated": "2018-12-03 23:38:53.398",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "PySpliceContext Class",
      "text": "%spark.pyspark\nfrom __future__ import print_function\nimport string\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import Row\nfrom pyspark.sql import DataFrame\n\n#java imports                                                                                                                               \nfrom py4j.java_gateway import java_import\nfrom py4j.java_gateway import JavaGateway, GatewayParameters\n\nclass PySpliceContext:\n    def __init__(self,JDBC_URL,sparkSQLContext):\n        self.jdbcurl \u003d JDBC_URL\n        self.sparkSQLContext \u003d sparkSQLContext\n        self.jvm \u003d self.sparkSQLContext._sc._jvm\n        java_import(self.jvm,\"com.splicemachine.spark.splicemachine.*\")\n        java_import(self.jvm, \"org.apache.spark.sql.execution.datasources.jdbc.{JDBCOptions, JdbcUtils}\")\n        java_import(self.jvm, \"scala.collection.JavaConverters._\")\n        self.context \u003d self.jvm.com.splicemachine.spark.splicemachine.SplicemachineContext(self.jdbcurl)\n    def getConnection(self):\n        return self.context.getConnection()\n        def getConnection(): Connection \n    def tableExists(self, schemaTableName):\n        return self.context.tableExists(schemaTableName)\n    def dropTable(self,schemaName, tableName):\n        return self.context.dropTable(schemaName, tableName)\n    def dropTable(self,schemaTableName):\n        return self.context.dropTable(schemaTableName)\n    def df(self,sql):\n        return DataFrame(self.context.df(sql), self.sparkSQLContext)\n    def insert(self,dataFrame,schemaTableName): \n        return self.context.insert(dataFrame._jdf,schemaTableName)\n    def delete(self, dataFrame, schemaTableName):\n        return self.context.delete(dataFrame._jdf, schemaTableName)\n    def update(self, dataFrame, schemaTableName):\n        return self.context.update(dataFrame._jdf, schemaTableName) \n    def bulkImportHFile(self, dataFrame, schemaTableName, options):\n        return self.context.bulkImportHFile(dataFrame._jdf, schemaTableName, options)\n    def getSchema(self,schemaTableName):\n        return self.context.getSchema(schemaTableName)\n    def createTable(self, tableName, structType, keys, createTableOptions):\n        keyAsString \u003d self.jvm.java.util.ArrayList()\n        for key in keys:\n            keyAsString.append(key)\n        return self.context.createTable(tableName, structType, self.jvm.scala.collection.JavaConversions.asScalaBuffer(keyAsString).toSeq(), createTableOptions)\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-06 23:59:53.954",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "fontSize": 9.0,
        "title": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1543880333399_986445985",
      "id": "20180118-015126_1752791454",
      "dateCreated": "2018-12-03 23:38:53.400",
      "dateStarted": "2018-12-06 23:59:54.009",
      "dateFinished": "2018-12-06 23:59:56.901",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n## 2. Create a Spark context\n\nNext, we use the `PySpliceContext` to create a connection to Splice Machine:",
      "user": "anonymous",
      "dateUpdated": "2018-12-03 23:38:53.402",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003e2. Create a Spark context\u003c/h2\u003e\n\u003cp\u003eNext, we use the \u003ccode\u003ePySpliceContext\u003c/code\u003e to create a connection to Splice Machine:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1543880333402_-2050257534",
      "id": "20180129-153516_1751825816",
      "dateCreated": "2018-12-03 23:38:53.402",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create Context",
      "text": "%spark.pyspark\n\nsplice \u003d PySpliceContext(\u0027jdbc:splice://localhost:1527/splicedb;user\u003dsplice;password\u003dadmin\u0027, sqlContext)",
      "user": "anonymous",
      "dateUpdated": "2018-12-07 00:00:02.130",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "fontSize": 9.0,
        "title": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1543880333403_-1864855422",
      "id": "20180118-014113_2141603268",
      "dateCreated": "2018-12-03 23:38:53.403",
      "dateStarted": "2018-12-07 00:00:02.197",
      "dateFinished": "2018-12-07 00:00:11.616",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## 3. Create a Simple Table\n\nNow we create simple table in Splice Machine that we\u0027ll subsequently populate:\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-03 23:38:53.404",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003e3. Create a Simple Table\u003c/h2\u003e\n\u003cp\u003eNow we create simple table in Splice Machine that we\u0026rsquo;ll subsequently populate:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1543880333404_-1185353888",
      "id": "20180129-153654_674823995",
      "dateCreated": "2018-12-03 23:38:53.404",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create a simple table",
      "text": "%splicemachine\n\ncreate table DS.foo (I int, F float, V varchar(100), primary key (I));\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-07 00:00:18.386",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "title": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Query executed successfully. Affected rows : 0"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1543880333405_1076663825",
      "id": "20180118-015418_1427121084",
      "dateCreated": "2018-12-03 23:38:53.405",
      "dateStarted": "2018-12-07 00:00:18.462",
      "dateFinished": "2018-12-07 00:00:19.900",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## 4. Create a Spark Dataframe and Insert into Splice Machine\n\nThen we use `spark.pyspark` to create a Spark dataframe from some sample data, and insert that into our Splice Machine table.\n\n\u003cp class\u003d\"noteNote\"\u003eYou can ignore the \u003ccode\u003eRuntimeWarning:\u003c/code\u003e warning messages that may display when you run the code in the next paragraph.\u003c/p\u003e\n\nAfter inserting the data, we do a `select *` to display the contents of the Splice Machine table. ",
      "user": "anonymous",
      "dateUpdated": "2018-12-07 00:02:30.609",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003e4. Create a Spark Dataframe and Insert into Splice Machine\u003c/h2\u003e\n\u003cp\u003eThen we use \u003ccode\u003espark.pyspark\u003c/code\u003e to create a Spark dataframe from some sample data, and insert that into our Splice Machine table.\u003c/p\u003e\n\u003cp class\u003d\"noteNote\"\u003eYou can ignore the \u003ccode\u003eRuntimeWarning:\u003c/code\u003e warning messages that may display when you run the code in the next paragraph.\u003c/p\u003e\n\u003cp\u003eAfter inserting the data, we do a \u003ccode\u003eselect *\u003c/code\u003e to display the contents of the Splice Machine table.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1543880333406_1515588593",
      "id": "20180129-153818_1709035487",
      "dateCreated": "2018-12-03 23:38:53.406",
      "dateStarted": "2018-12-07 00:02:30.610",
      "dateFinished": "2018-12-07 00:02:30.640",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create a dataframe and insert into Splice",
      "text": "%spark.pyspark\nfrom pyspark.sql import Row\nl \u003d [(0,3.14,\u0027Turing\u0027), (1,4.14,\u0027Newell\u0027), (2,5.14,\u0027Simon\u0027), (3,6.14,\u0027Minsky\u0027)]\nrdd \u003d sc.parallelize(l)\nrows \u003d rdd.map(lambda x: Row(I\u003dx[0], F\u003dfloat(x[1]), V\u003dstr(x[2])))\nschemaRows \u003d sqlContext.createDataFrame(rows)\nsplice.insert(schemaRows,\u0027DS.foo\u0027)\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-07 00:00:29.599",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "fontSize": 9.0,
        "title": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "/usr/lib64/python2.7/site-packages/pandas/_libs/__init__.py:4: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from .tslib import iNaT, NaT, Timestamp, Timedelta, OutOfBoundsDatetime\n/usr/lib64/python2.7/site-packages/pandas/__init__.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from pandas._libs import (hashtable as _hashtable,\n/usr/lib64/python2.7/site-packages/pandas/core/dtypes/common.py:6: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from pandas._libs import algos, lib\n/usr/lib64/python2.7/site-packages/pandas/core/util/hashing.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from pandas._libs import hashing, tslib\n/usr/lib64/python2.7/site-packages/pandas/core/indexes/base.py:6: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from pandas._libs import (lib, index as libindex, tslib as libts,\n/usr/lib64/python2.7/site-packages/pandas/core/tools/datetimes.py:6: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from pandas._libs.tslibs.strptime import array_strptime\n/usr/lib64/python2.7/site-packages/pandas/tseries/frequencies.py:24: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from pandas._libs.tslibs.frequencies import (  # noqa\n/usr/lib64/python2.7/site-packages/pandas/core/indexes/datetimelike.py:28: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from pandas._libs.period import Period\n/usr/lib64/python2.7/site-packages/pandas/core/sparse/array.py:33: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  import pandas._libs.sparse as splib\n/usr/lib64/python2.7/site-packages/pandas/core/window.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  import pandas._libs.window as _window\n/usr/lib64/python2.7/site-packages/pandas/core/groupby.py:68: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from pandas._libs import lib, groupby as libgroupby, Timestamp, NaT, iNaT\n/usr/lib64/python2.7/site-packages/pandas/core/reshape/reshape.py:31: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from pandas._libs import algos as _algos, reshape as _reshape\n/usr/lib64/python2.7/site-packages/pandas/io/parsers.py:45: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  import pandas._libs.parsers as parsers\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://172.17.0.2:4041/jobs/job?id\u003d1"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1543880333406_563698802",
      "id": "20180118-015449_13133342",
      "dateCreated": "2018-12-03 23:38:53.406",
      "dateStarted": "2018-12-07 00:00:29.648",
      "dateFinished": "2018-12-07 00:00:48.107",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Native Spark Datasource\n\nIf you look closely, you\u0027ll see that we went straight from a Spark Dataframe into Splice Machine\u0027s database. \n\nThis is Splice Machine\u0027s Native Spark Datasource.  Not only is this mechanism convenient, it is also very performant, leveraging parallelism in large datasets.  The main API for the Python version we just used is just in the paragraph at the top of this notebook.\n\nHere\u0027s the result:",
      "user": "anonymous",
      "dateUpdated": "2018-12-03 23:38:53.407",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eNative Spark Datasource\u003c/h3\u003e\n\u003cp\u003eIf you look closely, you\u0026rsquo;ll see that we went straight from a Spark Dataframe into Splice Machine\u0026rsquo;s database. \u003c/p\u003e\n\u003cp\u003eThis is Splice Machine\u0026rsquo;s Native Spark Datasource. Not only is this mechanism convenient, it is also very performant, leveraging parallelism in large datasets. The main API for the Python version we just used is just in the paragraph at the top of this notebook.\u003c/p\u003e\n\u003cp\u003eHere\u0026rsquo;s the result:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1543880333407_1557549193",
      "id": "20181202-173710_908436948",
      "dateCreated": "2018-12-03 23:38:53.407",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\nselect * from DS.foo;",
      "user": "anonymous",
      "dateUpdated": "2018-12-07 00:01:29.034",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 169.006,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "I": "string",
                      "F": "string",
                      "V": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "I\tF\tV\n0\t3.14\tTuring\n1\t4.14\tNewell\n2\t5.14\tSimon\n3\t6.14\tMinsky\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1543880333408_1085566372",
      "id": "20180118-015510_1158645327",
      "dateCreated": "2018-12-03 23:38:53.408",
      "dateStarted": "2018-12-07 00:01:29.088",
      "dateFinished": "2018-12-07 00:01:29.331",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n## 5. Run a Simple Splice Machine Transaction\n\nNow we\u0027ll add more data to that table in a transactional context: ",
      "user": "anonymous",
      "dateUpdated": "2018-12-03 23:38:53.409",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003e5. Run a Simple Splice Machine Transaction\u003c/h2\u003e\n\u003cp\u003eNow we\u0026rsquo;ll add more data to that table in a transactional context:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1543880333408_1060353056",
      "id": "20180129-154333_772550654",
      "dateCreated": "2018-12-03 23:38:53.408",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Transactional Context",
      "text": "%spark.pyspark\nconn \u003d splice.getConnection()\nconn.setAutoCommit(False)\nl \u003d [(4,3.14,\u0027Turing\u0027), (5,4.14,\u0027Newell\u0027), (6,5.14,\u0027Simon\u0027), (7,6.14,\u0027Minsky\u0027)]\nrdd \u003d sc.parallelize(l)\nrows \u003d rdd.map(lambda x: Row(I\u003dx[0], F\u003dfloat(x[1]), V\u003dstr(x[2])))\nschemaRows \u003d sqlContext.createDataFrame(rows)\nsplice.insert(schemaRows,\u0027DS.foo\u0027)\ndf \u003d splice.df(\"select * from DS.foo\")\ndf.collect\nz.show(df)",
      "user": "anonymous",
      "dateUpdated": "2018-12-07 00:01:34.856",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "fontSize": 9.0,
        "title": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "I": "string",
                      "F": "string",
                      "V": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "I\tF\tV\n0\t3.14\tTuring\n1\t4.14\tNewell\n2\t5.14\tSimon\n3\t6.14\tMinsky\n4\t3.14\tTuring\n5\t4.14\tNewell\n6\t5.14\tSimon\n7\t6.14\tMinsky\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://172.17.0.2:4041/jobs/job?id\u003d4",
            "http://172.17.0.2:4041/jobs/job?id\u003d7"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1543880333409_1088068790",
      "id": "20180118-015525_2011115580",
      "dateCreated": "2018-12-03 23:38:53.409",
      "dateStarted": "2018-12-07 00:01:34.895",
      "dateFinished": "2018-12-07 00:01:38.887",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n## 6. Rollback the transaction\n\nFinally, we\u0027ll rollback the transaction we just ran:\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-03 23:38:53.411",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003e6. Rollback the transaction\u003c/h2\u003e\n\u003cp\u003eFinally, we\u0026rsquo;ll rollback the transaction we just ran:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1543880333410_684958816",
      "id": "20180129-154912_774169413",
      "dateCreated": "2018-12-03 23:38:53.410",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Rollback",
      "text": "%spark.pyspark\nconn.rollback()\ndf \u003d splice.df(\"select * from DS.foo\")\ndf.collect\nz.show(df)",
      "user": "anonymous",
      "dateUpdated": "2018-12-07 00:01:44.103",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "title": false,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 159.006,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "I": "string",
                      "F": "string",
                      "V": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "I\tF\tV\n0\t3.14\tTuring\n1\t4.14\tNewell\n2\t5.14\tSimon\n3\t6.14\tMinsky\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://172.17.0.2:4041/jobs/job?id\u003d8"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1543880333411_1402892373",
      "id": "20180118-015552_1456308237",
      "dateCreated": "2018-12-03 23:38:53.411",
      "dateStarted": "2018-12-07 00:01:44.145",
      "dateFinished": "2018-12-07 00:01:46.245",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Where to Go Next\n\nNow let\u0027s explore Machine Learning with these Splice Machine, starting with our next notebook: [*Machine Learning with MLlib*](/#/notebook/2DYC2S8DY).",
      "user": "anonymous",
      "dateUpdated": "2018-12-04 00:02:13.427",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eWhere to Go Next\u003c/h2\u003e\n\u003cp\u003eNow let\u0026rsquo;s explore Machine Learning with these Splice Machine, starting with our next notebook: \u003ca href\u003d\"/#/notebook/2DYC2S8DY\"\u003e\u003cem\u003eMachine Learning with MLlib\u003c/em\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1543880333412_-76730605",
      "id": "20180122-173624_509126525",
      "dateCreated": "2018-12-03 23:38:53.412",
      "dateStarted": "2018-12-04 00:02:08.905",
      "dateFinished": "2018-12-04 00:02:08.914",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Splice Machine Training / For Data Scientists / e. Using our Native Spark DataSource",
  "id": "2DWHEE99H",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "splicemachine:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}