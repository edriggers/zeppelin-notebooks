{
  "paragraphs": [
    {
      "text": "%md\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\n# Machine Learning with Spark MLlib Using Scala\n\nThis notebook contains code that uses the Machine Learning (\u003cem\u003eML\u003c/em\u003e) Library embedded in Spark, *MLlib*, with the Splice Machine Spark Adapter to realize in-process machine learning. Specifically, the example in this notebook uses data that tracks international shipments to learn, and then predicts how late a shipment will be, based on various factors.\n\nIf you\u0027re not familiar with Machine Learning with Spark MLlib, you can learn more about this library here: \u003ca href\u003d\"https://spark.apache.org/docs/latest/ml-guide.html\" target\u003d\"_blank\"\u003ehttps://spark.apache.org/docs/latest/ml-guide.html\u003c/a\u003e.\n\nThe remainder of this notebook contains these sections:\n\n* \u003cem\u003eBasic Terminology\u003c/em\u003e defines a few major ML terms used in this notebook.\n* \u003cem\u003eAbout Our Sample Data\u003c/em\u003e introduces the shipping data that we use. \n* \u003cem\u003eAbout our Learning Model\u003c/em\u003e describes the learning model method we\u0027re using.\n* \u003cem\u003eCreating our Splice Machine Database\u003c/em\u003e walks you through setting up our database with our sample data.\n* \u003cem\u003eCreating, Training, and Deploying our Learning Model\u003c/em\u003e walks you through our Machine Learning sample code.\n* \u003cem\u003eProgram Listing\u003c/em\u003e contains a listing of all of the code used in this notebook.\n\n## Basic Terminology\n\nHere\u0027s some basic terminology you need to be familiar with to understand the code in this notebook. These descriptions are paraphrased from the \u003ca href\u003d\"https://spark.apache.org/docs/latest/ml-guide.html\" target\u003d\"_blank\"\u003eabove-mentioned Spark MLlib guide.\u003c/a\u003e\n\n\u003ctable class\u003d\"splicezep\"\u003e\n    \u003ccol /\u003e\n    \u003ccol /\u003e\n    \u003cthead\u003e\n        \u003ctr\u003e\n            \u003cth\u003eTerm\u003c/th\u003e\n            \u003cth\u003eDescription\u003c/th\u003e\n        \u003c/tr\u003e\n    \u003c/thead\u003e\n    \u003ctbody\u003e\n        \u003ctr\u003e\n            \u003ctd class\u003d\"ItalicFont\"\u003eDataFrame\u003c/td\u003e\n            \u003ctd\u003eA DataFrame is a basic Spark SQL concept. A DataFrame is similar to a table in a database: it contains rows of data with columns of varying types. The MLlib operates on datasets that are organized in DataFrames. \u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n            \u003ctd class\u003d\"ItalicFont\"\u003ePipeline\u003c/td\u003e\n            \u003ctd\u003eIn MLlib, you chain together a sequence of algorithms, or \u003cem\u003estages\u003c/em\u003e that operate on your DataFrame into a \u003cem\u003epipeline\u003c/em\u003e that learns.\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n            \u003ctd class\u003d\"ItalicFont\"\u003eTransformer\u003c/td\u003e\n            \u003ctd\u003eAn algorithm that transforms a DataFrame into another DataFrame. Each transformer implements a method named \u003ccode\u003etransform\u003c/code\u003e that converts the DataFrame, typically by appending additional columns to it. A \u003cem\u003emodel\u003c/em\u003e is a kind of transformer.\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n            \u003ctd class\u003d\"ItalicFont\"\u003eEstimator\u003c/td\u003e\n            \u003ctd\u003eA learning algorithm that trains or \u003cem\u003efits\u003c/em\u003e on a DataFrame and produces a \u003ccode\u003emodel\u003c/code\u003e. Each estimator implements a method named \u003ccode\u003efit\u003c/code\u003e that produces a model.\u003c/td\u003e\n        \u003c/tr\u003e\n    \u003c/tbody\u003e\n\u003c/table\u003e\n    \n\n## About our Sample Data\n\nWe\u0027ve obtained some actual shipping data that tracks international shipments between ports, and have imported that data into a Splice Machine database that we\u0027ve named `ASN.` The tables of interest are named `SHIPMENT_IN_TRANSIT` and `SHIPMENT_HISTORY;` you\u0027ll see these table used in the sample code below. We also create a database table named `Features` that forms the basis of the DataFrame we use for our learning model; this is the table you\u0027ll see featured in this notebook\u0027s code. The idea of this model is to predict, in real-time, how late a specific shipment will be, based on past data and other factors. Over time, as more data is processed by the model, the predictions become more accurate. \n\n## About our Learning Model\n\nWe use a Logistic Regression *estimator* as the final stage in our pipeline to produce a Logistic Regression Model of lateness from our data, and then deploy that model on a dataset to predict lateness.\n\nThe estimator operates on data that is formatted into vectors of integers. Since most of the fields in  our input dataset contain string values, we need to convert any data that will be used by the estimator into this format, as you\u0027ll see below. ",
      "user": "anonymous",
      "dateUpdated": "2018-11-20 11:06:44.276",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\u003ch1\u003eMachine Learning with Spark MLlib Using Scala\u003c/h1\u003e\n\u003cp\u003eThis notebook contains code that uses the Machine Learning (\u003cem\u003eML\u003c/em\u003e) Library embedded in Spark, \u003cem\u003eMLlib\u003c/em\u003e, with the Splice Machine Spark Adapter to realize in-process machine learning. Specifically, the example in this notebook uses data that tracks international shipments to learn, and then predicts how late a shipment will be, based on various factors.\u003c/p\u003e\n\u003cp\u003eIf you\u0026rsquo;re not familiar with Machine Learning with Spark MLlib, you can learn more about this library here: \u003ca href\u003d\"https://spark.apache.org/docs/latest/ml-guide.html\" target\u003d\"_blank\"\u003e\u003ca href\u003d\"https://spark.apache.org/docs/latest/ml-guide.html\"\u003ehttps://spark.apache.org/docs/latest/ml-guide.html\u003c/a\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe remainder of this notebook contains these sections:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\u003cem\u003eBasic Terminology\u003c/em\u003e defines a few major ML terms used in this notebook.\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eAbout Our Sample Data\u003c/em\u003e introduces the shipping data that we use.\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eAbout our Learning Model\u003c/em\u003e describes the learning model method we\u0026rsquo;re using.\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eCreating our Splice Machine Database\u003c/em\u003e walks you through setting up our database with our sample data.\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eCreating, Training, and Deploying our Learning Model\u003c/em\u003e walks you through our Machine Learning sample code.\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eProgram Listing\u003c/em\u003e contains a listing of all of the code used in this notebook.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eBasic Terminology\u003c/h2\u003e\n\u003cp\u003eHere\u0026rsquo;s some basic terminology you need to be familiar with to understand the code in this notebook. These descriptions are paraphrased from the \u003ca href\u003d\"https://spark.apache.org/docs/latest/ml-guide.html\" target\u003d\"_blank\"\u003eabove-mentioned Spark MLlib guide.\u003c/a\u003e\u003c/p\u003e\n\u003ctable class\u003d\"splicezep\"\u003e\n    \u003ccol /\u003e\n    \u003ccol /\u003e\n    \u003cthead\u003e\n        \u003ctr\u003e\n            \u003cth\u003eTerm\u003c/th\u003e\n            \u003cth\u003eDescription\u003c/th\u003e\n        \u003c/tr\u003e\n    \u003c/thead\u003e\n    \u003ctbody\u003e\n        \u003ctr\u003e\n            \u003ctd class\u003d\"ItalicFont\"\u003eDataFrame\u003c/td\u003e\n            \u003ctd\u003eA DataFrame is a basic Spark SQL concept. A DataFrame is similar to a table in a database: it contains rows of data with columns of varying types. The MLlib operates on datasets that are organized in DataFrames. \u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n            \u003ctd class\u003d\"ItalicFont\"\u003ePipeline\u003c/td\u003e\n            \u003ctd\u003eIn MLlib, you chain together a sequence of algorithms, or \u003cem\u003estages\u003c/em\u003e that operate on your DataFrame into a \u003cem\u003epipeline\u003c/em\u003e that learns.\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n            \u003ctd class\u003d\"ItalicFont\"\u003eTransformer\u003c/td\u003e\n            \u003ctd\u003eAn algorithm that transforms a DataFrame into another DataFrame. Each transformer implements a method named \u003ccode\u003etransform\u003c/code\u003e that converts the DataFrame, typically by appending additional columns to it. A \u003cem\u003emodel\u003c/em\u003e is a kind of transformer.\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n            \u003ctd class\u003d\"ItalicFont\"\u003eEstimator\u003c/td\u003e\n            \u003ctd\u003eA learning algorithm that trains or \u003cem\u003efits\u003c/em\u003e on a DataFrame and produces a \u003ccode\u003emodel\u003c/code\u003e. Each estimator implements a method named \u003ccode\u003efit\u003c/code\u003e that produces a model.\u003c/td\u003e\n        \u003c/tr\u003e\n    \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003eAbout our Sample Data\u003c/h2\u003e\n\u003cp\u003eWe\u0026rsquo;ve obtained some actual shipping data that tracks international shipments between ports, and have imported that data into a Splice Machine database that we\u0026rsquo;ve named \u003ccode\u003eASN.\u003c/code\u003e The tables of interest are named \u003ccode\u003eSHIPMENT_IN_TRANSIT\u003c/code\u003e and \u003ccode\u003eSHIPMENT_HISTORY;\u003c/code\u003e you\u0026rsquo;ll see these table used in the sample code below. We also create a database table named \u003ccode\u003eFeatures\u003c/code\u003e that forms the basis of the DataFrame we use for our learning model; this is the table you\u0026rsquo;ll see featured in this notebook\u0026rsquo;s code. The idea of this model is to predict, in real-time, how late a specific shipment will be, based on past data and other factors. Over time, as more data is processed by the model, the predictions become more accurate. \u003c/p\u003e\n\u003ch2\u003eAbout our Learning Model\u003c/h2\u003e\n\u003cp\u003eWe use a Logistic Regression \u003cem\u003eestimator\u003c/em\u003e as the final stage in our pipeline to produce a Logistic Regression Model of lateness from our data, and then deploy that model on a dataset to predict lateness.\u003c/p\u003e\n\u003cp\u003eThe estimator operates on data that is formatted into vectors of integers. Since most of the fields in our input dataset contain string values, we need to convert any data that will be used by the estimator into this format, as you\u0026rsquo;ll see below.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900748_-926908250",
      "id": "20180129-160012_924943773",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-20 11:06:44.276",
      "dateFinished": "2018-11-20 11:06:44.295",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n##  Creating our Splice Machine Database\n\nBefore working with the MLlib, we need to create a Splice Machine database that contains the shipping data we\u0027re using. We:\n\n1. Connect to you database via JDBC\n2. Create the schema and tables\n2. Import the data\n3. Create our features table\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-30 05:21:42.487",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eCreating our Splice Machine Database\u003c/h2\u003e\n\u003cp\u003eBefore working with the MLlib, we need to create a Splice Machine database that contains the shipping data we\u0026rsquo;re using. We:\u003c/p\u003e\n\u003col\u003e\n  \u003cli\u003eConnect to you database via JDBC\u003c/li\u003e\n  \u003cli\u003eCreate the schema and tables\u003c/li\u003e\n  \u003cli\u003eImport the data\u003c/li\u003e\n  \u003cli\u003eCreate our features table\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900749_1583688504",
      "id": "20180202-100849_1233744001",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-30 05:21:42.487",
      "dateFinished": "2018-11-30 05:21:42.501",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 1. Connect to Your Database via JDBC\n\nFirst we\u0027ll configure the URL we\u0027ll use in our JDBC connection to Splice Machine:\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-30 05:22:13.882",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003e1. Connect to Your Database via JDBC\u003c/h3\u003e\n\u003cp\u003eFirst we\u0026rsquo;ll configure the URL we\u0026rsquo;ll use in our JDBC connection to Splice Machine:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542762501844_853259015",
      "id": "20181120-170821_377250142",
      "dateCreated": "2018-11-20 17:08:21.844",
      "dateStarted": "2018-11-30 05:22:13.883",
      "dateFinished": "2018-11-30 05:22:13.889",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n  val defaultJDBCURL \u003d \"\"\"jdbc:splice://localhost:1527/splicedb;user\u003dsplice;password\u003dadmin\"\"\"\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-21 17:52:20.694",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {
          "JDBCurl": "jdbc:splice://localhost:1527/splicedb;user\u003dsplice;password\u003dadmin",
          "JDBCurl Scala": "jdbc:splice://localhost:1527/splicedb;user\u003dsplice;password\u003dadmin;useSpark\u003dtrue",
          "JDBCURL Scala": "jdbc:splice:/localhost:1527/splicedb;user\u003dsplice;password\u003dadmin",
          "JDBCxurl": "jdbc:splice://localhost:1527/splicedb;user\u003dsplice;password\u003dadmin;useSpark\u003dtrue"
        },
        "forms": {
          "JDBCurl": {
            "type": "TextBox",
            "name": "JDBCurl",
            "displayName": "JDBCurl",
            "defaultValue": "jdbc:splice://localhost:1527/splicedb;user\u003dsplice;password\u003dadmin",
            "hidden": false
          }
        }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Please copy and paste your JDBC URL. You can find it at the bottom right of your cluster dashboard\nwarning: there was one deprecation warning; re-run with -deprecation for details\ndefaultJDBCURL: String \u003d jdbc:splice://localhost:1527/splicedb;user\u003dsplice;password\u003dadmin\nlocalJDBCURL: String \u003d jdbc:splice://localhost:1527/splicedb;user\u003dsplice;password\u003dadmin\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900749_745584856",
      "id": "20180215-062654_2077965041",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n### 2. Create the Schema and Tables\n\nWe\u0027ll now create our new schema, make it our default schema, and then create the tables for the `shipment_in_transit` and `shipment_history` data that we will import.",
      "user": "anonymous",
      "dateUpdated": "2018-11-20 17:28:11.273",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003e2. Create the Schema and Tables\u003c/h3\u003e\n\u003cp\u003eWe\u0026rsquo;ll now create our new schema, make it our default schema, and then create the tables for the \u003ccode\u003eshipment_in_transit\u003c/code\u003e and \u003ccode\u003eshipment_history\u003c/code\u003e data that we will import.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542762538991_-1515467679",
      "id": "20181120-170858_626769940",
      "dateCreated": "2018-11-20 17:08:58.991",
      "dateStarted": "2018-11-20 17:28:11.274",
      "dateFinished": "2018-11-20 17:28:11.278",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\n\nCREATE SCHEMA ASN;\nSET SCHEMA ASN;\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:18:20.000",
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 4.0,
        "editorMode": "ace/mode/sql",
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Query executed successfully. Affected rows : 0"
          },
          {
            "type": "TEXT",
            "data": "Query executed successfully. Affected rows : 0"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900750_-1075245355",
      "id": "20180202-101539_620068341",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\n\nDROP TABLE IF EXISTS SHIPMENT_IN_TRANSIT;\nCREATE TABLE SHIPMENT_IN_TRANSIT(\n    SHIPMENTID VARCHAR(11) NOT NULL PRIMARY KEY,\n    STATUS VARCHAR(50),\n    SHIPMODE VARCHAR(30),\n    PRODUCT_DESCRIPTION VARCHAR(500),\n    CONSIGNEE VARCHAR(200),\n    SHIPPER VARCHAR(100),\n    ARRIVAL_DATE TIMESTAMP,\n    GROSS_WEIGHT_LB INTEGER,\n    GROSS_WEIGHT_KG INTEGER,\n    FOREIGN_PORT VARCHAR(50),\n    US_PORT VARCHAR(50),\n    VESSEL_NAME VARCHAR(40),\n    COUNTRY_OF_ORIGIN VARCHAR(40),\n    CONSIGNEE_ADDRESS VARCHAR(150),\n    SHIPPER_ADDRESS VARCHAR(150),\n    ZIPCODE VARCHAR(20),\n    NO_OF_CONTAINERS INTEGER,\n    CONTAINER_NUMBER VARCHAR(200),\n    CONTAINER_TYPE VARCHAR(80),\n    QUANTITY INTEGER,\n    QUANTITY_UNIT VARCHAR(10),\n    MEASUREMENT INTEGER,\n    MEASUREMENT_UNIT VARCHAR(5),\n    BILL_OF_LADING VARCHAR(20),\n    HOUSE_VS_MASTER CHAR(1),\n    DISTRIBUTION_PORT VARCHAR(40),\n    MASTER_BL VARCHAR(20),\n    VOYAGE_NUMBER VARCHAR(10),\n    SEAL VARCHAR(300),\n    SHIP_REGISTERED_IN VARCHAR(40),\n    INBOND_ENTRY_TYPE VARCHAR(30),\n    CARRIER_CODE VARCHAR(10),\n    CARRIER_NAME VARCHAR(40),\n    CARRIER_CITY VARCHAR(40),\n    CARRIER_STATE VARCHAR(10),\n    CARRIER_ZIP VARCHAR(10),\n    CARRIER_ADDRESS VARCHAR(200),\n    NOTIFY_PARTY VARCHAR(50),\n    NOTIFY_ADDRESS VARCHAR(200),\n    PLACE_OF_RECEIPT VARCHAR(50),\n    DATE_OF_RECEIPT TIMESTAMP\n    );\n\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:18:20.000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 4.0,
        "editorMode": "ace/mode/sql",
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395900750_-869324414",
      "id": "20180202-102107_567153190",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\nDROP TABLE IF EXISTS SHIPMENT_HISTORY;\nCREATE TABLE SHIPMENT_HISTORY(\n    SHIPMENTID VARCHAR(11) NOT NULL PRIMARY KEY,\n    STATUS VARCHAR(50),\n    SHIPMODE VARCHAR(30),\n    PRODUCT_DESCRIPTION VARCHAR(500),\n    CONSIGNEE VARCHAR(200),\n    SHIPPER VARCHAR(100),\n    ARRIVAL_DATE TIMESTAMP,\n    GROSS_WEIGHT_LB INTEGER,\n    GROSS_WEIGHT_KG INTEGER,\n    FOREIGN_PORT VARCHAR(50),\n    US_PORT VARCHAR(50),\n    VESSEL_NAME VARCHAR(40),\n    COUNTRY_OF_ORIGIN VARCHAR(40),\n    CONSIGNEE_ADDRESS VARCHAR(150),\n    SHIPPER_ADDRESS VARCHAR(150),\n    ZIPCODE VARCHAR(20),\n    NO_OF_CONTAINERS INTEGER,\n    CONTAINER_NUMBER VARCHAR(200),\n    CONTAINER_TYPE VARCHAR(80),\n    QUANTITY INTEGER,\n    QUANTITY_UNIT VARCHAR(10),\n    MEASUREMENT INTEGER,\n    MEASUREMENT_UNIT VARCHAR(5),\n    BILL_OF_LADING VARCHAR(20),\n    HOUSE_VS_MASTER CHAR(1),\n    DISTRIBUTION_PORT VARCHAR(40),\n    MASTER_BL VARCHAR(20),\n    VOYAGE_NUMBER VARCHAR(10),\n    SEAL VARCHAR(300),\n    SHIP_REGISTERED_IN VARCHAR(40),\n    INBOND_ENTRY_TYPE VARCHAR(30),\n    CARRIER_CODE VARCHAR(10),\n    CARRIER_NAME VARCHAR(40),\n    CARRIER_CITY VARCHAR(40),\n    CARRIER_STATE VARCHAR(10),\n    CARRIER_ZIP VARCHAR(10),\n    CARRIER_ADDRESS VARCHAR(200),\n    NOTIFY_PARTY VARCHAR(50),\n    NOTIFY_ADDRESS VARCHAR(200),\n    PLACE_OF_RECEIPT VARCHAR(50),\n    DATE_OF_RECEIPT TIMESTAMP\n);\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:18:20.000",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 4.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395900751_-38536210",
      "id": "20180202-102130_85303245",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 2. Import the Data\n\nNext we import the shipping data, which is in csv format, into our Splice Machine database.\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-20 17:28:14.096",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003e2. Import the Data\u003c/h3\u003e\n\u003cp\u003eNext we import the shipping data, which is in csv format, into our Splice Machine database.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900751_1680877428",
      "id": "20180202-104354_1962297047",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-20 17:28:14.097",
      "dateFinished": "2018-11-20 17:28:14.100",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\ncall SYSCS_UTIL.IMPORT_DATA (\n     \u0027ASN\u0027,\n     \u0027SHIPMENT_IN_TRANSIT\u0027,\n     null,\n     \u0027s3a://splice-demo/shipment/shipment_in_transit.csv\u0027,\n     \u0027|\u0027,\n     null,\n     \u0027yyyy-MM-dd HH:mm:ss.SSSSSS\u0027,\n     \u0027yyyy-MM-dd\u0027,\n     null,\n     -1,\n     \u0027/tmp\u0027,\n     true, null);",
      "user": "anonymous",
      "dateUpdated": "2018-11-30 05:23:26.874",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "rowsImported": "string",
                      "failedRows": "string",
                      "files": "string",
                      "dataSize": "string",
                      "failedLog": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395900752_86084010",
      "id": "20180202-104501_1686524282",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 3. Create our Features Table\n\nWe create a features table in our database that we use with our learning model. We add three computed fields in the `features` table that are important to our model:\n\n* `quantity_bin` categorizes shipping quantities into bins, to improve learning accuracy \n* `lateness` computes how many days late a shipment was\n* `label` categorizes lateness into one of four values:\n\n\u003ctable class\u003d\"spliceZepNoBorder\" style\u003d\"margin: 0 0 100px 50px;\"\u003e\n    \u003ctbody\u003e\n            \u003ctr\u003e\u003ctd\u003e0\u003c/td\u003e\u003ctd\u003e0 days late\u003c/td\u003e\u003c/tr\u003e\n            \u003ctr\u003e\u003ctd\u003e1\u003c/td\u003e\u003ctd\u003e1-5 days late\u003c/td\u003e\u003c/tr\u003e\n            \u003ctr\u003e\u003ctd\u003e2\u003c/td\u003e\u003ctd\u003e5-10 days late\u003c/td\u003e\u003c/tr\u003e\n            \u003ctr\u003e\u003ctd\u003e3\u003c/td\u003e\u003ctd\u003e10 days or more late\u003c/td\u003e\u003c/tr\u003e\n    \u003c/tbody\u003e\n\u003c/table\u003e",
      "user": "anonymous",
      "dateUpdated": "2018-11-20 17:03:03.543",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003e3. Create our Features Table\u003c/h3\u003e\n\u003cp\u003eWe create a features table in our database that we use with our learning model. We add three computed fields in the \u003ccode\u003efeatures\u003c/code\u003e table that are important to our model:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\u003ccode\u003equantity_bin\u003c/code\u003e categorizes shipping quantities into bins, to improve learning accuracy\u003c/li\u003e\n  \u003cli\u003e\u003ccode\u003elateness\u003c/code\u003e computes how many days late a shipment was\u003c/li\u003e\n  \u003cli\u003e\u003ccode\u003elabel\u003c/code\u003e categorizes lateness into one of four values:\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable class\u003d\"spliceZepNoBorder\" style\u003d\"margin: 0 0 100px 50px;\"\u003e\n    \u003ctbody\u003e\n            \u003ctr\u003e\u003ctd\u003e0\u003c/td\u003e\u003ctd\u003e0 days late\u003c/td\u003e\u003c/tr\u003e\n            \u003ctr\u003e\u003ctd\u003e1\u003c/td\u003e\u003ctd\u003e1-5 days late\u003c/td\u003e\u003c/tr\u003e\n            \u003ctr\u003e\u003ctd\u003e2\u003c/td\u003e\u003ctd\u003e5-10 days late\u003c/td\u003e\u003c/tr\u003e\n            \u003ctr\u003e\u003ctd\u003e3\u003c/td\u003e\u003ctd\u003e10 days or more late\u003c/td\u003e\u003c/tr\u003e\n    \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900753_1167702356",
      "id": "20180202-104618_659628734",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-20 17:03:03.543",
      "dateFinished": "2018-11-20 17:03:03.554",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\ndrop table IF EXISTS ASN.FEATURES;\nCREATE table ASN.FEATURES AS\n    SELECT\n        SHIPMENTID,\n        SHIPMODE,\n        CONSIGNEE,\n        SHIPPER,\n        ARRIVAL_DATE,\n        GROSS_WEIGHT_LB,\n        FOREIGN_PORT,\n        US_PORT,\n        VESSEL_NAME,\n        COUNTRY_OF_ORIGIN,\n        CONSIGNEE_ADDRESS,\n        SHIPPER_ADDRESS,\n        ZIPCODE,\n        NO_OF_CONTAINERS,\n        CONTAINER_NUMBER,\n        CONTAINER_TYPE,\n        QUANTITY,\n        QUANTITY_UNIT,\n        MEASUREMENT,\n        MEASUREMENT_UNIT,\n        BILL_OF_LADING,\n        HOUSE_VS_MASTER,\n        DISTRIBUTION_PORT,\n        MASTER_BL,\n        VOYAGE_NUMBER,\n        SEAL,\n        SHIP_REGISTERED_IN,\n        INBOND_ENTRY_TYPE,\n        CARRIER_CODE,\n        CARRIER_NAME,\n        CARRIER_CITY,\n        CARRIER_STATE,\n        CARRIER_ZIP,\n        CARRIER_ADDRESS,\n        NOTIFY_PARTY,\n        NOTIFY_ADDRESS,\n        PLACE_OF_RECEIPT,\n        DATE_OF_RECEIPT,\n        CASE\n        WHEN ASN.SHIPMENT_HISTORY.QUANTITY \u003e 10\n        THEN\n            CASE\n                WHEN ASN.SHIPMENT_HISTORY.QUANTITY \u003e 100\n                THEN\n                    CASE\n                        WHEN ASN.SHIPMENT_HISTORY.QUANTITY \u003e 1000\n                        THEN 3\n                        ELSE 2\n                    END\n                ELSE 1\n    \tEND\n        ELSE 0\n        END AS QUANTITY_BIN,\n        ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - ASN.SHIPMENT_HISTORY.ARRIVAL_DATE as LATENESS,\n        CASE\n        WHEN   ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - ASN.SHIPMENT_HISTORY.ARRIVAL_DATE \u003e 0\n        THEN\n            CASE\n                WHEN   ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - ASN.SHIPMENT_HISTORY.ARRIVAL_DATE \u003e 5\n                THEN\n                    CASE\n                        WHEN   ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - ASN.SHIPMENT_HISTORY.ARRIVAL_DATE \u003e 10\n                        THEN 3\n                        ELSE 2\n                    END\n                ELSE 1\n    \tEND\n        ELSE 0\n        END AS LABEL\n    FROM ASN.SHIPMENT_HISTORY",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:18:20.000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395900754_1495604585",
      "id": "20180202-104614_1527675689",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\ndrop table IF EXISTS ASN.FEATURES;\nCREATE table ASN.FEATURES AS\n    SELECT \n    SHIPMENTID,\n    STATUS,\n    SHIPMODE,\n    PRODUCT_DESCRIPTION,\n    CONSIGNEE,\n    SHIPPER,\n    ARRIVAL_DATE,\n    GROSS_WEIGHT_LB,\n    GROSS_WEIGHT_KG,\n    FOREIGN_PORT,\n    US_PORT,\n    VESSEL_NAME,\n    COUNTRY_OF_ORIGIN,\n    CONSIGNEE_ADDRESS,\n    SHIPPER_ADDRESS,\n    ZIPCODE,\n    NO_OF_CONTAINERS,\n    CONTAINER_NUMBER,\n    CONTAINER_TYPE,\n    QUANTITY,\n    QUANTITY_UNIT,\n    MEASUREMENT,\n    MEASUREMENT_UNIT,\n    BILL_OF_LADING,\n    HOUSE_VS_MASTER,\n    DISTRIBUTION_PORT,\n    MASTER_BL,\n    VOYAGE_NUMBER,\n    SEAL,\n    SHIP_REGISTERED_IN,\n    INBOND_ENTRY_TYPE,\n    CARRIER_CODE,\n    CARRIER_NAME,\n    CARRIER_CITY,\n    CARRIER_STATE,\n    CARRIER_ZIP,\n    CARRIER_ADDRESS,\n    NOTIFY_PARTY,\n    NOTIFY_ADDRESS,\n    PLACE_OF_RECEIPT,\n    DATE_OF_RECEIPT,\n    CASE\n    WHEN ASN.SHIPMENT_HISTORY.QUANTITY \u003e 10\n    THEN\n        CASE\n            WHEN ASN.SHIPMENT_HISTORY.QUANTITY \u003e 100\n            THEN\n                CASE\n                    WHEN ASN.SHIPMENT_HISTORY.QUANTITY \u003e 1000\n                    THEN 3\n                    ELSE 2\n                END\n            ELSE 1\n    END\n    ELSE 0\n    END AS QUANTITY_BIN,\n    ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - ASN.SHIPMENT_HISTORY.ARRIVAL_DATE as LATENESS,\n    CASE\n    WHEN  ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - ASN.SHIPMENT_HISTORY.ARRIVAL_DATE \u003e 0\n    THEN\n        CASE\n            WHEN  ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - ASN.SHIPMENT_HISTORY.ARRIVAL_DATE \u003e 5\n            THEN\n                CASE\n                    WHEN  ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - ASN.SHIPMENT_HISTORY.ARRIVAL_DATE \u003e 10\n                    THEN 3\n                    ELSE 2\n                END\n            ELSE 1\n    END\n    ELSE 0\n    END AS LABEL\nFROM ASN.SHIPMENT_HISTORY ",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:18:20.000",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395900754_2105484018",
      "id": "20180205-022847_1435938411",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Create, Train, and Deploy our Learning Model\n\nThe remainder of this notebook walks you through the code we use to create, train, and deploy our learning model, in these steps:\n\n1. *Perform Spark+MLlib Setup Tasks*\n2. *Create our DataFrame*\n3. *Create Pipeline Stages*\n4. *Assemble the Pipeline\u003eTrain our Model*\n5. *Deploy our Model*\n\nWe include the entire program at the end of this notebook.",
      "user": "anonymous",
      "dateUpdated": "2018-11-30 05:24:15.258",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eCreate, Train, and Deploy our Learning Model\u003c/h2\u003e\n\u003cp\u003eThe remainder of this notebook walks you through the code we use to create, train, and deploy our learning model, in these steps:\u003c/p\u003e\n\u003col\u003e\n  \u003cli\u003e\u003cem\u003ePerform Spark+MLlib Setup Tasks\u003c/em\u003e\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eCreate our DataFrame\u003c/em\u003e\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eCreate Pipeline Stages\u003c/em\u003e\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eAssemble the Pipeline\u0026gt;Train our Model\u003c/em\u003e\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eDeploy our Model\u003c/em\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eWe include the entire program at the end of this notebook.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900754_-678231194",
      "id": "20180131-172852_644197695",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-30 05:24:15.259",
      "dateFinished": "2018-11-30 05:24:15.268",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 1. Perform Spark+MLlib Setup Tasks\n\nFirst we set up our resources and initialize the Splice Machine Spark Adapter (`spliceMachineContext`). \n\n```\nimport org.apache.spark.ml.feature.VectorAssembler\nimport java.sql.{Connection,Timestamp}\nimport com.splicemachine.spark.splicemachine._\nimport org.apache.spark.sql.execution.datasources.jdbc.{JDBCOptions, JdbcUtils}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature.StringIndexer\nimport spark.implicits._\n\nval splicemachineContext \u003d new SplicemachineContext(defaultJDBCURL)\n```",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:18:20.000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003e1. Perform Spark+MLlib Setup Tasks\u003c/h3\u003e\n\u003cp\u003eFirst we set up our resources and initialize the Splice Machine Spark Adapter (\u003ccode\u003espliceMachineContext\u003c/code\u003e). \u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport org.apache.spark.ml.feature.VectorAssembler\nimport java.sql.{Connection,Timestamp}\nimport com.splicemachine.spark.splicemachine._\nimport org.apache.spark.sql.execution.datasources.jdbc.{JDBCOptions, JdbcUtils}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature.StringIndexer\nimport spark.implicits._\n\nval splicemachineContext \u003d new SplicemachineContext(defaultJDBCURL)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900755_-1564252731",
      "id": "20180130-185957_1517048437",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n#### Initialize our Context\n\nNow we initialize our context:\n\n```\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\n\nsplice \u003d PySpliceContext(defaultJDBCURL, sqlContext)\n```",
      "user": "anonymous",
      "dateUpdated": "2018-11-30 05:24:52.975",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch4\u003eInitialize our Context\u003c/h4\u003e\n\u003cp\u003eNow we initialize our context:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\n\nsplice \u003d PySpliceContext(defaultJDBCURL, sqlContext)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900755_602150681",
      "id": "20180611-203526_1071983380",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-30 05:24:52.976",
      "dateFinished": "2018-11-30 05:24:52.982",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 2. Create our DataFrame\n\nWe need to pull the schema from our shipping database, `ASN.Features`, and convert it into a Spark DataFrame. We then create a sequential list (a Scala `seq` object) of the features (fields) from that table that we want to include in our model, and concatenate that onto our DataFrame. `MLlib` expects the schema to contain uppercase field names, so we convert our sequence to uppercase with a built-in function. \n\n```\nval df_with_uppercase_schema \u003d splicemachineContext.df(\"select * from ASN.Features\")\nval newNames \u003d Seq(\n    \"consignee\",\n    \"shipper\",\n    \"shipmode\",\n    \"gross_weight_lb\",\n    \"foreign_port\",\n    \"us_port\",\n    \"vessel_name\",\n    \"country_of_origin\",\n    \"container_number\",\n    \"container_type\",\n    \"quantity\",\n    \"ship_registered_in\",\n    \"carrier_code\",\n    \"carrier_city\",\n    \"notify_party\",\n    \"place_of_receipt\",\n    \"zipcode\",\n    \"quantity_bin\"\n    )\nval df \u003d df_with_uppercase_schema.toDF(newNames: _*)\n```",
      "user": "anonymous",
      "dateUpdated": "2018-11-20 17:28:27.675",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003e2. Create our DataFrame\u003c/h3\u003e\n\u003cp\u003eWe need to pull the schema from our shipping database, \u003ccode\u003eASN.Features\u003c/code\u003e, and convert it into a Spark DataFrame. We then create a sequential list (a Scala \u003ccode\u003eseq\u003c/code\u003e object) of the features (fields) from that table that we want to include in our model, and concatenate that onto our DataFrame. \u003ccode\u003eMLlib\u003c/code\u003e expects the schema to contain uppercase field names, so we convert our sequence to uppercase with a built-in function. \u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval df_with_uppercase_schema \u003d splicemachineContext.df(\u0026quot;select * from ASN.Features\u0026quot;)\nval newNames \u003d Seq(\n    \u0026quot;consignee\u0026quot;,\n    \u0026quot;shipper\u0026quot;,\n    \u0026quot;shipmode\u0026quot;,\n    \u0026quot;gross_weight_lb\u0026quot;,\n    \u0026quot;foreign_port\u0026quot;,\n    \u0026quot;us_port\u0026quot;,\n    \u0026quot;vessel_name\u0026quot;,\n    \u0026quot;country_of_origin\u0026quot;,\n    \u0026quot;container_number\u0026quot;,\n    \u0026quot;container_type\u0026quot;,\n    \u0026quot;quantity\u0026quot;,\n    \u0026quot;ship_registered_in\u0026quot;,\n    \u0026quot;carrier_code\u0026quot;,\n    \u0026quot;carrier_city\u0026quot;,\n    \u0026quot;notify_party\u0026quot;,\n    \u0026quot;place_of_receipt\u0026quot;,\n    \u0026quot;zipcode\u0026quot;,\n    \u0026quot;quantity_bin\u0026quot;\n    )\nval df \u003d df_with_uppercase_schema.toDF(newNames: _*)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900756_728587372",
      "id": "20180130-190437_334148512",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-20 17:28:27.675",
      "dateFinished": "2018-11-20 17:28:27.681",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 3. Create Pipeline Stages\n\nOur pipeline stages are fairly simple:\n\n* Transform each row of data in the input dataset into an integer vector.\n* Assemble the vectors into a DataFrame\n* Use a Logistic Regression Estimator to create our model\n\n#### Transform each row of data into an integer vector\n\nThe Logistic Regression estimator operates on integer vectors, so we need to convert each row in our input dataframe into an integer vector. Remember that each row contains only the fields from our database that are of interest to our model: the fields that previously included in our sequence and concatenated onto our DataFrame.\n\nSpark includes a `StringIndexer` function that does exactly that, so we create a `StringIndexer` for each field, and we\u0027ll later use each of these as a stage in our learning pipeline. The `StringIndexer` transforms the data from a specified input column in our DataFrame and stores the output in a specified and new output column. By convention, we name each string indexer with the name of the field+`Indexer,` and name the output column the name of the field+`Index,` e.g. we create a transformer named `consigneeIndexer` to transform the input column `consignee` into the new output column `consigneeIndex.`\n\n```\n// Transform strings into numbers\nval consigneeIndexer \u003d new StringIndexer().setInputCol(\"consignee\").setOutputCol(\"consigneeIndex\").setHandleInvalid(\"skip\") \nval shipperIndexer \u003d new StringIndexer().setInputCol(\"shipper\").setOutputCol(\"shipperIndex\").setHandleInvalid(\"skip\")\nval shipmodeIndexer \u003d new StringIndexer().setInputCol(\"shipmode\").setOutputCol(\"shipmodeIndex\").setHandleInvalid(\"skip\") \nval gross_weight_lbIndexer \u003d new StringIndexer().setInputCol(\"gross_weight_lb\").setOutputCol(\"gross_weight_lbIndex\").setHandleInvalid(\"skip\") \nval foreign_portIndexer \u003d new StringIndexer().setInputCol(\"foreign_port\").setOutputCol(\"foreign_portIndex\").setHandleInvalid(\"skip\") \nval us_portIndexer \u003d new StringIndexer().setInputCol(\"us_port\").setOutputCol(\"us_portIndex\").setHandleInvalid(\"skip\") \nval vessel_nameIndexer \u003d new StringIndexer().setInputCol(\"vessel_name\").setOutputCol(\"vessel_nameIndex\").setHandleInvalid(\"skip\") \nval country_of_originIndexer \u003d new StringIndexer().setInputCol(\"country_of_origin\").setOutputCol(\"country_of_originIndex\").setHandleInvalid(\"skip\") \nval container_numberIndexer \u003d new StringIndexer().setInputCol(\"container_number\").setOutputCol(\"container_numberIndex\").setHandleInvalid(\"skip\")\nval container_typeIndexer \u003d new StringIndexer().setInputCol(\"container_type\").setOutputCol(\"container_typeIndex\").setHandleInvalid(\"skip\") \nval ship_registered_inIndexer \u003d new StringIndexer().setInputCol(\"ship_registered_in\").setOutputCol(\"ship_registered_inIndex\").setHandleInvalid(\"skip\") \nval carrier_codeIndexer \u003d new StringIndexer().setInputCol(\"carrier_code\").setOutputCol(\"carrier_codeIndex\").setHandleInvalid(\"skip\") \nval carrier_cityIndexer \u003d new StringIndexer().setInputCol(\"carrier_city\").setOutputCol(\"carrier_cityIndex\").setHandleInvalid(\"skip\") \nval notify_partyIndexer \u003d new StringIndexer().setInputCol(\"notify_party\").setOutputCol(\"notify_partyIndex\").setHandleInvalid(\"skip\") \nval place_of_receiptIndexer \u003d new StringIndexer().setInputCol(\"place_of_receipt\").setOutputCol(\"place_of_receiptIndex\").setHandleInvalid(\"skip\")\nval zipcodeIndexer \u003d new StringIndexer().setInputCol(\"zipcode\").setOutputCol(\"zipcodeIndex\").setHandleInvalid(\"skip\")\n```\n\n#### Assemble the Vectors\n\nAfter our pipeline has transformed data into numbers, we need to assemble those into vectors. Spark includes a `VectorAssembler` object that does just that, transforming a set of input columns into a vector that is stored in the `features` column in the DataFrame:\n\n```\n//assemble raw features\nval assembler \u003d new VectorAssembler()\n                .setInputCols(Array(\n                    \"shipmodeIndex\",\n                    \"consigneeIndex\",\n                    \"shipperIndex\",\n                    \"gross_weight_lbIndex\",\n                    \"foreign_portIndex\",\n                    \"us_portIndex\",\n                    \"vessel_nameIndex\",\n                    \"country_of_originIndex\",\n                    \"container_numberIndex\",\n                    \"container_typeIndex\",\n                    \"quantity_bin\",\n                    \"ship_registered_inIndex\",\n                    \"carrier_codeIndex\",\n                    \"carrier_cityIndex\",\n                    \"notify_partyIndex\",\n                    \"place_of_receiptIndex\",\n                    \"zipcodeIndex\",\n                    \"quantity_bin\"\n                    ))\n                .setOutputCol(\"features\")\n```\n\n#### Create the Estimator\n\nCreating the estimator is a simple matter of specifying a few parameters, including which column in the DataFrame is the label, and which column contains the feature set:\n\n```\n//Create ML analytic\nval lr \u003d new LogisticRegression()\n    .setMaxIter(30)\n    .setLabelCol(\"label\")\n    .setFeaturesCol(\"features\")\n    .set\n    RegParam(0.3)\n```\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-20 17:13:14.301",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003e3. Create Pipeline Stages\u003c/h3\u003e\n\u003cp\u003eOur pipeline stages are fairly simple:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eTransform each row of data in the input dataset into an integer vector.\u003c/li\u003e\n  \u003cli\u003eAssemble the vectors into a DataFrame\u003c/li\u003e\n  \u003cli\u003eUse a Logistic Regression Estimator to create our model\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eTransform each row of data into an integer vector\u003c/h4\u003e\n\u003cp\u003eThe Logistic Regression estimator operates on integer vectors, so we need to convert each row in our input dataframe into an integer vector. Remember that each row contains only the fields from our database that are of interest to our model: the fields that previously included in our sequence and concatenated onto our DataFrame.\u003c/p\u003e\n\u003cp\u003eSpark includes a \u003ccode\u003eStringIndexer\u003c/code\u003e function that does exactly that, so we create a \u003ccode\u003eStringIndexer\u003c/code\u003e for each field, and we\u0026rsquo;ll later use each of these as a stage in our learning pipeline. The \u003ccode\u003eStringIndexer\u003c/code\u003e transforms the data from a specified input column in our DataFrame and stores the output in a specified and new output column. By convention, we name each string indexer with the name of the field+\u003ccode\u003eIndexer,\u003c/code\u003e and name the output column the name of the field+\u003ccode\u003eIndex,\u003c/code\u003e e.g. we create a transformer named \u003ccode\u003econsigneeIndexer\u003c/code\u003e to transform the input column \u003ccode\u003econsignee\u003c/code\u003e into the new output column \u003ccode\u003econsigneeIndex.\u003c/code\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e// Transform strings into numbers\nval consigneeIndexer \u003d new StringIndexer().setInputCol(\u0026quot;consignee\u0026quot;).setOutputCol(\u0026quot;consigneeIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;) \nval shipperIndexer \u003d new StringIndexer().setInputCol(\u0026quot;shipper\u0026quot;).setOutputCol(\u0026quot;shipperIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;)\nval shipmodeIndexer \u003d new StringIndexer().setInputCol(\u0026quot;shipmode\u0026quot;).setOutputCol(\u0026quot;shipmodeIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;) \nval gross_weight_lbIndexer \u003d new StringIndexer().setInputCol(\u0026quot;gross_weight_lb\u0026quot;).setOutputCol(\u0026quot;gross_weight_lbIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;) \nval foreign_portIndexer \u003d new StringIndexer().setInputCol(\u0026quot;foreign_port\u0026quot;).setOutputCol(\u0026quot;foreign_portIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;) \nval us_portIndexer \u003d new StringIndexer().setInputCol(\u0026quot;us_port\u0026quot;).setOutputCol(\u0026quot;us_portIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;) \nval vessel_nameIndexer \u003d new StringIndexer().setInputCol(\u0026quot;vessel_name\u0026quot;).setOutputCol(\u0026quot;vessel_nameIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;) \nval country_of_originIndexer \u003d new StringIndexer().setInputCol(\u0026quot;country_of_origin\u0026quot;).setOutputCol(\u0026quot;country_of_originIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;) \nval container_numberIndexer \u003d new StringIndexer().setInputCol(\u0026quot;container_number\u0026quot;).setOutputCol(\u0026quot;container_numberIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;)\nval container_typeIndexer \u003d new StringIndexer().setInputCol(\u0026quot;container_type\u0026quot;).setOutputCol(\u0026quot;container_typeIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;) \nval ship_registered_inIndexer \u003d new StringIndexer().setInputCol(\u0026quot;ship_registered_in\u0026quot;).setOutputCol(\u0026quot;ship_registered_inIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;) \nval carrier_codeIndexer \u003d new StringIndexer().setInputCol(\u0026quot;carrier_code\u0026quot;).setOutputCol(\u0026quot;carrier_codeIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;) \nval carrier_cityIndexer \u003d new StringIndexer().setInputCol(\u0026quot;carrier_city\u0026quot;).setOutputCol(\u0026quot;carrier_cityIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;) \nval notify_partyIndexer \u003d new StringIndexer().setInputCol(\u0026quot;notify_party\u0026quot;).setOutputCol(\u0026quot;notify_partyIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;) \nval place_of_receiptIndexer \u003d new StringIndexer().setInputCol(\u0026quot;place_of_receipt\u0026quot;).setOutputCol(\u0026quot;place_of_receiptIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;)\nval zipcodeIndexer \u003d new StringIndexer().setInputCol(\u0026quot;zipcode\u0026quot;).setOutputCol(\u0026quot;zipcodeIndex\u0026quot;).setHandleInvalid(\u0026quot;skip\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eAssemble the Vectors\u003c/h4\u003e\n\u003cp\u003eAfter our pipeline has transformed data into numbers, we need to assemble those into vectors. Spark includes a \u003ccode\u003eVectorAssembler\u003c/code\u003e object that does just that, transforming a set of input columns into a vector that is stored in the \u003ccode\u003efeatures\u003c/code\u003e column in the DataFrame:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e//assemble raw features\nval assembler \u003d new VectorAssembler()\n                .setInputCols(Array(\n                    \u0026quot;shipmodeIndex\u0026quot;,\n                    \u0026quot;consigneeIndex\u0026quot;,\n                    \u0026quot;shipperIndex\u0026quot;,\n                    \u0026quot;gross_weight_lbIndex\u0026quot;,\n                    \u0026quot;foreign_portIndex\u0026quot;,\n                    \u0026quot;us_portIndex\u0026quot;,\n                    \u0026quot;vessel_nameIndex\u0026quot;,\n                    \u0026quot;country_of_originIndex\u0026quot;,\n                    \u0026quot;container_numberIndex\u0026quot;,\n                    \u0026quot;container_typeIndex\u0026quot;,\n                    \u0026quot;quantity_bin\u0026quot;,\n                    \u0026quot;ship_registered_inIndex\u0026quot;,\n                    \u0026quot;carrier_codeIndex\u0026quot;,\n                    \u0026quot;carrier_cityIndex\u0026quot;,\n                    \u0026quot;notify_partyIndex\u0026quot;,\n                    \u0026quot;place_of_receiptIndex\u0026quot;,\n                    \u0026quot;zipcodeIndex\u0026quot;,\n                    \u0026quot;quantity_bin\u0026quot;\n                    ))\n                .setOutputCol(\u0026quot;features\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eCreate the Estimator\u003c/h4\u003e\n\u003cp\u003eCreating the estimator is a simple matter of specifying a few parameters, including which column in the DataFrame is the label, and which column contains the feature set:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e//Create ML analytic\nval lr \u003d new LogisticRegression()\n    .setMaxIter(30)\n    .setLabelCol(\u0026quot;label\u0026quot;)\n    .setFeaturesCol(\u0026quot;features\u0026quot;)\n    .set\n    RegParam(0.3)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900756_1858794936",
      "id": "20180130-190241_1891417263",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-20 17:13:14.301",
      "dateFinished": "2018-11-20 17:13:14.319",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 4. Assemble our Pipeline\n\nNow that we\u0027ve got our stages set up, we\u0027re ready to assemble our Machine Learning pipeline, which chains those stages together in sequence:\n\n```\n// Chain indexers and tree in a Pipeline\nval lrPipeline \u003d new Pipeline().setStages(\n        Array(consigneeIndexer,\n                shipperIndexer,\n                shipmodeIndexer,\n                gross_weight_lbIndexer,\n                foreign_portIndexer,\n                us_portIndexer,\n                vessel_nameIndexer,\n                country_of_originIndexer,\n                container_numberIndexer,\n                container_typeIndexer,\n                ship_registered_inIndexer,\n                carrier_codeIndexer,\n                carrier_cityIndexer,\n                notify_partyIndexer,\n                place_of_receiptIndexer,\n                zipcodeIndexer,\n                assembler,\n                lr\n                ))\n```",
      "user": "anonymous",
      "dateUpdated": "2018-11-30 05:25:31.886",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003e4. Assemble our Pipeline\u003c/h3\u003e\n\u003cp\u003eNow that we\u0026rsquo;ve got our stages set up, we\u0026rsquo;re ready to assemble our Machine Learning pipeline, which chains those stages together in sequence:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e// Chain indexers and tree in a Pipeline\nval lrPipeline \u003d new Pipeline().setStages(\n        Array(consigneeIndexer,\n                shipperIndexer,\n                shipmodeIndexer,\n                gross_weight_lbIndexer,\n                foreign_portIndexer,\n                us_portIndexer,\n                vessel_nameIndexer,\n                country_of_originIndexer,\n                container_numberIndexer,\n                container_typeIndexer,\n                ship_registered_inIndexer,\n                carrier_codeIndexer,\n                carrier_cityIndexer,\n                notify_partyIndexer,\n                place_of_receiptIndexer,\n                zipcodeIndexer,\n                assembler,\n                lr\n                ))\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900757_32313036",
      "id": "20180130-201715_1588482510",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-30 05:25:31.886",
      "dateFinished": "2018-11-30 05:25:31.893",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 5. Train our Model\n\nNow that our pipeline is set up, all we need to do to train our model is feed our dataframe into the pipeline\u0027s `fit` method, which learns from the data. \n```\n// Train model. \nval lrModel \u003d lrPipeline.fit(df)\n```\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-20 17:13:44.618",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003e5. Train our Model\u003c/h3\u003e\n\u003cp\u003eNow that our pipeline is set up, all we need to do to train our model is feed our dataframe into the pipeline\u0026rsquo;s \u003ccode\u003efit\u003c/code\u003e method, which learns from the data. \u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e// Train model. \nval lrModel \u003d lrPipeline.fit(df)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900758_-510863185",
      "id": "20180130-201949_1288501052",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-20 17:13:44.618",
      "dateFinished": "2018-11-20 17:13:44.623",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Using Spark MLlib",
      "text": "%md\n### 6. Materialize the Model\n\nNow that we\u0027ve trained our model, we can apply it to real data and display the results. For simplicity sake, we\u0027ll simply apply the model to our feature table itself.\n\n```\nlrModel.transform(df).select(\"prediction\", \"probability\", \"features\").show(100)\n```",
      "user": "anonymous",
      "dateUpdated": "2018-11-20 17:14:01.627",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "title": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003e6. Materialize the Model\u003c/h3\u003e\n\u003cp\u003eNow that we\u0026rsquo;ve trained our model, we can apply it to real data and display the results. For simplicity sake, we\u0026rsquo;ll simply apply the model to our feature table itself.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003elrModel.transform(df).select(\u0026quot;prediction\u0026quot;, \u0026quot;probability\u0026quot;, \u0026quot;features\u0026quot;).show(100)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900759_897973427",
      "id": "20180118-020316_1913850778",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-20 17:14:01.628",
      "dateFinished": "2018-11-20 17:14:01.632",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\nselect *  from ASN.features { limit 100 }\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:18:20.000",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "SHIPMENTID": "string",
                      "STATUS": "string",
                      "SHIPMODE": "string",
                      "PRODUCT_DESCRIPTION": "string",
                      "CONSIGNEE": "string",
                      "SHIPPER": "string",
                      "ARRIVAL_DATE": "string",
                      "GROSS_WEIGHT_LB": "string",
                      "GROSS_WEIGHT_KG": "string",
                      "FOREIGN_PORT": "string",
                      "US_PORT": "string",
                      "VESSEL_NAME": "string",
                      "COUNTRY_OF_ORIGIN": "string",
                      "CONSIGNEE_ADDRESS": "string",
                      "SHIPPER_ADDRESS": "string",
                      "ZIPCODE": "string",
                      "NO_OF_CONTAINERS": "string",
                      "CONTAINER_NUMBER": "string",
                      "CONTAINER_TYPE": "string",
                      "QUANTITY": "string",
                      "QUANTITY_UNIT": "string",
                      "MEASUREMENT": "string",
                      "MEASUREMENT_UNIT": "string",
                      "BILL_OF_LADING": "string",
                      "HOUSE_VS_MASTER": "string",
                      "DISTRIBUTION_PORT": "string",
                      "MASTER_BL": "string",
                      "VOYAGE_NUMBER": "string",
                      "SEAL": "string",
                      "SHIP_REGISTERED_IN": "string",
                      "INBOND_ENTRY_TYPE": "string",
                      "CARRIER_CODE": "string",
                      "CARRIER_NAME": "string",
                      "CARRIER_CITY": "string",
                      "CARRIER_STATE": "string",
                      "CARRIER_ZIP": "string",
                      "CARRIER_ADDRESS": "string",
                      "NOTIFY_PARTY": "string",
                      "NOTIFY_ADDRESS": "string",
                      "PLACE_OF_RECEIPT": "string",
                      "DATE_OF_RECEIPT": "string",
                      "QUANTITY_BIN": "string",
                      "LATENESS": "string",
                      "LABEL": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395900759_1824152226",
      "id": "20180205-024307_1130627818",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\nDROP TABLE IF EXISTS ASN.PREDICTIONS;\nCREATE TABLE ASN.PREDICTIONS (\n    SHIPMENTID VARCHAR(11) NOT NULL PRIMARY KEY,\n    PREDICTION DOUBLE\n    );",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:18:20.000",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395900759_-2137361316",
      "id": "20180205-024429_478433529",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## The Scala Code \n\nThe Scala code is listed here:",
      "user": "anonymous",
      "dateUpdated": "2018-11-20 17:14:29.612",
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eThe Scala Code\u003c/h2\u003e\n\u003cp\u003eThe Scala code is listed here:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900765_312337259",
      "id": "20180611-220634_1810473505",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-20 17:14:29.613",
      "dateFinished": "2018-11-20 17:14:29.616",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nimport org.apache.spark.ml.feature.VectorAssembler\nimport java.sql.{Connection}\nimport com.splicemachine.spark.splicemachine._\nimport org.apache.spark.sql.execution.datasources.jdbc.{JDBCOptions, JdbcUtils}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature.StringIndexer\nimport spark.implicits._\n\n\nval optionMap \u003d Map(\n  JDBCOptions.JDBC_TABLE_NAME -\u003e \"ASN.Features\",\n  JDBCOptions.JDBC_URL -\u003e defaultJDBCURL\n)\nval splicemachineContext \u003d new SplicemachineContext(defaultJDBCURL)\nval df_with_uppercase_schema \u003d splicemachineContext.df(\"select * from ASN.Features\")\nval newNames \u003d Seq(\n    \"shipmentid\",\n    \"status\",\n    \"shipmode\",\n    \"product_description\",\n    \"consignee\",\n    \"shipper\",\n    \"arrival_date\",\n    \"gross_weight_lb\",\n    \"gross_weight_kg\",\n    \"foreign_port\",\n    \"us_port\",\n    \"vessel_name\",\n    \"country_of_origin\",\n    \"consignee_address\",\n    \"shipper_address\",\n    \"zipcode\",\n    \"no_of_containers\",\n    \"container_number\",\n    \"container_type\",\n    \"quantity\",\n    \"quantity_unit\",\n    \"measurement\",\n    \"measurement_unit\",\n    \"bill_of_lading\",\n    \"house_vs_master\",\n    \"distribution_port\",\n    \"master_bl\",\n    \"voyage_number\",\n    \"seal\",\n    \"ship_registered_in\",\n    \"inbond_entry_type\",\n    \"carrier_code\",\n    \"carrier_name\",\n    \"carrier_city\",\n    \"carrier_state\",\n    \"carrier_zip\",\n    \"carrier_address\",\n    \"notify_party\",\n    \"notify_address\",\n    \"place_of_receipt\",\n    \"date_of_receipt\",\n    \"quantity_bin\",\n    \"lateness\",\n    \"label\"\n)\nval df \u003d df_with_uppercase_schema.toDF(newNames: _*)\n\n//assemble raw features\nval assembler \u003d new VectorAssembler().\n                setInputCols(Array(\n                    \"consigneeIndex\",\n                    \"shipperIndex\",\n                    \"gross_weight_lbIndex\",\n                    \"foreign_portIndex\",\n                    \"us_portIndex\",\n                    \"vessel_nameIndex\",\n                    \"country_of_originIndex\",\n                    \"container_numberIndex\",\n                    \"container_typeIndex\",\n                    \"quantity_bin\",\n                    \"ship_registered_inIndex\",\n                    \"carrier_codeIndex\",\n                    \"carrier_cityIndex\",\n                    \"notify_partyIndex\",\n                    \"place_of_receiptIndex\",\n                    \"zipcodeIndex\"\n                    )).\n                setOutputCol(\"features\")\n\n// Transform strings into numbers\nval zipcodeIndexer \u003d new StringIndexer().setInputCol(\"zipcode\").setOutputCol(\"zipcodeIndex\").setHandleInvalid(\"skip\")\nval consigneeIndexer \u003d new StringIndexer().setInputCol(\"consignee\").setOutputCol(\"consigneeIndex\").setHandleInvalid(\"skip\") \nval shipperIndexer \u003d new StringIndexer().setInputCol(\"shipper\").setOutputCol(\"shipperIndex\").setHandleInvalid(\"skip\")\nval statusIndexer \u003d new StringIndexer().setInputCol(\"status\").setOutputCol(\"statusIndex\").setHandleInvalid(\"skip\") \nval shipmodeIndexer \u003d new StringIndexer().setInputCol(\"shipmode\").setOutputCol(\"shipmodeIndex\").setHandleInvalid(\"skip\") \nval gross_weight_lbIndexer \u003d new StringIndexer().setInputCol(\"gross_weight_lb\").setOutputCol(\"gross_weight_lbIndex\").setHandleInvalid(\"skip\") \nval foreign_portIndexer \u003d new StringIndexer().setInputCol(\"foreign_port\").setOutputCol(\"foreign_portIndex\").setHandleInvalid(\"skip\") \nval us_portIndexer \u003d new StringIndexer().setInputCol(\"us_port\").setOutputCol(\"us_portIndex\").setHandleInvalid(\"skip\") \nval vessel_nameIndexer \u003d new StringIndexer().setInputCol(\"vessel_name\").setOutputCol(\"vessel_nameIndex\").setHandleInvalid(\"skip\") \nval country_of_originIndexer \u003d new StringIndexer().setInputCol(\"country_of_origin\").setOutputCol(\"country_of_originIndex\").setHandleInvalid(\"skip\") \nval container_numberIndexer \u003d new StringIndexer().setInputCol(\"container_number\").setOutputCol(\"container_numberIndex\").setHandleInvalid(\"skip\")\nval container_typeIndexer \u003d new StringIndexer().setInputCol(\"container_type\").setOutputCol(\"container_typeIndex\").setHandleInvalid(\"skip\") \nval distribution_portIndexer \u003d new StringIndexer().setInputCol(\"distribution_port\").setOutputCol(\"distribution_portIndex\").setHandleInvalid(\"skip\") \nval ship_registered_inIndexer \u003d new StringIndexer().setInputCol(\"ship_registered_in\").setOutputCol(\"ship_registered_inIndex\").setHandleInvalid(\"skip\") \nval inbond_entry_typeIndexer \u003d new StringIndexer().setInputCol(\"inbond_entry_type\").setOutputCol(\"inbond_entry_typeIndex\").setHandleInvalid(\"skip\") \nval carrier_codeIndexer \u003d new StringIndexer().setInputCol(\"carrier_code\").setOutputCol(\"carrier_codeIndex\").setHandleInvalid(\"skip\") \nval carrier_cityIndexer \u003d new StringIndexer().setInputCol(\"carrier_city\").setOutputCol(\"carrier_cityIndex\").setHandleInvalid(\"skip\") \nval carrier_stateIndexer \u003d new StringIndexer().setInputCol(\"carrier_state\").setOutputCol(\"carrier_stateIndex\").setHandleInvalid(\"skip\") \nval carrier_zipIndexer \u003d new StringIndexer().setInputCol(\"carrier_zip\").setOutputCol(\"carrier_zipIndex\").setHandleInvalid(\"skip\") \nval notify_partyIndexer \u003d new StringIndexer().setInputCol(\"notify_party\").setOutputCol(\"notify_partyIndex\").setHandleInvalid(\"skip\") \nval place_of_receiptIndexer \u003d new StringIndexer().setInputCol(\"place_of_receipt\").setOutputCol(\"place_of_receiptIndex\").setHandleInvalid(\"skip\") \n\n//Create ML analytic\nval lr \u003d new LogisticRegression().\n    setMaxIter(30).\n    setLabelCol(\"label\").\n    setFeaturesCol(\"features\").\n    setRegParam(0.3)\n\n\n// Chain indexers and tree in a Pipeline\nval lrPipeline \u003d new Pipeline().setStages(\n        Array(consigneeIndexer,\n                shipperIndexer,\n                shipmodeIndexer,\n                gross_weight_lbIndexer,\n                foreign_portIndexer,\n                us_portIndexer,\n                vessel_nameIndexer,\n                country_of_originIndexer,\n                container_numberIndexer,\n                container_typeIndexer,\n                ship_registered_inIndexer,\n                carrier_codeIndexer,\n                carrier_cityIndexer,\n                notify_partyIndexer,\n                place_of_receiptIndexer,\n                zipcodeIndexer,\n                assembler,\n                lr\n                ))\n\n// Train model. \nval lrModel \u003d lrPipeline.fit(df)\n\nlrModel.transform(df).select(\"prediction\", \"probability\", \"features\").show(100)\n\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:18:20.000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395900770_835891442",
      "id": "20180130-201612_556450692",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\ndrop table IF EXISTS ASN.TEST_FEATURES;\nCREATE table ASN.TEST_FEATURES AS\n    SELECT \n    SHIPMENTID,\n    STATUS,\n    SHIPMODE,\n    PRODUCT_DESCRIPTION,\n    CONSIGNEE,\n    SHIPPER,\n    ARRIVAL_DATE,\n    GROSS_WEIGHT_LB,\n    GROSS_WEIGHT_KG,\n    FOREIGN_PORT,\n    US_PORT,\n    VESSEL_NAME,\n    COUNTRY_OF_ORIGIN,\n    CONSIGNEE_ADDRESS,\n    SHIPPER_ADDRESS,\n    ZIPCODE,\n    NO_OF_CONTAINERS,\n    CONTAINER_NUMBER,\n    CONTAINER_TYPE,\n    QUANTITY,\n    QUANTITY_UNIT,\n    MEASUREMENT,\n    MEASUREMENT_UNIT,\n    BILL_OF_LADING,\n    HOUSE_VS_MASTER,\n    DISTRIBUTION_PORT,\n    MASTER_BL,\n    VOYAGE_NUMBER,\n    SEAL,\n    SHIP_REGISTERED_IN,\n    INBOND_ENTRY_TYPE,\n    CARRIER_CODE,\n    CARRIER_NAME,\n    CARRIER_CITY,\n    CARRIER_STATE,\n    CARRIER_ZIP,\n    CARRIER_ADDRESS,\n    NOTIFY_PARTY,\n    NOTIFY_ADDRESS,\n    PLACE_OF_RECEIPT,\n    DATE_OF_RECEIPT,\n    CASE\n    WHEN ASN.SHIPMENT_IN_TRANSIT.QUANTITY \u003e 10\n    THEN\n        CASE\n            WHEN ASN.SHIPMENT_IN_TRANSIT.QUANTITY \u003e 100\n            THEN\n                CASE\n                    WHEN ASN.SHIPMENT_IN_TRANSIT.QUANTITY \u003e 1000\n                    THEN 3\n                    ELSE 2\n                END\n            ELSE 1\n\tEND\n    ELSE 0\n    END AS QUANTITY_BIN\n    FROM ASN.SHIPMENT_IN_TRANSIT;\n    \n",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:18:20.000",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395900771_1776441852",
      "id": "20180304-185514_1167859763",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n## Testing the Code\n\nNow we\u0027ll test our code on the `testing` features table:",
      "user": "anonymous",
      "dateUpdated": "2018-11-20 17:27:23.624",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eTesting the Code\u003c/h2\u003e\n\u003cp\u003eNow we\u0026rsquo;ll test our code on the \u003ccode\u003etesting\u003c/code\u003e features table:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900771_659397738",
      "id": "20180612-031525_84277416",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-20 17:14:58.670",
      "dateFinished": "2018-11-20 17:14:58.673",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, LongType, BooleanType };\n\n    val test_data_with_uppercase_schema \u003d splicemachineContext.df(\"select * from ASN.TEST_FEATURES\")\n    val newNames \u003d Seq(\n        \"shipmentid\",\n        \"status\",\n        \"shipmode\",\n        \"product_description\",\n        \"consignee\",\n        \"shipper\",\n        \"arrival_date\",\n        \"gross_weight_lb\",\n        \"gross_weight_kg\",\n        \"foreign_port\",\n        \"us_port\",\n        \"vessel_name\",\n        \"country_of_origin\",\n        \"consignee_address\",\n        \"shipper_address\",\n        \"zipcode\",\n        \"no_of_containers\",\n        \"container_number\",\n        \"container_type\",\n        \"quantity\",\n        \"quantity_unit\",\n        \"measurement\",\n        \"measurement_unit\",\n        \"bill_of_lading\",\n        \"house_vs_master\",\n        \"distribution_port\",\n        \"master_bl\",\n        \"voyage_number\",\n        \"seal\",\n        \"ship_registered_in\",\n        \"inbond_entry_type\",\n        \"carrier_code\",\n        \"carrier_name\",\n        \"carrier_city\",\n        \"carrier_state\",\n        \"carrier_zip\",\n        \"carrier_address\",\n        \"notify_party\",\n        \"notify_address\",\n        \"place_of_receipt\",\n        \"date_of_receipt\",\n        \"quantity_bin\"\n    )\n    val test_data \u003d test_data_with_uppercase_schema.toDF(newNames: _*)\n\n    // Make predictions.\n    val lrPredictions \u003d lrModel.transform(test_data)\n\n    var z \u003d Array(\"Zara\", \"Nuha\", \"Ayan\")\n\n    // Select example rows to display.\n     val innerStruct \u003d\n   StructType(\n     StructField(\"f1\", IntegerType, true) ::\n     StructField(\"f2\", LongType, false) ::\n     StructField(\"f3\", BooleanType, false) :: Nil)\n    lrPredictions.select(\"prediction\", \"probability\",\"features\").show(10)\n    splicemachineContext.createTable(\"tabletest\", innerStruct, z, null)",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:18:20.000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395900772_-909627617",
      "id": "20180125-142959_1101825868",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nimport org.apache.spark.sql.types.{StructType, StructField, StringType};\n\nval predictions \u003d lrPredictions.select(\"SHIPMENTID\", \"PREDICTION\")\n\npredictions.printSchema()\n \nsplicemachineContext.insert(predictions,\"ASN.predictions\") \n\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:18:20.000",
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.types.{StructType, StructField, StringType}\npredictions: org.apache.spark.sql.DataFrame \u003d [SHIPMENTID: string, PREDICTION: double]\nroot\n |-- SHIPMENTID: string (nullable \u003d true)\n |-- PREDICTION: double (nullable \u003d true)\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900772_1514447564",
      "id": "20180304-185743_1150073560",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\nselect * from ASN.predictions;",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:18:20.000",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 318.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "SHIPMENTID": "string",
                      "PREDICTION": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395900773_1334841789",
      "id": "20180304-190035_379792791",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Where to Go Next\n\nThe next notebook in this class, [*Creating Custom Stored Procedures*](/#/notebook/2DWAGKSPM), shows you how to create and use custom stored procedures with Splice Machine.\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-20 11:08:16.730",
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eWhere to Go Next\u003c/h2\u003e\n\u003cp\u003eThe next notebook in this class, \u003ca href\u003d\"/#/notebook/2DWAGKSPM\"\u003e\u003cem\u003eCreating Custom Stored Procedures\u003c/em\u003e\u003c/a\u003e, shows you how to create and use custom stored procedures with Splice Machine.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900773_911637495",
      "id": "20180304-190115_1109571681",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-20 11:08:16.731",
      "dateFinished": "2018-11-20 11:08:16.737",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-20 11:08:16.703",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542740896703_483275809",
      "id": "20181120-110816_675707832",
      "dateCreated": "2018-11-20 11:08:16.703",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Splice Machine Training/For Developers, Part II - Intermediate/j. ML with Spark MLlib Using Scala",
  "id": "2DX1UV9MX",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "splicemachine:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}