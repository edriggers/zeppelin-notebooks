{
  "paragraphs": [
    {
      "text": "%md\n<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" />\n\n# Machine Learning with Spark MLlib Using Scala\n\nThis notebook contains code that uses the Machine Learning (<em>ML</em>) Library embedded in Spark, *MLlib*, with the Splice Machine Spark Adapter to realize in-process machine learning. Specifically, the example in this notebook uses data that tracks international shipments to learn, and then predicts how late a shipment will be, based on various factors.\n\n<p class=\"noteIcon\">This notebook implements the same functionality as the previous notebook in this class, <em>Machine Learning with Spark MLlib Using Python</em>. This notebook uses Scala instead of Python as the implementation language.</p> \n\nThe remainder of this notebook contains these sections:\n\n* <em>Creating our Splice Machine Database</em> walks you through setting up our database with our sample data.\n* <em>Creating, Training, and Deploying our Learning Model</em> walks you through our Machine Learning sample code.\n* <em>Program Listing</em> contains a listing of all of the code used in this notebook.",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:28:58.052",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" />\n<h1>Machine Learning with Spark MLlib Using Scala</h1>\n<p>This notebook contains code that uses the Machine Learning (<em>ML</em>) Library embedded in Spark, <em>MLlib</em>, with the Splice Machine Spark Adapter to realize in-process machine learning. Specifically, the example in this notebook uses data that tracks international shipments to learn, and then predicts how late a shipment will be, based on various factors.</p>\n<p class=\"noteIcon\">This notebook implements the same functionality as the previous notebook in this class, <em>Machine Learning with Spark MLlib Using Python</em>. This notebook uses Scala instead of Python as the implementation language.</p>\n<p>The remainder of this notebook contains these sections:</p>\n<ul>\n  <li><em>Creating our Splice Machine Database</em> walks you through setting up our database with our sample data.</li>\n  <li><em>Creating, Training, and Deploying our Learning Model</em> walks you through our Machine Learning sample code.</li>\n  <li><em>Program Listing</em> contains a listing of all of the code used in this notebook.</li>\n</ul>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900748_-926908250",
      "id": "20180129-160012_924943773",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-12-01 05:28:46.127",
      "dateFinished": "2018-12-01 05:28:46.147",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n##  Creating our Splice Machine Database\n\nBefore working with the MLlib, we need to create a Splice Machine database that contains the shipping data we're using. We:\n\n1. Connect to you database via JDBC\n2. Create the schema and tables\n2. Import the data\n3. Create our features table\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:29:02.085",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Creating our Splice Machine Database</h2>\n<p>Before working with the MLlib, we need to create a Splice Machine database that contains the shipping data we&rsquo;re using. We:</p>\n<ol>\n  <li>Connect to you database via JDBC</li>\n  <li>Create the schema and tables</li>\n  <li>Import the data</li>\n  <li>Create our features table</li>\n</ol>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900749_1583688504",
      "id": "20180202-100849_1233744001",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-30 05:21:42.487",
      "dateFinished": "2018-11-30 05:21:42.501",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 1. Connect to Your Database via JDBC\n\nFirst we'll configure the URL we'll use in our JDBC connection to Splice Machine. \n\nFor this class, you can simply use the `defaultJDBCURL` assignment in the next paragraph. When running on a cluster, you can copy and paste the JDBC URL you'll find displayed at the bottom right of your cluster dashboard.",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:29:10.303",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>1. Connect to Your Database via JDBC</h3>\n<p>First we&rsquo;ll configure the URL we&rsquo;ll use in our JDBC connection to Splice Machine. </p>\n<p>For this class, you can simply use the <code>defaultJDBCURL</code> assignment in the next paragraph. When running on a cluster, you can copy and paste the JDBC URL you&rsquo;ll find displayed at the bottom right of your cluster dashboard.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542762501844_853259015",
      "id": "20181120-170821_377250142",
      "dateCreated": "2018-11-20 17:08:21.844",
      "dateStarted": "2018-12-01 05:29:07.495",
      "dateFinished": "2018-12-01 05:29:07.512",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n  val defaultJDBCURL = \"\"\"jdbc:splice://localhost:1527/splicedb;user=splice;password=admin\"\"\"\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-21 17:52:20.694",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {
          "JDBCurl": "jdbc:splice://localhost:1527/splicedb;user=splice;password=admin",
          "JDBCurl Scala": "jdbc:splice://localhost:1527/splicedb;user=splice;password=admin;useSpark=true",
          "JDBCURL Scala": "jdbc:splice:/localhost:1527/splicedb;user=splice;password=admin",
          "JDBCxurl": "jdbc:splice://localhost:1527/splicedb;user=splice;password=admin;useSpark=true"
        },
        "forms": {
          "JDBCurl": {
            "type": "TextBox",
            "name": "JDBCurl",
            "displayName": "JDBCurl",
            "defaultValue": "jdbc:splice://localhost:1527/splicedb;user=splice;password=admin",
            "hidden": false
          }
        }
      },
      "apps": [],
      "jobName": "paragraph_1542395900749_745584856",
      "id": "20180215-062654_2077965041",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n### 2. Create the Schema and Tables\n\nWe'll now create our new schema, make it our default schema, and then create the tables for the `shipment_in_transit` and `shipment_history` data that we will import.",
      "user": "anonymous",
      "dateUpdated": "2018-11-20 17:28:11.273",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>2. Create the Schema and Tables</h3>\n<p>We&rsquo;ll now create our new schema, make it our default schema, and then create the tables for the <code>shipment_in_transit</code> and <code>shipment_history</code> data that we will import.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542762538991_-1515467679",
      "id": "20181120-170858_626769940",
      "dateCreated": "2018-11-20 17:08:58.991",
      "dateStarted": "2018-11-20 17:28:11.274",
      "dateFinished": "2018-11-20 17:28:11.278",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\n\nCREATE SCHEMA DEV2_ASN;\nSET SCHEMA DEV2_ASN;\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:30:14.841",
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 4.0,
        "editorMode": "ace/mode/sql",
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Query executed successfully. Affected rows : 0"
          },
          {
            "type": "TEXT",
            "data": "Query executed successfully. Affected rows : 0"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900750_-1075245355",
      "id": "20180202-101539_620068341",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\n\nDROP TABLE IF EXISTS SHIPMENT_IN_TRANSIT;\nCREATE TABLE SHIPMENT_IN_TRANSIT(\n    SHIPMENTID VARCHAR(11) NOT NULL PRIMARY KEY,\n    STATUS VARCHAR(50),\n    SHIPMODE VARCHAR(30),\n    PRODUCT_DESCRIPTION VARCHAR(500),\n    CONSIGNEE VARCHAR(200),\n    SHIPPER VARCHAR(100),\n    ARRIVAL_DATE TIMESTAMP,\n    GROSS_WEIGHT_LB INTEGER,\n    GROSS_WEIGHT_KG INTEGER,\n    FOREIGN_PORT VARCHAR(50),\n    US_PORT VARCHAR(50),\n    VESSEL_NAME VARCHAR(40),\n    COUNTRY_OF_ORIGIN VARCHAR(40),\n    CONSIGNEE_ADDRESS VARCHAR(150),\n    SHIPPER_ADDRESS VARCHAR(150),\n    ZIPCODE VARCHAR(20),\n    NO_OF_CONTAINERS INTEGER,\n    CONTAINER_NUMBER VARCHAR(200),\n    CONTAINER_TYPE VARCHAR(80),\n    QUANTITY INTEGER,\n    QUANTITY_UNIT VARCHAR(10),\n    MEASUREMENT INTEGER,\n    MEASUREMENT_UNIT VARCHAR(5),\n    BILL_OF_LADING VARCHAR(20),\n    HOUSE_VS_MASTER CHAR(1),\n    DISTRIBUTION_PORT VARCHAR(40),\n    MASTER_BL VARCHAR(20),\n    VOYAGE_NUMBER VARCHAR(10),\n    SEAL VARCHAR(300),\n    SHIP_REGISTERED_IN VARCHAR(40),\n    INBOND_ENTRY_TYPE VARCHAR(30),\n    CARRIER_CODE VARCHAR(10),\n    CARRIER_NAME VARCHAR(40),\n    CARRIER_CITY VARCHAR(40),\n    CARRIER_STATE VARCHAR(10),\n    CARRIER_ZIP VARCHAR(10),\n    CARRIER_ADDRESS VARCHAR(200),\n    NOTIFY_PARTY VARCHAR(50),\n    NOTIFY_ADDRESS VARCHAR(200),\n    PLACE_OF_RECEIPT VARCHAR(50),\n    DATE_OF_RECEIPT TIMESTAMP\n    );\n\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:18:20.000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 4.0,
        "editorMode": "ace/mode/sql",
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395900750_-869324414",
      "id": "20180202-102107_567153190",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\nDROP TABLE IF EXISTS SHIPMENT_HISTORY;\nCREATE TABLE SHIPMENT_HISTORY(\n    SHIPMENTID VARCHAR(11) NOT NULL PRIMARY KEY,\n    STATUS VARCHAR(50),\n    SHIPMODE VARCHAR(30),\n    PRODUCT_DESCRIPTION VARCHAR(500),\n    CONSIGNEE VARCHAR(200),\n    SHIPPER VARCHAR(100),\n    ARRIVAL_DATE TIMESTAMP,\n    GROSS_WEIGHT_LB INTEGER,\n    GROSS_WEIGHT_KG INTEGER,\n    FOREIGN_PORT VARCHAR(50),\n    US_PORT VARCHAR(50),\n    VESSEL_NAME VARCHAR(40),\n    COUNTRY_OF_ORIGIN VARCHAR(40),\n    CONSIGNEE_ADDRESS VARCHAR(150),\n    SHIPPER_ADDRESS VARCHAR(150),\n    ZIPCODE VARCHAR(20),\n    NO_OF_CONTAINERS INTEGER,\n    CONTAINER_NUMBER VARCHAR(200),\n    CONTAINER_TYPE VARCHAR(80),\n    QUANTITY INTEGER,\n    QUANTITY_UNIT VARCHAR(10),\n    MEASUREMENT INTEGER,\n    MEASUREMENT_UNIT VARCHAR(5),\n    BILL_OF_LADING VARCHAR(20),\n    HOUSE_VS_MASTER CHAR(1),\n    DISTRIBUTION_PORT VARCHAR(40),\n    MASTER_BL VARCHAR(20),\n    VOYAGE_NUMBER VARCHAR(10),\n    SEAL VARCHAR(300),\n    SHIP_REGISTERED_IN VARCHAR(40),\n    INBOND_ENTRY_TYPE VARCHAR(30),\n    CARRIER_CODE VARCHAR(10),\n    CARRIER_NAME VARCHAR(40),\n    CARRIER_CITY VARCHAR(40),\n    CARRIER_STATE VARCHAR(10),\n    CARRIER_ZIP VARCHAR(10),\n    CARRIER_ADDRESS VARCHAR(200),\n    NOTIFY_PARTY VARCHAR(50),\n    NOTIFY_ADDRESS VARCHAR(200),\n    PLACE_OF_RECEIPT VARCHAR(50),\n    DATE_OF_RECEIPT TIMESTAMP\n);\n",
      "user": "anonymous",
      "dateUpdated": "2018-11-16 11:18:20.000",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 4.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395900751_-38536210",
      "id": "20180202-102130_85303245",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 2. Import the Data\n\nNext we import the shipping data, which is in csv format, into our Splice Machine database.\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:30:17.979",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>2. Import the Data</h3>\n<p>Next we import the shipping data, which is in csv format, into our Splice Machine database.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900751_1680877428",
      "id": "20180202-104354_1962297047",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-20 17:28:14.097",
      "dateFinished": "2018-11-20 17:28:14.100",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\ncall SYSCS_UTIL.IMPORT_DATA (\n     'DEV2_ASN',\n     'SHIPMENT_IN_TRANSIT',\n     null,\n     's3a://splice-demo/shipment/shipment_in_transit.csv',\n     '|',\n     null,\n     'yyyy-MM-dd HH:mm:ss.SSSSSS',\n     'yyyy-MM-dd',\n     null,\n     -1,\n     '/tmp',\n     true, null);",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:30:26.584",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "rowsImported": "string",
                      "failedRows": "string",
                      "files": "string",
                      "dataSize": "string",
                      "failedLog": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395900752_86084010",
      "id": "20180202-104501_1686524282",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\ncall SYSCS_UTIL.IMPORT_DATA (\n     'DEV2_ASN',\n     'SHIPMENT_HISTORY',\n     null,\n     's3a://splice-demo/shipment/shipment_history.csv',\n     '|',\n     null,\n     'yyyy-MM-dd HH:mm:ss.SSSSSS',\n     'yyyy-MM-dd',\n     null,\n     -1,\n     '/tmp',\n     true, null);",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:30:35.983",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "rowsImported": "string",
                      "failedRows": "string",
                      "files": "string",
                      "dataSize": "string",
                      "failedLog": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1543614624957_163405087",
      "id": "20181130-215024_317065913",
      "dateCreated": "2018-11-30 21:50:24.957",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 3. Create our Features Table\n\nWe create a features table in our database that we use with our learning model. We add three computed fields in the `features` table that are important to our model:\n\n* `quantity_bin` categorizes shipping quantities into bins, to improve learning accuracy \n* `lateness` computes how many days late a shipment was\n* `label` categorizes lateness into one of four values:\n\n<table class=\"spliceZepNoBorder\" style=\"margin: 0 0 100px 50px;\">\n    <tbody>\n            <tr><td>0</td><td>0 days late</td></tr>\n            <tr><td>1</td><td>1-5 days late</td></tr>\n            <tr><td>2</td><td>5-10 days late</td></tr>\n            <tr><td>3</td><td>10 days or more late</td></tr>\n    </tbody>\n</table>",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:30:37.664",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>3. Create our Features Table</h3>\n<p>We create a features table in our database that we use with our learning model. We add three computed fields in the <code>features</code> table that are important to our model:</p>\n<ul>\n  <li><code>quantity_bin</code> categorizes shipping quantities into bins, to improve learning accuracy</li>\n  <li><code>lateness</code> computes how many days late a shipment was</li>\n  <li><code>label</code> categorizes lateness into one of four values:</li>\n</ul>\n<table class=\"spliceZepNoBorder\" style=\"margin: 0 0 100px 50px;\">\n    <tbody>\n            <tr><td>0</td><td>0 days late</td></tr>\n            <tr><td>1</td><td>1-5 days late</td></tr>\n            <tr><td>2</td><td>5-10 days late</td></tr>\n            <tr><td>3</td><td>10 days or more late</td></tr>\n    </tbody>\n</table>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900753_1167702356",
      "id": "20180202-104618_659628734",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-20 17:03:03.543",
      "dateFinished": "2018-11-20 17:03:03.554",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\ndrop table IF EXISTS DEV2_ASN.FEATURES;\nCREATE table DEV2_ASN.FEATURES AS\n    SELECT\n        SHIPMENTID,\n        SHIPMODE,\n        CONSIGNEE,\n        SHIPPER,\n        ARRIVAL_DATE,\n        GROSS_WEIGHT_LB,\n        FOREIGN_PORT,\n        US_PORT,\n        VESSEL_NAME,\n        COUNTRY_OF_ORIGIN,\n        CONSIGNEE_ADDRESS,\n        SHIPPER_ADDRESS,\n        ZIPCODE,\n        NO_OF_CONTAINERS,\n        CONTAINER_NUMBER,\n        CONTAINER_TYPE,\n        QUANTITY,\n        QUANTITY_UNIT,\n        MEASUREMENT,\n        MEASUREMENT_UNIT,\n        BILL_OF_LADING,\n        HOUSE_VS_MASTER,\n        DISTRIBUTION_PORT,\n        MASTER_BL,\n        VOYAGE_NUMBER,\n        SEAL,\n        SHIP_REGISTERED_IN,\n        INBOND_ENTRY_TYPE,\n        CARRIER_CODE,\n        CARRIER_NAME,\n        CARRIER_CITY,\n        CARRIER_STATE,\n        CARRIER_ZIP,\n        CARRIER_ADDRESS,\n        NOTIFY_PARTY,\n        NOTIFY_ADDRESS,\n        PLACE_OF_RECEIPT,\n        DATE_OF_RECEIPT,\n        CASE\n        WHEN DEV2_ASN.SHIPMENT_HISTORY.QUANTITY > 10\n        THEN\n            CASE\n                WHEN DEV2_ASN.SHIPMENT_HISTORY.QUANTITY > 100\n                THEN\n                    CASE\n                        WHEN DEV2_ASN.SHIPMENT_HISTORY.QUANTITY > 1000\n                        THEN 3\n                        ELSE 2\n                    END\n                ELSE 1\n    \tEND\n        ELSE 0\n        END AS QUANTITY_BIN,\n        DEV2_ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - DEV2_ASN.SHIPMENT_HISTORY.ARRIVAL_DATE as LATENESS,\n        CASE\n        WHEN   DEV2_ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - DEV2_ASN.SHIPMENT_HISTORY.ARRIVAL_DATE > 0\n        THEN\n            CASE\n                WHEN   DEV2_ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - DEV2_ASN.SHIPMENT_HISTORY.ARRIVAL_DATE > 5\n                THEN\n                    CASE\n                        WHEN   DEV2_ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - DEV2_ASN.SHIPMENT_HISTORY.ARRIVAL_DATE > 10\n                        THEN 3\n                        ELSE 2\n                    END\n                ELSE 1\n    \tEND\n        ELSE 0\n        END AS LABEL\n    FROM DEV2_ASN.SHIPMENT_HISTORY",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:32:04.808",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395900754_1495604585",
      "id": "20180202-104614_1527675689",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\ndrop table IF EXISTS DEV2_ASN.FEATURES;\nCREATE table DEV2_ASN.FEATURES AS\n    SELECT \n    SHIPMENTID,\n    STATUS,\n    SHIPMODE,\n    PRODUCT_DESCRIPTION,\n    CONSIGNEE,\n    SHIPPER,\n    ARRIVAL_DATE,\n    GROSS_WEIGHT_LB,\n    GROSS_WEIGHT_KG,\n    FOREIGN_PORT,\n    US_PORT,\n    VESSEL_NAME,\n    COUNTRY_OF_ORIGIN,\n    CONSIGNEE_ADDRESS,\n    SHIPPER_ADDRESS,\n    ZIPCODE,\n    NO_OF_CONTAINERS,\n    CONTAINER_NUMBER,\n    CONTAINER_TYPE,\n    QUANTITY,\n    QUANTITY_UNIT,\n    MEASUREMENT,\n    MEASUREMENT_UNIT,\n    BILL_OF_LADING,\n    HOUSE_VS_MASTER,\n    DISTRIBUTION_PORT,\n    MASTER_BL,\n    VOYAGE_NUMBER,\n    SEAL,\n    SHIP_REGISTERED_IN,\n    INBOND_ENTRY_TYPE,\n    CARRIER_CODE,\n    CARRIER_NAME,\n    CARRIER_CITY,\n    CARRIER_STATE,\n    CARRIER_ZIP,\n    CARRIER_ADDRESS,\n    NOTIFY_PARTY,\n    NOTIFY_ADDRESS,\n    PLACE_OF_RECEIPT,\n    DATE_OF_RECEIPT,\n    CASE\n    WHEN DEV2_ASN.SHIPMENT_HISTORY.QUANTITY > 10\n    THEN\n        CASE\n            WHEN DEV2_ASN.SHIPMENT_HISTORY.QUANTITY > 100\n            THEN\n                CASE\n                    WHEN DEV2_ASN.SHIPMENT_HISTORY.QUANTITY > 1000\n                    THEN 3\n                    ELSE 2\n                END\n            ELSE 1\n    END\n    ELSE 0\n    END AS QUANTITY_BIN,\n    DEV2_ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - DEV2_ASN.SHIPMENT_HISTORY.ARRIVAL_DATE as LATENESS,\n    CASE\n    WHEN  DEV2_ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - DEV2_ASN.SHIPMENT_HISTORY.ARRIVAL_DATE > 0\n    THEN\n        CASE\n            WHEN  DEV2_ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - DEV2_ASN.SHIPMENT_HISTORY.ARRIVAL_DATE > 5\n            THEN\n                CASE\n                    WHEN  DEV2_ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - DEV2_ASN.SHIPMENT_HISTORY.ARRIVAL_DATE > 10\n                    THEN 3\n                    ELSE 2\n                END\n            ELSE 1\n    END\n    ELSE 0\n    END AS LABEL\nFROM DEV2_ASN.SHIPMENT_HISTORY ",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:32:57.719",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395900754_2105484018",
      "id": "20180205-022847_1435938411",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Create, Train, and Deploy our Learning Model\n\nThe remainder of this notebook walks you through the code we use to create, train, and deploy our learning model, in these steps:\n\n1. *Perform Spark+MLlib Setup Tasks*\n2. *Create our DataFrame*\n3. *Create Pipeline Stages*\n4. *Assemble the Pipeline>Train our Model*\n5. *Deploy our Model*\n\nWe include the entire program at the end of this notebook.",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:33:34.091",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Create, Train, and Deploy our Learning Model</h2>\n<p>The remainder of this notebook walks you through the code we use to create, train, and deploy our learning model, in these steps:</p>\n<ol>\n  <li><em>Perform Spark+MLlib Setup Tasks</em></li>\n  <li><em>Create our DataFrame</em></li>\n  <li><em>Create Pipeline Stages</em></li>\n  <li><em>Assemble the Pipeline&gt;Train our Model</em></li>\n  <li><em>Deploy our Model</em></li>\n</ol>\n<p>We include the entire program at the end of this notebook.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900754_-678231194",
      "id": "20180131-172852_644197695",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-30 05:24:15.259",
      "dateFinished": "2018-11-30 05:24:15.268",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 1. Perform Spark+MLlib Setup Tasks\n\nFirst we set up our resources and initialize the Splice Machine Spark Adapter (`spliceMachineContext`). \n\n```\nimport org.apache.spark.ml.feature.VectorAssembler\nimport java.sql.{Connection,Timestamp}\nimport com.splicemachine.spark.splicemachine._\nimport org.apache.spark.sql.execution.datasources.jdbc.{JDBCOptions, JdbcUtils}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature.StringIndexer\nimport spark.implicits._\n\nval splicemachineContext = new SplicemachineContext(defaultJDBCURL)\n```",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:33:41.961",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>1. Perform Spark+MLlib Setup Tasks</h3>\n<p>First we set up our resources and initialize the Splice Machine Spark Adapter (<code>spliceMachineContext</code>). </p>\n<pre><code>import org.apache.spark.ml.feature.VectorAssembler\nimport java.sql.{Connection,Timestamp}\nimport com.splicemachine.spark.splicemachine._\nimport org.apache.spark.sql.execution.datasources.jdbc.{JDBCOptions, JdbcUtils}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature.StringIndexer\nimport spark.implicits._\n\nval splicemachineContext = new SplicemachineContext(defaultJDBCURL)\n</code></pre>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900755_-1564252731",
      "id": "20180130-185957_1517048437",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n#### Initialize our Context\n\nNow we initialize our context:\n\n```\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\n\nsplice = PySpliceContext(defaultJDBCURL, sqlContext)\n```",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:33:47.225",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h4>Initialize our Context</h4>\n<p>Now we initialize our context:</p>\n<pre><code>from pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\n\nsplice = PySpliceContext(defaultJDBCURL, sqlContext)\n</code></pre>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900755_602150681",
      "id": "20180611-203526_1071983380",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-30 05:24:52.976",
      "dateFinished": "2018-11-30 05:24:52.982",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 2. Create our DataFrame\n\nWe need to pull the schema from our shipping database, `DEV2_ASN.Features`, and convert it into a Spark DataFrame. We then create a sequential list (a Scala `seq` object) of the features (fields) from that table that we want to include in our model, and concatenate that onto our DataFrame. `MLlib` expects the schema to contain uppercase field names, so we convert our sequence to uppercase with a built-in function. \n\n```\nval df_with_uppercase_schema = splicemachineContext.df(\"select * from DEV2_ASN.Features\")\nval newNames = Seq(\n    \"consignee\",\n    \"shipper\",\n    \"shipmode\",\n    \"gross_weight_lb\",\n    \"foreign_port\",\n    \"us_port\",\n    \"vessel_name\",\n    \"country_of_origin\",\n    \"container_number\",\n    \"container_type\",\n    \"quantity\",\n    \"ship_registered_in\",\n    \"carrier_code\",\n    \"carrier_city\",\n    \"notify_party\",\n    \"place_of_receipt\",\n    \"zipcode\",\n    \"quantity_bin\"\n    )\nval df = df_with_uppercase_schema.toDF(newNames: _*)\n```",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:34:12.200",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>2. Create our DataFrame</h3>\n<p>We need to pull the schema from our shipping database, <code>DEV2_ASN.Features</code>, and convert it into a Spark DataFrame. We then create a sequential list (a Scala <code>seq</code> object) of the features (fields) from that table that we want to include in our model, and concatenate that onto our DataFrame. <code>MLlib</code> expects the schema to contain uppercase field names, so we convert our sequence to uppercase with a built-in function. </p>\n<pre><code>val df_with_uppercase_schema = splicemachineContext.df(&quot;select * from DEV2_ASN.Features&quot;)\nval newNames = Seq(\n    &quot;consignee&quot;,\n    &quot;shipper&quot;,\n    &quot;shipmode&quot;,\n    &quot;gross_weight_lb&quot;,\n    &quot;foreign_port&quot;,\n    &quot;us_port&quot;,\n    &quot;vessel_name&quot;,\n    &quot;country_of_origin&quot;,\n    &quot;container_number&quot;,\n    &quot;container_type&quot;,\n    &quot;quantity&quot;,\n    &quot;ship_registered_in&quot;,\n    &quot;carrier_code&quot;,\n    &quot;carrier_city&quot;,\n    &quot;notify_party&quot;,\n    &quot;place_of_receipt&quot;,\n    &quot;zipcode&quot;,\n    &quot;quantity_bin&quot;\n    )\nval df = df_with_uppercase_schema.toDF(newNames: _*)\n</code></pre>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900756_728587372",
      "id": "20180130-190437_334148512",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-12-01 05:34:12.202",
      "dateFinished": "2018-12-01 05:34:12.224",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 3. Create Pipeline Stages\n\nOur pipeline stages are fairly simple:\n\n* Transform each row of data in the input dataset into an integer vector.\n* Assemble the vectors into a DataFrame\n* Use a Logistic Regression Estimator to create our model\n\n#### Transform each row of data into an integer vector\n\nThe Logistic Regression estimator operates on integer vectors, so we need to convert each row in our input dataframe into an integer vector. Remember that each row contains only the fields from our database that are of interest to our model: the fields that previously included in our sequence and concatenated onto our DataFrame.\n\nSpark includes a `StringIndexer` function that does exactly that, so we create a `StringIndexer` for each field, and we'll later use each of these as a stage in our learning pipeline. The `StringIndexer` transforms the data from a specified input column in our DataFrame and stores the output in a specified and new output column. By convention, we name each string indexer with the name of the field+`Indexer,` and name the output column the name of the field+`Index,` e.g. we create a transformer named `consigneeIndexer` to transform the input column `consignee` into the new output column `consigneeIndex.`\n\n```\n// Transform strings into numbers\nval consigneeIndexer = new StringIndexer().setInputCol(\"consignee\").setOutputCol(\"consigneeIndex\").setHandleInvalid(\"skip\") \nval shipperIndexer = new StringIndexer().setInputCol(\"shipper\").setOutputCol(\"shipperIndex\").setHandleInvalid(\"skip\")\nval shipmodeIndexer = new StringIndexer().setInputCol(\"shipmode\").setOutputCol(\"shipmodeIndex\").setHandleInvalid(\"skip\") \nval gross_weight_lbIndexer = new StringIndexer().setInputCol(\"gross_weight_lb\").setOutputCol(\"gross_weight_lbIndex\").setHandleInvalid(\"skip\") \nval foreign_portIndexer = new StringIndexer().setInputCol(\"foreign_port\").setOutputCol(\"foreign_portIndex\").setHandleInvalid(\"skip\") \nval us_portIndexer = new StringIndexer().setInputCol(\"us_port\").setOutputCol(\"us_portIndex\").setHandleInvalid(\"skip\") \nval vessel_nameIndexer = new StringIndexer().setInputCol(\"vessel_name\").setOutputCol(\"vessel_nameIndex\").setHandleInvalid(\"skip\") \nval country_of_originIndexer = new StringIndexer().setInputCol(\"country_of_origin\").setOutputCol(\"country_of_originIndex\").setHandleInvalid(\"skip\") \nval container_numberIndexer = new StringIndexer().setInputCol(\"container_number\").setOutputCol(\"container_numberIndex\").setHandleInvalid(\"skip\")\nval container_typeIndexer = new StringIndexer().setInputCol(\"container_type\").setOutputCol(\"container_typeIndex\").setHandleInvalid(\"skip\") \nval ship_registered_inIndexer = new StringIndexer().setInputCol(\"ship_registered_in\").setOutputCol(\"ship_registered_inIndex\").setHandleInvalid(\"skip\") \nval carrier_codeIndexer = new StringIndexer().setInputCol(\"carrier_code\").setOutputCol(\"carrier_codeIndex\").setHandleInvalid(\"skip\") \nval carrier_cityIndexer = new StringIndexer().setInputCol(\"carrier_city\").setOutputCol(\"carrier_cityIndex\").setHandleInvalid(\"skip\") \nval notify_partyIndexer = new StringIndexer().setInputCol(\"notify_party\").setOutputCol(\"notify_partyIndex\").setHandleInvalid(\"skip\") \nval place_of_receiptIndexer = new StringIndexer().setInputCol(\"place_of_receipt\").setOutputCol(\"place_of_receiptIndex\").setHandleInvalid(\"skip\")\nval zipcodeIndexer = new StringIndexer().setInputCol(\"zipcode\").setOutputCol(\"zipcodeIndex\").setHandleInvalid(\"skip\")\n```\n\n#### Assemble the Vectors\n\nAfter our pipeline has transformed data into numbers, we need to assemble those into vectors. Spark includes a `VectorAssembler` object that does just that, transforming a set of input columns into a vector that is stored in the `features` column in the DataFrame:\n\n```\n//assemble raw features\nval assembler = new VectorAssembler()\n                .setInputCols(Array(\n                    \"shipmodeIndex\",\n                    \"consigneeIndex\",\n                    \"shipperIndex\",\n                    \"gross_weight_lbIndex\",\n                    \"foreign_portIndex\",\n                    \"us_portIndex\",\n                    \"vessel_nameIndex\",\n                    \"country_of_originIndex\",\n                    \"container_numberIndex\",\n                    \"container_typeIndex\",\n                    \"quantity_bin\",\n                    \"ship_registered_inIndex\",\n                    \"carrier_codeIndex\",\n                    \"carrier_cityIndex\",\n                    \"notify_partyIndex\",\n                    \"place_of_receiptIndex\",\n                    \"zipcodeIndex\",\n                    \"quantity_bin\"\n                    ))\n                .setOutputCol(\"features\")\n```\n\n#### Create the Estimator\n\nCreating the estimator is a simple matter of specifying a few parameters, including which column in the DataFrame is the label, and which column contains the feature set:\n\n```\n//Create ML analytic\nval lr = new LogisticRegression()\n    .setMaxIter(30)\n    .setLabelCol(\"label\")\n    .setFeaturesCol(\"features\")\n    .set\n    RegParam(0.3)\n```\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:35:32.800",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>3. Create Pipeline Stages</h3>\n<p>Our pipeline stages are fairly simple:</p>\n<ul>\n  <li>Transform each row of data in the input dataset into an integer vector.</li>\n  <li>Assemble the vectors into a DataFrame</li>\n  <li>Use a Logistic Regression Estimator to create our model</li>\n</ul>\n<h4>Transform each row of data into an integer vector</h4>\n<p>The Logistic Regression estimator operates on integer vectors, so we need to convert each row in our input dataframe into an integer vector. Remember that each row contains only the fields from our database that are of interest to our model: the fields that previously included in our sequence and concatenated onto our DataFrame.</p>\n<p>Spark includes a <code>StringIndexer</code> function that does exactly that, so we create a <code>StringIndexer</code> for each field, and we&rsquo;ll later use each of these as a stage in our learning pipeline. The <code>StringIndexer</code> transforms the data from a specified input column in our DataFrame and stores the output in a specified and new output column. By convention, we name each string indexer with the name of the field+<code>Indexer,</code> and name the output column the name of the field+<code>Index,</code> e.g. we create a transformer named <code>consigneeIndexer</code> to transform the input column <code>consignee</code> into the new output column <code>consigneeIndex.</code></p>\n<pre><code>// Transform strings into numbers\nval consigneeIndexer = new StringIndexer().setInputCol(&quot;consignee&quot;).setOutputCol(&quot;consigneeIndex&quot;).setHandleInvalid(&quot;skip&quot;) \nval shipperIndexer = new StringIndexer().setInputCol(&quot;shipper&quot;).setOutputCol(&quot;shipperIndex&quot;).setHandleInvalid(&quot;skip&quot;)\nval shipmodeIndexer = new StringIndexer().setInputCol(&quot;shipmode&quot;).setOutputCol(&quot;shipmodeIndex&quot;).setHandleInvalid(&quot;skip&quot;) \nval gross_weight_lbIndexer = new StringIndexer().setInputCol(&quot;gross_weight_lb&quot;).setOutputCol(&quot;gross_weight_lbIndex&quot;).setHandleInvalid(&quot;skip&quot;) \nval foreign_portIndexer = new StringIndexer().setInputCol(&quot;foreign_port&quot;).setOutputCol(&quot;foreign_portIndex&quot;).setHandleInvalid(&quot;skip&quot;) \nval us_portIndexer = new StringIndexer().setInputCol(&quot;us_port&quot;).setOutputCol(&quot;us_portIndex&quot;).setHandleInvalid(&quot;skip&quot;) \nval vessel_nameIndexer = new StringIndexer().setInputCol(&quot;vessel_name&quot;).setOutputCol(&quot;vessel_nameIndex&quot;).setHandleInvalid(&quot;skip&quot;) \nval country_of_originIndexer = new StringIndexer().setInputCol(&quot;country_of_origin&quot;).setOutputCol(&quot;country_of_originIndex&quot;).setHandleInvalid(&quot;skip&quot;) \nval container_numberIndexer = new StringIndexer().setInputCol(&quot;container_number&quot;).setOutputCol(&quot;container_numberIndex&quot;).setHandleInvalid(&quot;skip&quot;)\nval container_typeIndexer = new StringIndexer().setInputCol(&quot;container_type&quot;).setOutputCol(&quot;container_typeIndex&quot;).setHandleInvalid(&quot;skip&quot;) \nval ship_registered_inIndexer = new StringIndexer().setInputCol(&quot;ship_registered_in&quot;).setOutputCol(&quot;ship_registered_inIndex&quot;).setHandleInvalid(&quot;skip&quot;) \nval carrier_codeIndexer = new StringIndexer().setInputCol(&quot;carrier_code&quot;).setOutputCol(&quot;carrier_codeIndex&quot;).setHandleInvalid(&quot;skip&quot;) \nval carrier_cityIndexer = new StringIndexer().setInputCol(&quot;carrier_city&quot;).setOutputCol(&quot;carrier_cityIndex&quot;).setHandleInvalid(&quot;skip&quot;) \nval notify_partyIndexer = new StringIndexer().setInputCol(&quot;notify_party&quot;).setOutputCol(&quot;notify_partyIndex&quot;).setHandleInvalid(&quot;skip&quot;) \nval place_of_receiptIndexer = new StringIndexer().setInputCol(&quot;place_of_receipt&quot;).setOutputCol(&quot;place_of_receiptIndex&quot;).setHandleInvalid(&quot;skip&quot;)\nval zipcodeIndexer = new StringIndexer().setInputCol(&quot;zipcode&quot;).setOutputCol(&quot;zipcodeIndex&quot;).setHandleInvalid(&quot;skip&quot;)\n</code></pre>\n<h4>Assemble the Vectors</h4>\n<p>After our pipeline has transformed data into numbers, we need to assemble those into vectors. Spark includes a <code>VectorAssembler</code> object that does just that, transforming a set of input columns into a vector that is stored in the <code>features</code> column in the DataFrame:</p>\n<pre><code>//assemble raw features\nval assembler = new VectorAssembler()\n                .setInputCols(Array(\n                    &quot;shipmodeIndex&quot;,\n                    &quot;consigneeIndex&quot;,\n                    &quot;shipperIndex&quot;,\n                    &quot;gross_weight_lbIndex&quot;,\n                    &quot;foreign_portIndex&quot;,\n                    &quot;us_portIndex&quot;,\n                    &quot;vessel_nameIndex&quot;,\n                    &quot;country_of_originIndex&quot;,\n                    &quot;container_numberIndex&quot;,\n                    &quot;container_typeIndex&quot;,\n                    &quot;quantity_bin&quot;,\n                    &quot;ship_registered_inIndex&quot;,\n                    &quot;carrier_codeIndex&quot;,\n                    &quot;carrier_cityIndex&quot;,\n                    &quot;notify_partyIndex&quot;,\n                    &quot;place_of_receiptIndex&quot;,\n                    &quot;zipcodeIndex&quot;,\n                    &quot;quantity_bin&quot;\n                    ))\n                .setOutputCol(&quot;features&quot;)\n</code></pre>\n<h4>Create the Estimator</h4>\n<p>Creating the estimator is a simple matter of specifying a few parameters, including which column in the DataFrame is the label, and which column contains the feature set:</p>\n<pre><code>//Create ML analytic\nval lr = new LogisticRegression()\n    .setMaxIter(30)\n    .setLabelCol(&quot;label&quot;)\n    .setFeaturesCol(&quot;features&quot;)\n    .set\n    RegParam(0.3)\n</code></pre>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900756_1858794936",
      "id": "20180130-190241_1891417263",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-20 17:13:14.301",
      "dateFinished": "2018-11-20 17:13:14.319",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 4. Assemble our Pipeline\n\nNow that we've got our stages set up, we're ready to assemble our Machine Learning pipeline, which chains those stages together in sequence:\n\n```\n// Chain indexers and tree in a Pipeline\nval lrPipeline = new Pipeline().setStages(\n        Array(consigneeIndexer,\n                shipperIndexer,\n                shipmodeIndexer,\n                gross_weight_lbIndexer,\n                foreign_portIndexer,\n                us_portIndexer,\n                vessel_nameIndexer,\n                country_of_originIndexer,\n                container_numberIndexer,\n                container_typeIndexer,\n                ship_registered_inIndexer,\n                carrier_codeIndexer,\n                carrier_cityIndexer,\n                notify_partyIndexer,\n                place_of_receiptIndexer,\n                zipcodeIndexer,\n                assembler,\n                lr\n                ))\n```",
      "user": "anonymous",
      "dateUpdated": "2018-11-30 05:25:31.886",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>4. Assemble our Pipeline</h3>\n<p>Now that we&rsquo;ve got our stages set up, we&rsquo;re ready to assemble our Machine Learning pipeline, which chains those stages together in sequence:</p>\n<pre><code>// Chain indexers and tree in a Pipeline\nval lrPipeline = new Pipeline().setStages(\n        Array(consigneeIndexer,\n                shipperIndexer,\n                shipmodeIndexer,\n                gross_weight_lbIndexer,\n                foreign_portIndexer,\n                us_portIndexer,\n                vessel_nameIndexer,\n                country_of_originIndexer,\n                container_numberIndexer,\n                container_typeIndexer,\n                ship_registered_inIndexer,\n                carrier_codeIndexer,\n                carrier_cityIndexer,\n                notify_partyIndexer,\n                place_of_receiptIndexer,\n                zipcodeIndexer,\n                assembler,\n                lr\n                ))\n</code></pre>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900757_32313036",
      "id": "20180130-201715_1588482510",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-30 05:25:31.886",
      "dateFinished": "2018-11-30 05:25:31.893",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 5. Train our Model\n\nNow that our pipeline is set up, all we need to do to train our model is feed our dataframe into the pipeline's `fit` method, which learns from the data. \n```\n// Train model. \nval lrModel = lrPipeline.fit(df)\n```\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:35:38.349",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>5. Train our Model</h3>\n<p>Now that our pipeline is set up, all we need to do to train our model is feed our dataframe into the pipeline&rsquo;s <code>fit</code> method, which learns from the data. </p>\n<pre><code>// Train model. \nval lrModel = lrPipeline.fit(df)\n</code></pre>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900758_-510863185",
      "id": "20180130-201949_1288501052",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-20 17:13:44.618",
      "dateFinished": "2018-11-20 17:13:44.623",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Using Spark MLlib",
      "text": "%md\n### 6. Materialize the Model\n\nNow that we've trained our model, we can apply it to real data and display the results. For simplicity sake, we'll simply apply the model to our feature table itself.\n\n```\nlrModel.transform(df).select(\"prediction\", \"probability\", \"features\").show(100)\n```",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:35:43.123",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "title": false,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>6. Materialize the Model</h3>\n<p>Now that we&rsquo;ve trained our model, we can apply it to real data and display the results. For simplicity sake, we&rsquo;ll simply apply the model to our feature table itself.</p>\n<pre><code>lrModel.transform(df).select(&quot;prediction&quot;, &quot;probability&quot;, &quot;features&quot;).show(100)\n</code></pre>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900759_897973427",
      "id": "20180118-020316_1913850778",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-20 17:14:01.628",
      "dateFinished": "2018-11-20 17:14:01.632",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\nselect *  from DEV2_ASN.features { limit 100 }\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:35:52.870",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "SHIPMENTID": "string",
                      "STATUS": "string",
                      "SHIPMODE": "string",
                      "PRODUCT_DESCRIPTION": "string",
                      "CONSIGNEE": "string",
                      "SHIPPER": "string",
                      "ARRIVAL_DATE": "string",
                      "GROSS_WEIGHT_LB": "string",
                      "GROSS_WEIGHT_KG": "string",
                      "FOREIGN_PORT": "string",
                      "US_PORT": "string",
                      "VESSEL_NAME": "string",
                      "COUNTRY_OF_ORIGIN": "string",
                      "CONSIGNEE_ADDRESS": "string",
                      "SHIPPER_ADDRESS": "string",
                      "ZIPCODE": "string",
                      "NO_OF_CONTAINERS": "string",
                      "CONTAINER_NUMBER": "string",
                      "CONTAINER_TYPE": "string",
                      "QUANTITY": "string",
                      "QUANTITY_UNIT": "string",
                      "MEASUREMENT": "string",
                      "MEASUREMENT_UNIT": "string",
                      "BILL_OF_LADING": "string",
                      "HOUSE_VS_MASTER": "string",
                      "DISTRIBUTION_PORT": "string",
                      "MASTER_BL": "string",
                      "VOYAGE_NUMBER": "string",
                      "SEAL": "string",
                      "SHIP_REGISTERED_IN": "string",
                      "INBOND_ENTRY_TYPE": "string",
                      "CARRIER_CODE": "string",
                      "CARRIER_NAME": "string",
                      "CARRIER_CITY": "string",
                      "CARRIER_STATE": "string",
                      "CARRIER_ZIP": "string",
                      "CARRIER_ADDRESS": "string",
                      "NOTIFY_PARTY": "string",
                      "NOTIFY_ADDRESS": "string",
                      "PLACE_OF_RECEIPT": "string",
                      "DATE_OF_RECEIPT": "string",
                      "QUANTITY_BIN": "string",
                      "LATENESS": "string",
                      "LABEL": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395900759_1824152226",
      "id": "20180205-024307_1130627818",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\nDROP TABLE IF EXISTS DEV2_ASN.PREDICTIONS;\nCREATE TABLE DEV2_ASN.PREDICTIONS (\n    SHIPMENTID VARCHAR(11) NOT NULL PRIMARY KEY,\n    PREDICTION DOUBLE\n    );",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:36:05.083",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395900759_-2137361316",
      "id": "20180205-024429_478433529",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## The Scala Code \n\nThe Scala code is listed here:",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:36:06.444",
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>The Scala Code</h2>\n<p>The Scala code is listed here:</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900765_312337259",
      "id": "20180611-220634_1810473505",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-20 17:14:29.613",
      "dateFinished": "2018-11-20 17:14:29.616",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nimport org.apache.spark.ml.feature.VectorAssembler\nimport java.sql.{Connection}\nimport com.splicemachine.spark.splicemachine._\nimport org.apache.spark.sql.execution.datasources.jdbc.{JDBCOptions, JdbcUtils}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature.StringIndexer\nimport spark.implicits._\n\n\nval optionMap = Map(\n  JDBCOptions.JDBC_TABLE_NAME -> \"DEV2_ASN.Features\",\n  JDBCOptions.JDBC_URL -> defaultJDBCURL\n)\nval splicemachineContext = new SplicemachineContext(defaultJDBCURL)\nval df_with_uppercase_schema = splicemachineContext.df(\"select * from DEV2_ASN.Features\")\nval newNames = Seq(\n    \"shipmentid\",\n    \"status\",\n    \"shipmode\",\n    \"product_description\",\n    \"consignee\",\n    \"shipper\",\n    \"arrival_date\",\n    \"gross_weight_lb\",\n    \"gross_weight_kg\",\n    \"foreign_port\",\n    \"us_port\",\n    \"vessel_name\",\n    \"country_of_origin\",\n    \"consignee_address\",\n    \"shipper_address\",\n    \"zipcode\",\n    \"no_of_containers\",\n    \"container_number\",\n    \"container_type\",\n    \"quantity\",\n    \"quantity_unit\",\n    \"measurement\",\n    \"measurement_unit\",\n    \"bill_of_lading\",\n    \"house_vs_master\",\n    \"distribution_port\",\n    \"master_bl\",\n    \"voyage_number\",\n    \"seal\",\n    \"ship_registered_in\",\n    \"inbond_entry_type\",\n    \"carrier_code\",\n    \"carrier_name\",\n    \"carrier_city\",\n    \"carrier_state\",\n    \"carrier_zip\",\n    \"carrier_address\",\n    \"notify_party\",\n    \"notify_address\",\n    \"place_of_receipt\",\n    \"date_of_receipt\",\n    \"quantity_bin\",\n    \"lateness\",\n    \"label\"\n)\nval df = df_with_uppercase_schema.toDF(newNames: _*)\n\n//assemble raw features\nval assembler = new VectorAssembler().\n                setInputCols(Array(\n                    \"consigneeIndex\",\n                    \"shipperIndex\",\n                    \"gross_weight_lbIndex\",\n                    \"foreign_portIndex\",\n                    \"us_portIndex\",\n                    \"vessel_nameIndex\",\n                    \"country_of_originIndex\",\n                    \"container_numberIndex\",\n                    \"container_typeIndex\",\n                    \"quantity_bin\",\n                    \"ship_registered_inIndex\",\n                    \"carrier_codeIndex\",\n                    \"carrier_cityIndex\",\n                    \"notify_partyIndex\",\n                    \"place_of_receiptIndex\",\n                    \"zipcodeIndex\"\n                    )).\n                setOutputCol(\"features\")\n\n// Transform strings into numbers\nval zipcodeIndexer = new StringIndexer().setInputCol(\"zipcode\").setOutputCol(\"zipcodeIndex\").setHandleInvalid(\"skip\")\nval consigneeIndexer = new StringIndexer().setInputCol(\"consignee\").setOutputCol(\"consigneeIndex\").setHandleInvalid(\"skip\") \nval shipperIndexer = new StringIndexer().setInputCol(\"shipper\").setOutputCol(\"shipperIndex\").setHandleInvalid(\"skip\")\nval statusIndexer = new StringIndexer().setInputCol(\"status\").setOutputCol(\"statusIndex\").setHandleInvalid(\"skip\") \nval shipmodeIndexer = new StringIndexer().setInputCol(\"shipmode\").setOutputCol(\"shipmodeIndex\").setHandleInvalid(\"skip\") \nval gross_weight_lbIndexer = new StringIndexer().setInputCol(\"gross_weight_lb\").setOutputCol(\"gross_weight_lbIndex\").setHandleInvalid(\"skip\") \nval foreign_portIndexer = new StringIndexer().setInputCol(\"foreign_port\").setOutputCol(\"foreign_portIndex\").setHandleInvalid(\"skip\") \nval us_portIndexer = new StringIndexer().setInputCol(\"us_port\").setOutputCol(\"us_portIndex\").setHandleInvalid(\"skip\") \nval vessel_nameIndexer = new StringIndexer().setInputCol(\"vessel_name\").setOutputCol(\"vessel_nameIndex\").setHandleInvalid(\"skip\") \nval country_of_originIndexer = new StringIndexer().setInputCol(\"country_of_origin\").setOutputCol(\"country_of_originIndex\").setHandleInvalid(\"skip\") \nval container_numberIndexer = new StringIndexer().setInputCol(\"container_number\").setOutputCol(\"container_numberIndex\").setHandleInvalid(\"skip\")\nval container_typeIndexer = new StringIndexer().setInputCol(\"container_type\").setOutputCol(\"container_typeIndex\").setHandleInvalid(\"skip\") \nval distribution_portIndexer = new StringIndexer().setInputCol(\"distribution_port\").setOutputCol(\"distribution_portIndex\").setHandleInvalid(\"skip\") \nval ship_registered_inIndexer = new StringIndexer().setInputCol(\"ship_registered_in\").setOutputCol(\"ship_registered_inIndex\").setHandleInvalid(\"skip\") \nval inbond_entry_typeIndexer = new StringIndexer().setInputCol(\"inbond_entry_type\").setOutputCol(\"inbond_entry_typeIndex\").setHandleInvalid(\"skip\") \nval carrier_codeIndexer = new StringIndexer().setInputCol(\"carrier_code\").setOutputCol(\"carrier_codeIndex\").setHandleInvalid(\"skip\") \nval carrier_cityIndexer = new StringIndexer().setInputCol(\"carrier_city\").setOutputCol(\"carrier_cityIndex\").setHandleInvalid(\"skip\") \nval carrier_stateIndexer = new StringIndexer().setInputCol(\"carrier_state\").setOutputCol(\"carrier_stateIndex\").setHandleInvalid(\"skip\") \nval carrier_zipIndexer = new StringIndexer().setInputCol(\"carrier_zip\").setOutputCol(\"carrier_zipIndex\").setHandleInvalid(\"skip\") \nval notify_partyIndexer = new StringIndexer().setInputCol(\"notify_party\").setOutputCol(\"notify_partyIndex\").setHandleInvalid(\"skip\") \nval place_of_receiptIndexer = new StringIndexer().setInputCol(\"place_of_receipt\").setOutputCol(\"place_of_receiptIndex\").setHandleInvalid(\"skip\") \n\n//Create ML analytic\nval lr = new LogisticRegression().\n    setMaxIter(30).\n    setLabelCol(\"label\").\n    setFeaturesCol(\"features\").\n    setRegParam(0.3)\n\n\n// Chain indexers and tree in a Pipeline\nval lrPipeline = new Pipeline().setStages(\n        Array(consigneeIndexer,\n                shipperIndexer,\n                shipmodeIndexer,\n                gross_weight_lbIndexer,\n                foreign_portIndexer,\n                us_portIndexer,\n                vessel_nameIndexer,\n                country_of_originIndexer,\n                container_numberIndexer,\n                container_typeIndexer,\n                ship_registered_inIndexer,\n                carrier_codeIndexer,\n                carrier_cityIndexer,\n                notify_partyIndexer,\n                place_of_receiptIndexer,\n                zipcodeIndexer,\n                assembler,\n                lr\n                ))\n\n// Train model. \nval lrModel = lrPipeline.fit(df)\n\nlrModel.transform(df).select(\"prediction\", \"probability\", \"features\").show(100)\n\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:37:10.823",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395900770_835891442",
      "id": "20180130-201612_556450692",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\ndrop table IF EXISTS DEV2_ASN.TEST_FEATURES;\nCREATE table DEV2_ASN.TEST_FEATURES AS\n    SELECT \n    SHIPMENTID,\n    STATUS,\n    SHIPMODE,\n    PRODUCT_DESCRIPTION,\n    CONSIGNEE,\n    SHIPPER,\n    ARRIVAL_DATE,\n    GROSS_WEIGHT_LB,\n    GROSS_WEIGHT_KG,\n    FOREIGN_PORT,\n    US_PORT,\n    VESSEL_NAME,\n    COUNTRY_OF_ORIGIN,\n    CONSIGNEE_ADDRESS,\n    SHIPPER_ADDRESS,\n    ZIPCODE,\n    NO_OF_CONTAINERS,\n    CONTAINER_NUMBER,\n    CONTAINER_TYPE,\n    QUANTITY,\n    QUANTITY_UNIT,\n    MEASUREMENT,\n    MEASUREMENT_UNIT,\n    BILL_OF_LADING,\n    HOUSE_VS_MASTER,\n    DISTRIBUTION_PORT,\n    MASTER_BL,\n    VOYAGE_NUMBER,\n    SEAL,\n    SHIP_REGISTERED_IN,\n    INBOND_ENTRY_TYPE,\n    CARRIER_CODE,\n    CARRIER_NAME,\n    CARRIER_CITY,\n    CARRIER_STATE,\n    CARRIER_ZIP,\n    CARRIER_ADDRESS,\n    NOTIFY_PARTY,\n    NOTIFY_ADDRESS,\n    PLACE_OF_RECEIPT,\n    DATE_OF_RECEIPT,\n    CASE\n    WHEN DEV2_ASN.SHIPMENT_IN_TRANSIT.QUANTITY > 10\n    THEN\n        CASE\n            WHEN DEV2_ASN.SHIPMENT_IN_TRANSIT.QUANTITY > 100\n            THEN\n                CASE\n                    WHEN DEV2_ASN.SHIPMENT_IN_TRANSIT.QUANTITY > 1000\n                    THEN 3\n                    ELSE 2\n                END\n            ELSE 1\n\tEND\n    ELSE 0\n    END AS QUANTITY_BIN\n    FROM DEV2_ASN.SHIPMENT_IN_TRANSIT;\n    \n",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:37:39.040",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395900771_1776441852",
      "id": "20180304-185514_1167859763",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n## Testing the Code\n\nNow we'll test our code on the `testing` features table:",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:37:51.744",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Testing the Code</h2>\n<p>Now we&rsquo;ll test our code on the <code>testing</code> features table:</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900771_659397738",
      "id": "20180612-031525_84277416",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-11-20 17:14:58.670",
      "dateFinished": "2018-11-20 17:14:58.673",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, LongType, BooleanType };\n\n    val test_data_with_uppercase_schema = splicemachineContext.df(\"select * from DEV2_ASN.TEST_FEATURES\")\n    val newNames = Seq(\n        \"shipmentid\",\n        \"status\",\n        \"shipmode\",\n        \"product_description\",\n        \"consignee\",\n        \"shipper\",\n        \"arrival_date\",\n        \"gross_weight_lb\",\n        \"gross_weight_kg\",\n        \"foreign_port\",\n        \"us_port\",\n        \"vessel_name\",\n        \"country_of_origin\",\n        \"consignee_address\",\n        \"shipper_address\",\n        \"zipcode\",\n        \"no_of_containers\",\n        \"container_number\",\n        \"container_type\",\n        \"quantity\",\n        \"quantity_unit\",\n        \"measurement\",\n        \"measurement_unit\",\n        \"bill_of_lading\",\n        \"house_vs_master\",\n        \"distribution_port\",\n        \"master_bl\",\n        \"voyage_number\",\n        \"seal\",\n        \"ship_registered_in\",\n        \"inbond_entry_type\",\n        \"carrier_code\",\n        \"carrier_name\",\n        \"carrier_city\",\n        \"carrier_state\",\n        \"carrier_zip\",\n        \"carrier_address\",\n        \"notify_party\",\n        \"notify_address\",\n        \"place_of_receipt\",\n        \"date_of_receipt\",\n        \"quantity_bin\"\n    )\n    val test_data = test_data_with_uppercase_schema.toDF(newNames: _*)\n\n    // Make predictions.\n    val lrPredictions = lrModel.transform(test_data)\n\n    var z = Array(\"Zara\", \"Nuha\", \"Ayan\")\n\n    // Select example rows to display.\n     val innerStruct =\n   StructType(\n     StructField(\"f1\", IntegerType, true) ::\n     StructField(\"f2\", LongType, false) ::\n     StructField(\"f3\", BooleanType, false) :: Nil)\n    lrPredictions.select(\"prediction\", \"probability\",\"features\").show(10)\n    splicemachineContext.createTable(\"tabletest\", innerStruct, z, null)",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:38:12.550",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395900772_-909627617",
      "id": "20180125-142959_1101825868",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nimport org.apache.spark.sql.types.{StructType, StructField, StringType};\n\nval predictions = lrPredictions.select(\"SHIPMENTID\", \"PREDICTION\")\n\npredictions.printSchema()\n \nsplicemachineContext.insert(predictions,\"DEV2_ASN.predictions\") \n\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:38:23.767",
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.types.{StructType, StructField, StringType}\npredictions: org.apache.spark.sql.DataFrame = [SHIPMENTID: string, PREDICTION: double]\nroot\n |-- SHIPMENTID: string (nullable = true)\n |-- PREDICTION: double (nullable = true)\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900772_1514447564",
      "id": "20180304-185743_1150073560",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%splicemachine\nselect * from DEV2_ASN.predictions;",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:38:34.148",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 318.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "SHIPMENTID": "string",
                      "PREDICTION": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1542395900773_1334841789",
      "id": "20180304-190035_379792791",
      "dateCreated": "2018-11-16 11:18:20.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Where to Go Next\n\nThe next notebook in this class, [*Creating Custom Stored Procedures*](/#/notebook/2DWAGKSPM), shows you how to create and use custom stored procedures with Splice Machine.\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-01 05:38:52.552",
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "results": {},
        "enabled": false,
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Where to Go Next</h2>\n<p>The next notebook in this class, <a href=\"/#/notebook/2DWAGKSPM\"><em>Creating Custom Stored Procedures</em></a>, shows you how to create and use custom stored procedures with Splice Machine.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1542395900773_911637495",
      "id": "20180304-190115_1109571681",
      "dateCreated": "2018-11-16 11:18:20.000",
      "dateStarted": "2018-12-01 05:38:44.896",
      "dateFinished": "2018-12-01 05:38:44.907",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Splice Machine Training/For Developers, Part II - Intermediate/j. ML with Spark MLlib Using Scala",
  "id": "2DX1UV9MX",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "splicemachine:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}