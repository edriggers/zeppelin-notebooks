{
  "paragraphs": [{
    "text": "%md\n<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/zeppelin/css/zepstyles.css\" />\n\n# Splice Machine Introduction\n\nSplice Machine is a scale-out SQL RDBMS and Data Warehouse combined. It supports both OLTP and OLAP workloads. This presentation will introduce you to what Splice Machine does and how it works in these notebooks:\n\n<table class=\"splicezep\">\n   <col />\n   <col />\n   <thead>\n        <tr>\n            <th>Notebook</th>\n            <th>Description</th>\n        </tr>\n   </thead>\n   <tbody>\n        <tr>\n            <td class=\"ItalicFont\">1. Introduction</a></td>\n            <td>This notebook introduces Splice Machine, with a brief overview of its architecture, technology stack, and SQL coverage.</td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\"><a href=\"/#/notebook/2D6ZFHE9U\">2. The Life of a Query</a></td>\n            <td>Walks you through importing and running the TPC-H benchmark data by creating a database in Splice Machine, importing the TPC-H dataset, running queries, and improving performance by indexing the data.</td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\"><a href=\"/#/notebook/2D93Y56QJ\">3. Monitoring Queries with the Database Console</a></td>\n            <td>Introduces you to the Splice Machine Database Console, which you can use to monitor and control your currently running queries.</td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\"><a href=\"/#/notebook/2D9M6ZUAW\">4. Using Zeppelin for Graphing and Filtering</a></td>\n            <td>Shows you how to use Apache Zeppelin notebooks for graphing and filtering the data in your Splice Machine databases.</td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\"><a href=\"/#/notebook/2D89MQA89\">5. Splice Transactions with Spark and JDBC</a></td>\n            <td>Introduces you to the transactional nature of Splice Machine and using JDBC to program transactions.</td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\"><a href=\"/#/notebook/2D8PMSAPJ\">6. Creating Applications with Splice Machine</a></td>\n            <td>Show you how you can easily create applications with Splice Machine.</td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\"><a href=\"/#/notebook/2D96GBY2P\">7. Using the Splice Machine Spark Adapter</a></td>\n            <td>Introduces you to Using the Splice Machine Spark Adapter for working with your database.</td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\"><a href=\"/#/notebook/2E3671BBN\">8. Machine Learning with Spark MLlib using Python</a></td>\n            <td>Presents an example of Using the Spark Machine Learning Library (MLlib) with Splice Machine and Python.</td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\"><a href=\"/#/notebook/\">9. Machine Learning with Spark MLlib using Scala</a></td>\n            <td>Presents an example of Using the Spark Machine Learning Library (MLlib) with Splice Machine and Scala.</td>\n        </tr>\n   </tbody>\n</table>\n<br />\nThis notebook introduces Splice Machine in the following sections:\n\n<ul class=\"italic\">\n    <li>Hybrid Transactional and Analytical Processing</li>\n    <li>ANSI SQL Coverage</li>\n    <li>Architecture Overview</li>\n    <li>Technology Stack Overview</li>\n    <li>Internal Storage Using HBase</li>\n</ul>\n",
    "user": "splice",
    "dateUpdated": "2019-02-08T19:47:51+0000",
    "config": {
      "tableHide": false,
      "editorSetting": {
        "language": "markdown",
        "editOnDblClick": true
      },
      "colWidth": 12,
      "editorMode": "ace/mode/markdown",
      "editorHide": true,
      "results": {},
      "enabled": true,
      "fontSize": 9
    },
    "settings": {
      "params": {},
      "forms": {}
    },
    "results": {
      "code": "SUCCESS",
      "msg": [{
        "type": "HTML",
        "data": "<div class=\"markdown-body\">\n<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/zeppelin/css/zepstyles.css\" />\n<h1>Splice Machine Introduction</h1>\n<p>Splice Machine is a scale-out SQL RDBMS and Data Warehouse combined. It supports both OLTP and OLAP workloads. This presentation will introduce you to what Splice Machine does and how it works in these notebooks:</p>\n<table class=\"splicezep\">\n   <col />\n   <col />\n   <thead>\n        <tr>\n            <th>Notebook</th>\n            <th>Description</th>\n        </tr>\n   </thead>\n   <tbody>\n        <tr>\n            <td class=\"ItalicFont\">1. Introduction</a></td>\n            <td>This notebook introduces Splice Machine, with a brief overview of its architecture, technology stack, and SQL coverage.</td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\"><a href=\"/#/notebook/2D6ZFHE9U\">2. The Life of a Query</a></td>\n            <td>Walks you through importing and running the TPC-H benchmark data by creating a database in Splice Machine, importing the TPC-H dataset, running queries, and improving performance by indexing the data.</td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\"><a href=\"/#/notebook/2D93Y56QJ\">3. Monitoring Queries with the Database Console</a></td>\n            <td>Introduces you to the Splice Machine Database Console, which you can use to monitor and control your currently running queries.</td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\"><a href=\"/#/notebook/2D9M6ZUAW\">4. Using Zeppelin for Graphing and Filtering</a></td>\n            <td>Shows you how to use Apache Zeppelin notebooks for graphing and filtering the data in your Splice Machine databases.</td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\"><a href=\"/#/notebook/2D89MQA89\">5. Splice Transactions with Spark and JDBC</a></td>\n            <td>Introduces you to the transactional nature of Splice Machine and using JDBC to program transactions.</td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\"><a href=\"/#/notebook/2D8PMSAPJ\">6. Creating Applications with Splice Machine</a></td>\n            <td>Show you how you can easily create applications with Splice Machine.</td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\"><a href=\"/#/notebook/2D96GBY2P\">7. Using the Splice Machine Spark Adapter</a></td>\n            <td>Introduces you to Using the Splice Machine Spark Adapter for working with your database.</td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\"><a href=\"/#/notebook/2E3671BBN\">8. Machine Learning with Spark MLlib using Python</a></td>\n            <td>Presents an example of Using the Spark Machine Learning Library (MLlib) with Splice Machine and Python.</td>\n        </tr>\n        <tr>\n            <td class=\"ItalicFont\"><a href=\"/#/notebook/\">9. Machine Learning with Spark MLlib using Scala</a></td>\n            <td>Presents an example of Using the Spark Machine Learning Library (MLlib) with Splice Machine and Scala.</td>\n        </tr>\n   </tbody>\n</table>\n<br />\n<p>This notebook introduces Splice Machine in the following sections:</p>\n<ul class=\"italic\">\n    <li>Hybrid Transactional and Analytical Processing</li>\n    <li>ANSI SQL Coverage</li>\n    <li>Architecture Overview</li>\n    <li>Technology Stack Overview</li>\n    <li>Internal Storage Using HBase</li>\n</ul>\n</div>"
      }]
    },
    "apps": [],
    "jobName": "paragraph_1520184188048_-345516075",
    "id": "20180125-141757_1025631852",
    "dateCreated": "2018-03-04T17:23:08+0000",
    "dateStarted": "2019-02-08T19:47:51+0000",
    "dateFinished": "2019-02-08T19:47:51+0000",
    "status": "FINISHED",
    "progressUpdateIntervalMs": 500,
    "focus": true,
    "$$hashKey": "object:119273"
  }, {
    "text": "%md\n## Hybrid Transactional and Analytical Processing\n\nSplice Machine has a unique “Dual Engine” architecture that it uses to provide outstanding performance for concurrent transactional (OLTP) and analytical (OLAP) workloads. The SQL parser and cost-based optimizer analyze an incoming query and then determine the best execution plan based on query type, data sizes, available indexes and more. Based on that analysis, Splice Machine:\n\n* Deploys HBase for OLTP-type lookups, inserts and short range scans\n* Uses Spark for lightning-fast in-memory processing of analytical workloads.\n\nThe Dual Engine architecture gives you the best of multiple worlds in a hybrid database: the performance, scale-out, and resilience of HBase, the in-memory analytics performance of Spark, and the performance of a cost-based optimizer.\n",
    "dateUpdated": "2018-03-04T17:23:08+0000",
    "config": {
      "tableHide": false,
      "editorSetting": {
        "language": "markdown",
        "editOnDblClick": true
      },
      "colWidth": 12,
      "editorMode": "ace/mode/markdown",
      "editorHide": true,
      "results": {},
      "enabled": true,
      "fontSize": 9
    },
    "settings": {
      "params": {},
      "forms": {}
    },
    "results": {
      "code": "SUCCESS",
      "msg": [{
        "type": "HTML",
        "data": "<div class=\"markdown-body\">\n<h2>Hybrid Transactional and Analytical Processing</h2>\n<p>Splice Machine has a unique “Dual Engine” architecture that it uses to provide outstanding performance for concurrent transactional (OLTP) and analytical (OLAP) workloads. The SQL parser and cost-based optimizer analyze an incoming query and then determine the best execution plan based on query type, data sizes, available indexes and more. Based on that analysis, Splice Machine:</p>\n<ul>\n  <li>Deploys HBase for OLTP-type lookups, inserts and short range scans</li>\n  <li>Uses Spark for lightning-fast in-memory processing of analytical workloads.</li>\n</ul>\n<p>The Dual Engine architecture gives you the best of multiple worlds in a hybrid database: the performance, scale-out, and resilience of HBase, the in-memory analytics performance of Spark, and the performance of a cost-based optimizer.</p>\n</div>"
      }]
    },
    "apps": [],
    "jobName": "paragraph_1520184188048_-345516075",
    "id": "20180125-141830_1881954222",
    "dateCreated": "2018-03-04T17:23:08+0000",
    "status": "READY",
    "errorMessage": "",
    "progressUpdateIntervalMs": 500,
    "$$hashKey": "object:119274"
  }, {
    "text": "%md\n## ANSI SQL Coverage\n\nUnlike other Big Data systems, Splice Machine supports full [ANSI SQL-2003](https://doc.splicemachine.com/sqlref_sqlsummary.html); here's a quick summary of our coverage:\n\n<table class=\"splicezep\" summary=\"Summary of SQL features available in Splice Machine.\">\n    <colgroup>\n       <col>\n      <col>\n    </colgroup>\n    <thead>\n        <tr>\n            <th>Feature</th>\n            <th>Examples</th>\n        </tr>\n    </thead>\n   <tbody>\n        <tr>\n            <td><em>Aggregation functions</em></td>\n            <td><code>AVG, COUNT, MAX, MIN, STDDEV_POP, STDDEV_SAMP, SUM</code></td>\n        </tr>\n        <tr>\n            <td><em>Conditional functions</em></td>\n            <td><code>CASE, searched CASE</code></td>\n        </tr>\n        <tr>\n            <td><em>Data Types</em></td>\n            <td><code>INTEGER, REAL, CHARACTER, DATE, BOOLEAN, BIGINT</code></td>\n        </tr>\n        <tr>\n            <td><em>DDL</em></td>\n            <td><code>CREATE TABLE, CREATE&nbsp;SCHEMA, CREATE&nbsp;INDEX, ALTER&nbsp;TABLE, DELETE, UPDATE</code></td>\n        </tr>\n        <tr>\n            <td><em>DML</em></td>\n            <td><code>INSERT, DELETE, UPDATE, SELECT</code></td>\n        </tr>\n        <tr>\n            <td><em>Isolation Levels</em></td>\n            <td>Snapshot isolation</td>\n        </tr>\n        <tr>\n            <td><em>Joins</em></td>\n            <td><code>INNER&nbsp;JOIN, LEFT&nbsp;OUTER&nbsp;JOIN, RIGHT&nbsp;OUTER&nbsp;JOIN</code></td>\n        </tr>\n        <tr>\n            <td><em>Predicates</em></td>\n            <td><code>IN, BETWEEN, LIKE, EXISTS</code></td>\n        </tr>\n        <tr>\n            <td><em>Privileges</em></td>\n            <td>Privileges for <code>SELECT, DELETE, INSERT, EXECUTE</code></td>\n        </tr>\n        <tr>\n            <td><em>Query Specification</em></td>\n            <td><code>SELECT&nbsp;DISTINCT, GROUP&nbsp;BY, HAVING</code></td>\n        </tr>\n        <tr>\n            <td><em>SET&nbsp;functions</em></td>\n            <td><code>UNION, ABS, MOD, ALL, CHECK</code></td>\n        </tr>\n        <tr>\n            <td><em>String functions</em></td>\n            <td><code>CHAR, Concatenation (||), INSTR, LCASE&nbsp;(LOWER), LENGTH,<br>LTRIM, REGEXP_LIKE, REPLACE, RTRIM, SUBSTR, UCASE&nbsp;(UPPER), VARCHAR</code></td>\n        </tr>\n        <tr>\n            <td><em>Sub-queries</em></td>\n            <td>Yes</td>\n        </tr>\n        <tr>\n            <td><em>Transactions</em></td>\n            <td><code>COMMIT, ROLLBACK</code></td>\n        </tr>\n        <tr>\n            <td><em>Triggers</em></td>\n            <td>Yes</td>\n        </tr>\n        <tr>\n            <td><em>User-defined functions (UDFs)</em></td>\n            <td>Yes</td>\n        </tr>\n        <tr>\n            <td><em>Views</em></td>\n            <td>Including grouped views</td>\n        </tr>\n        <tr>\n            <td><em>Window functions</em></td>\n            <td><code>AVG, COUNT, DENSE_RANK, FIRST_VALUE, LAG, LAST_VALUE, LEAD, MAX, MIN, RANK, ROW_NUMBER, STDDEV_POP, STDDEV_SAMP, SUM</code></td>\n        </tr>\n    </tbody>\n</table>\n\n",
    "user": "splice",
    "dateUpdated": "2018-06-14T05:15:22+0000",
    "config": {
      "tableHide": false,
      "editorSetting": {
        "language": "markdown",
        "editOnDblClick": true
      },
      "colWidth": 12,
      "editorMode": "ace/mode/markdown",
      "editorHide": true,
      "results": {},
      "enabled": true,
      "fontSize": 9
    },
    "settings": {
      "params": {},
      "forms": {}
    },
    "results": {
      "code": "SUCCESS",
      "msg": [{
        "type": "HTML",
        "data": "<div class=\"markdown-body\">\n<h2>ANSI SQL Coverage</h2>\n<p>Unlike other Big Data systems, Splice Machine supports full <a href=\"https://doc.splicemachine.com/sqlref_sqlsummary.html\">ANSI SQL-2003</a>; here&rsquo;s a quick summary of our coverage:</p>\n<table class=\"splicezep\" summary=\"Summary of SQL features available in Splice Machine.\">\n    <colgroup>\n       <col>\n      <col>\n    </colgroup>\n    <thead>\n        <tr>\n            <th>Feature</th>\n            <th>Examples</th>\n        </tr>\n    </thead>\n   <tbody>\n        <tr>\n            <td><em>Aggregation functions</em></td>\n            <td><code>AVG, COUNT, MAX, MIN, STDDEV_POP, STDDEV_SAMP, SUM</code></td>\n        </tr>\n        <tr>\n            <td><em>Conditional functions</em></td>\n            <td><code>CASE, searched CASE</code></td>\n        </tr>\n        <tr>\n            <td><em>Data Types</em></td>\n            <td><code>INTEGER, REAL, CHARACTER, DATE, BOOLEAN, BIGINT</code></td>\n        </tr>\n        <tr>\n            <td><em>DDL</em></td>\n            <td><code>CREATE TABLE, CREATE&nbsp;SCHEMA, CREATE&nbsp;INDEX, ALTER&nbsp;TABLE, DELETE, UPDATE</code></td>\n        </tr>\n        <tr>\n            <td><em>DML</em></td>\n            <td><code>INSERT, DELETE, UPDATE, SELECT</code></td>\n        </tr>\n        <tr>\n            <td><em>Isolation Levels</em></td>\n            <td>Snapshot isolation</td>\n        </tr>\n        <tr>\n            <td><em>Joins</em></td>\n            <td><code>INNER&nbsp;JOIN, LEFT&nbsp;OUTER&nbsp;JOIN, RIGHT&nbsp;OUTER&nbsp;JOIN</code></td>\n        </tr>\n        <tr>\n            <td><em>Predicates</em></td>\n            <td><code>IN, BETWEEN, LIKE, EXISTS</code></td>\n        </tr>\n        <tr>\n            <td><em>Privileges</em></td>\n            <td>Privileges for <code>SELECT, DELETE, INSERT, EXECUTE</code></td>\n        </tr>\n        <tr>\n            <td><em>Query Specification</em></td>\n            <td><code>SELECT&nbsp;DISTINCT, GROUP&nbsp;BY, HAVING</code></td>\n        </tr>\n        <tr>\n            <td><em>SET&nbsp;functions</em></td>\n            <td><code>UNION, ABS, MOD, ALL, CHECK</code></td>\n        </tr>\n        <tr>\n            <td><em>String functions</em></td>\n            <td><code>CHAR, Concatenation (||), INSTR, LCASE&nbsp;(LOWER), LENGTH,<br>LTRIM, REGEXP_LIKE, REPLACE, RTRIM, SUBSTR, UCASE&nbsp;(UPPER), VARCHAR</code></td>\n        </tr>\n        <tr>\n            <td><em>Sub-queries</em></td>\n            <td>Yes</td>\n        </tr>\n        <tr>\n            <td><em>Transactions</em></td>\n            <td><code>COMMIT, ROLLBACK</code></td>\n        </tr>\n        <tr>\n            <td><em>Triggers</em></td>\n            <td>Yes</td>\n        </tr>\n        <tr>\n            <td><em>User-defined functions (UDFs)</em></td>\n            <td>Yes</td>\n        </tr>\n        <tr>\n            <td><em>Views</em></td>\n            <td>Including grouped views</td>\n        </tr>\n        <tr>\n            <td><em>Window functions</em></td>\n            <td><code>AVG, COUNT, DENSE_RANK, FIRST_VALUE, LAG, LAST_VALUE, LEAD, MAX, MIN, RANK, ROW_NUMBER, STDDEV_POP, STDDEV_SAMP, SUM</code></td>\n        </tr>\n    </tbody>\n</table>\n</div>"
      }]
    },
    "apps": [],
    "jobName": "paragraph_1520184188049_-345900824",
    "id": "20180113-194734_1961030416",
    "dateCreated": "2018-03-04T17:23:08+0000",
    "dateStarted": "2018-06-14T05:15:22+0000",
    "dateFinished": "2018-06-14T05:15:22+0000",
    "status": "FINISHED",
    "progressUpdateIntervalMs": 500,
    "$$hashKey": "object:119275"
  }, {
    "text": "%md\n## Architecture Overview\n\nThe following diagram is a high-level representation of the architecture of Splice Machine:\n\n<img class=\"fitwidth\" src=\"https://doc.splicemachine.com/zeppelin/images/spliceArch1.png\">\n",
    "dateUpdated": "2018-03-04T17:23:08+0000",
    "config": {
      "tableHide": false,
      "editorSetting": {
        "language": "markdown",
        "editOnDblClick": true
      },
      "colWidth": 12,
      "editorMode": "ace/mode/markdown",
      "editorHide": true,
      "results": {},
      "enabled": true,
      "fontSize": 9
    },
    "settings": {
      "params": {},
      "forms": {}
    },
    "results": {
      "code": "SUCCESS",
      "msg": [{
        "type": "HTML",
        "data": "<div class=\"markdown-body\">\n<h2>Architecture Overview</h2>\n<p>The following diagram is a high-level representation of the architecture of Splice Machine:</p>\n<img class=\"fitwidth\" src=\"https://doc.splicemachine.com/zeppelin/images/spliceArch1.png\">\n</div>"
      }]
    },
    "apps": [],
    "jobName": "paragraph_1520184188049_-345900824",
    "id": "20180125-150608_1902680448",
    "dateCreated": "2018-03-04T17:23:08+0000",
    "status": "READY",
    "errorMessage": "",
    "progressUpdateIntervalMs": 500,
    "$$hashKey": "object:119276"
  }, {
    "text": "%md\n## Technology Stack Overview\n\nSplice Machine is built on open-sourced, proven, distributed database technology, including HBase/Hadoop and Spark.\n\n### HBase/Hadoop\n\nThe persistent, durable storage of operational data in Splice Machine resides in the Apache HBase key-value store. HBase:\n\n* is a non-relational (NoSQL) database that runs on top of HDFS\n* provides real-time read/write access to large datasets\n* scales linearly to handle huge data sets with billions of rows and millions of columns\n* is stored row-based and sorted by a primary key to deliver 1ms-10ms lookup speeds and short-range scans\n\n<img class=\"tiny\" src=\"https://hbase.apache.org/images/hbase_logo_with_orca.png\">\n\nHBase uses the Hadoop Distributed File System (HDFS) for reliable and replicated storage. HBase/HDFS provides auto-sharding and failover technology for scaling database tables across multiple servers. It is the only technology proven to scale to dozens of petabytes on commodity servers.\n\n### Spark In-Memory Computation Engine\n\nSplice Machine uses Spark for analytical processing.\n\nApache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports a general execution graph on sets of data.\n\n<img class=\"tiny\" src=\"https://spark.apache.org/docs/latest/img/spark-logo-hd.png\">\n\nSpark has very efficient in-memory processing that can spill to disk (instead of dropping the query) if the query processing exceeds available memory. Spark is also unique in its resilience to node failures, which may occur in a commodity cluster. Other in-memory technologies will drop all queries associated with a failed node, while Spark uses ancestry (as opposed to replicating data) to regenerate its in-memory Resilient Distributed Datasets (RDDs) on another node.\n\nThe main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. Users may also ask Spark to persist an RDD in memory, allowing it to be reused efficiently across parallel operations. Finally, RDDs automatically recover from node failures.\n\n#### Spark RDD Operations\n\nRDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. For example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset).\n\nAll transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently. For example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.\n\n#### Spark Acceleration\n\nSplice Machine accelerates generation of Spark RDDs by reading HBase HFiles in HDFS and augmenting it with any changes in Memstore that have not been flushed to HFiles. Splice Machine then uses the RDDs and Spark operators to distribute processing across Spark Workers.\n\n### Resource Isolation\n\nSplice Machine isolates the resources allocated to HBase and Spark from each other, so each can progress independent of the workload of the other. Combined with the MVCC locking mechanism, this ensures that the performance level of transactional workloads can remain high, even if large reports or analytic processes are running.\n\n",
    "user": "splice",
    "dateUpdated": "2018-05-21T21:43:33+0000",
    "config": {
      "tableHide": false,
      "editorSetting": {
        "language": "markdown",
        "editOnDblClick": true
      },
      "colWidth": 12,
      "editorMode": "ace/mode/markdown",
      "editorHide": true,
      "results": {},
      "enabled": true,
      "fontSize": 9
    },
    "settings": {
      "params": {},
      "forms": {}
    },
    "results": {
      "code": "SUCCESS",
      "msg": [{
        "type": "HTML",
        "data": "<div class=\"markdown-body\">\n<h2>Technology Stack Overview</h2>\n<p>Splice Machine is built on open-sourced, proven, distributed database technology, including HBase/Hadoop and Spark.</p>\n<h3>HBase/Hadoop</h3>\n<p>The persistent, durable storage of operational data in Splice Machine resides in the Apache HBase key-value store. HBase:</p>\n<ul>\n  <li>is a non-relational (NoSQL) database that runs on top of HDFS</li>\n  <li>provides real-time read/write access to large datasets</li>\n  <li>scales linearly to handle huge data sets with billions of rows and millions of columns</li>\n  <li>is stored row-based and sorted by a primary key to deliver 1ms-10ms lookup speeds and short-range scans</li>\n</ul>\n<img class=\"tiny\" src=\"https://hbase.apache.org/images/hbase_logo_with_orca.png\">\n<p>HBase uses the Hadoop Distributed File System (HDFS) for reliable and replicated storage. HBase/HDFS provides auto-sharding and failover technology for scaling database tables across multiple servers. It is the only technology proven to scale to dozens of petabytes on commodity servers.</p>\n<h3>Spark In-Memory Computation Engine</h3>\n<p>Splice Machine uses Spark for analytical processing.</p>\n<p>Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports a general execution graph on sets of data.</p>\n<img class=\"tiny\" src=\"https://spark.apache.org/docs/latest/img/spark-logo-hd.png\">\n<p>Spark has very efficient in-memory processing that can spill to disk (instead of dropping the query) if the query processing exceeds available memory. Spark is also unique in its resilience to node failures, which may occur in a commodity cluster. Other in-memory technologies will drop all queries associated with a failed node, while Spark uses ancestry (as opposed to replicating data) to regenerate its in-memory Resilient Distributed Datasets (RDDs) on another node.</p>\n<p>The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. Users may also ask Spark to persist an RDD in memory, allowing it to be reused efficiently across parallel operations. Finally, RDDs automatically recover from node failures.</p>\n<h4>Spark RDD Operations</h4>\n<p>RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. For example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset).</p>\n<p>All transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently. For example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.</p>\n<h4>Spark Acceleration</h4>\n<p>Splice Machine accelerates generation of Spark RDDs by reading HBase HFiles in HDFS and augmenting it with any changes in Memstore that have not been flushed to HFiles. Splice Machine then uses the RDDs and Spark operators to distribute processing across Spark Workers.</p>\n<h3>Resource Isolation</h3>\n<p>Splice Machine isolates the resources allocated to HBase and Spark from each other, so each can progress independent of the workload of the other. Combined with the MVCC locking mechanism, this ensures that the performance level of transactional workloads can remain high, even if large reports or analytic processes are running.</p>\n</div>"
      }]
    },
    "apps": [],
    "jobName": "paragraph_1520184188049_-345900824",
    "id": "20180125-145332_2050347825",
    "dateCreated": "2018-03-04T17:23:08+0000",
    "dateStarted": "2018-05-21T21:43:33+0000",
    "dateFinished": "2018-05-21T21:43:34+0000",
    "status": "FINISHED",
    "progressUpdateIntervalMs": 500,
    "$$hashKey": "object:119277"
  }, {
    "text": "%md\n## Internal Storage Using HBase\n\nSplice Machine uses HBase to internally store data. HBase is modeled after Google Big Table, which is a large, distributed associative map stored as a Log-Structured Merge Tree. In HBase:\n\n* Users store data rows in labelled tables.\n* Each data row has a sortable key and an aribtrary number of columns.\n* \nHBase is often misunderstood because many call it a column-oriented datastore. This just means columns are grouped in separately separately stored column families. But all data is still ordered by row.\n\nAn HBase cluster has a service known as the *HBase Master* that coordinates the HBase Cluster and is responsible for administrative operations.\n\nSplice Machine also uses ZooKeeper, which is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services on a cluster. \n\nHere's a diagram showing how HBase operates in Splice Machine:\n<img class=\"fitwidth\" src=\"https://s3.amazonaws.com/splice-examples/images/tutorials/hbases_storage_architecture2.png\">\n<br />\n### Region Servers and Regions\n\nHBase auto-shards the data in a table across *Region Servers*:\n\n* Each region server has a set of *Regions*.\n* Each region is a set of rows sorted by a primary key.\n* \nWhen a region server fails to respond, HBase makes its regions accessible on other region servers. HBase is resilient to region server failures as well as to failure of Hadoop Data Nodes. \n\n### HBase Data Storage\n\nHBase writes data to an in-memory store, called *memstore*. Once this memstore reaches a certain size, it is flushed to disk into a *store file*; everything is also written immediately to a log file for durability. \n\nThe store files created on disk are immutable. Sometimes the store files are merged together, this is done by a process called *compaction*. Store files are on the Hadoop Distributed File System (<em>HDFS</em>) and are replicated for fault-tolerance. \n\n",
    "user": "splice",
    "dateUpdated": "2018-07-26T21:25:30+0000",
    "config": {
      "tableHide": false,
      "editorSetting": {
        "language": "markdown",
        "editOnDblClick": true
      },
      "colWidth": 12,
      "editorMode": "ace/mode/markdown",
      "editorHide": true,
      "results": {},
      "enabled": true,
      "fontSize": 9
    },
    "settings": {
      "params": {},
      "forms": {}
    },
    "results": {
      "code": "SUCCESS",
      "msg": [{
        "type": "HTML",
        "data": "<div class=\"markdown-body\">\n<h2>Internal Storage Using HBase</h2>\n<p>Splice Machine uses HBase to internally store data. HBase is modeled after Google Big Table, which is a large, distributed associative map stored as a Log-Structured Merge Tree. In HBase:</p>\n<ul>\n  <li>Users store data rows in labelled tables.</li>\n  <li>Each data row has a sortable key and an aribtrary number of columns.</li>\n  <li>HBase is often misunderstood because many call it a column-oriented datastore. This just means columns are grouped in separately separately stored column families. But all data is still ordered by row.</li>\n</ul>\n<p>An HBase cluster has a service known as the <em>HBase Master</em> that coordinates the HBase Cluster and is responsible for administrative operations.</p>\n<p>Splice Machine also uses ZooKeeper, which is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services on a cluster. </p>\n<p>Here&rsquo;s a diagram showing how HBase operates in Splice Machine:<br/><img class=\"fitwidth\" src=\"https://s3.amazonaws.com/splice-examples/images/tutorials/hbases_storage_architecture2.png\"><br/><br /></p>\n<h3>Region Servers and Regions</h3>\n<p>HBase auto-shards the data in a table across <em>Region Servers</em>:</p>\n<ul>\n  <li>Each region server has a set of <em>Regions</em>.</li>\n  <li>Each region is a set of rows sorted by a primary key.</li>\n  <li>When a region server fails to respond, HBase makes its regions accessible on other region servers. HBase is resilient to region server failures as well as to failure of Hadoop Data Nodes.</li>\n</ul>\n<h3>HBase Data Storage</h3>\n<p>HBase writes data to an in-memory store, called <em>memstore</em>. Once this memstore reaches a certain size, it is flushed to disk into a <em>store file</em>; everything is also written immediately to a log file for durability. </p>\n<p>The store files created on disk are immutable. Sometimes the store files are merged together, this is done by a process called <em>compaction</em>. Store files are on the Hadoop Distributed File System (<em>HDFS</em>) and are replicated for fault-tolerance.</p>\n</div>"
      }]
    },
    "apps": [],
    "jobName": "paragraph_1520184188050_-344746577",
    "id": "20180125-141721_42146671",
    "dateCreated": "2018-03-04T17:23:08+0000",
    "dateStarted": "2018-07-26T21:25:30+0000",
    "dateFinished": "2018-07-26T21:25:30+0000",
    "status": "FINISHED",
    "progressUpdateIntervalMs": 500,
    "$$hashKey": "object:119278"
  }, {
    "text": "%md\n## Where to Go Next\n\nThe next notebook in this presentation walks you through <a href=\"/#/notebook/2D6ZFHE9U\">importing and running the TPC-H benchmark data</a> by creating a database in Splice Machine, importing the TPC-H dataset, running queries, and improving performance by indexing the data.\n",
    "user": "splice",
    "dateUpdated": "2018-08-14T15:05:49+0000",
    "config": {
      "tableHide": false,
      "editorSetting": {
        "language": "markdown",
        "editOnDblClick": true
      },
      "colWidth": 12,
      "editorMode": "ace/mode/markdown",
      "editorHide": true,
      "results": {},
      "enabled": true,
      "fontSize": 9
    },
    "settings": {
      "params": {},
      "forms": {}
    },
    "results": {
      "code": "SUCCESS",
      "msg": [{
        "type": "HTML",
        "data": "<div class=\"markdown-body\">\n<h2>Where to Go Next</h2>\n<p>The next notebook in this presentation walks you through <a href=\"/#/notebook/2D6ZFHE9U\">importing and running the TPC-H benchmark data</a> by creating a database in Splice Machine, importing the TPC-H dataset, running queries, and improving performance by indexing the data.</p>\n</div>"
      }]
    },
    "apps": [],
    "jobName": "paragraph_1520184188050_-344746577",
    "id": "20180125-142959_1101825868",
    "dateCreated": "2018-03-04T17:23:08+0000",
    "dateStarted": "2018-08-14T15:05:49+0000",
    "dateFinished": "2018-08-14T15:05:49+0000",
    "status": "FINISHED",
    "progressUpdateIntervalMs": 500,
    "$$hashKey": "object:119279"
  }, {
    "text": "%md\n",
    "dateUpdated": "2018-03-04T17:23:08+0000",
    "config": {
      "colWidth": 12,
      "editorMode": "ace/mode/markdown",
      "results": {},
      "enabled": true,
      "editorSetting": {
        "language": "markdown",
        "editOnDblClick": true
      },
      "fontSize": 9
    },
    "settings": {
      "params": {},
      "forms": {}
    },
    "apps": [],
    "jobName": "paragraph_1520184188050_-344746577",
    "id": "20180128-174743_132342643",
    "dateCreated": "2018-03-04T17:23:08+0000",
    "status": "READY",
    "errorMessage": "",
    "progressUpdateIntervalMs": 500,
    "$$hashKey": "object:119280"
  }],
  "name": "3. Splice Deep Dive / 1. Introduction",
  "id": "2D76NPKV6",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "2DKP5EMTC:shared_process": [],
    "2DKVQCZ7K:shared_process": [],
    "2DMCKRAR8:shared_process": [],
    "2DQ6VH68X:shared_process": [],
    "2DKQWEG1R:shared_process": [],
    "2DMC6KXQ8:shared_process": []
  },
  "config": {
    "looknfeel": "default",
    "personalizedMode": "false",
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}
