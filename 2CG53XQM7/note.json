{
  "paragraphs": [
    {
      "text": "%md\n<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/zeppelin/css/zepstyles.css\" />\n\n# Kafka Queue\n\nIn this notebook, kafka topic is created and the data producer is started.  The producer is currently reading data from a csv file, with each line corresponding to a message.  The message contains comma separated values of data that map to various tables in the splice database. \n\n* There are \"Items\" that have a ID, Serial Number, CreatedTime and UPCcode.  \n* There are \"ItemFlow\" events that occur at high frequency (1,000's per second, 600,000 in total in this demo) and are ingested into Splice\n* An event occurs when an Item moves from Warehouse, arrives at a store, is seen at the POS terminal, in a Dressingroom or at a Door\n* There are multiple warehouses and stores, with location coordinates all in a geographic region east of London.\n\nAfter verifying/setting the values for the following, the notebook (all paragraphs) can be run by just selecting the Run from top tool bar or each paragraph can be run individually in the order they appear.\n\n* Topic Name\n* Zookeeper URL\n* Broker URL\n* File Name of the data file  \n\n<p class=\"noteIcon\">\nNote: This assumes kafka server is already running. The Zookeeper and Broker need to set appropriately in the next paragraph 'Set Parameters'.\n</p>\n\n",
      "user": "splice",
      "dateUpdated": "2018-08-02T15:42:36+0000",
      "config": {
        "colWidth": 12,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/zeppelin/css/zepstyles.css\" />\n<h1>Kafka Queue</h1>\n<p>In this notebook, kafka topic is created and the data producer is started. The producer is currently reading data from a csv file, with each line corresponding to a message. The message contains comma separated values of data that map to various tables in the splice database. </p>\n<ul>\n  <li>There are &ldquo;Items&rdquo; that have a ID, Serial Number, CreatedTime and UPCcode.</li>\n  <li>There are &ldquo;ItemFlow&rdquo; events that occur at high frequency (1,000&rsquo;s per second, 600,000 in total in this demo) and are ingested into Splice</li>\n  <li>An event occurs when an Item moves from Warehouse, arrives at a store, is seen at the POS terminal, in a Dressingroom or at a Door</li>\n  <li>There are multiple warehouses and stores, with location coordinates all in a geographic region east of London.</li>\n</ul>\n<p>After verifying/setting the values for the following, the notebook (all paragraphs) can be run by just selecting the Run from top tool bar or each paragraph can be run individually in the order they appear.</p>\n<ul>\n  <li>Topic Name</li>\n  <li>Zookeeper URL</li>\n  <li>Broker URL</li>\n  <li>File Name of the data file</li>\n</ul>\n<p class=\"noteIcon\">\nNote: This assumes kafka server is already running. The Zookeeper and Broker need to set appropriately in the next paragraph 'Set Parameters'.\n</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533224103671_2072631894",
      "id": "20180802-153503_97749617",
      "dateCreated": "2018-08-02T15:35:03+0000",
      "dateStarted": "2018-08-02T15:42:36+0000",
      "dateFinished": "2018-08-02T15:42:36+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "focus": true,
      "$$hashKey": "object:74346"
    },
    {
      "title": "Set Parameters",
      "text": "%spark\nz.put(\"topicname\", \"iotdemo\")\nz.put(\"zookeeper\", \"zookeeper-0-node.{FRAMEWORKNAME}.mesos:2182\")\nz.put(\"brokers\", \"kafka-0-node.{FRAMEWORKNAME}.mesos:9092\")\n",
      "dateUpdated": "2017-05-19T21:53:41+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {
          "topic_name": "zeptest",
          "topicname": "zeptest",
          "topic name": "zeptest"
        },
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1495230821060_1722266738",
      "id": "20170508-124350_1273037402",
      "dateCreated": "2017-05-19T21:53:41+0000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:74347"
    },
    {
      "text": "%angular\nNext we will create kafka topic\nThe steps to create kafka topic are\n<ui>\n<li> Specify the Queue parameters : like session timeout, connectiontimeout, number of partitions and replication factor.\n<li> Create Zookeper client\n<li> Invoke AdminUtils to create topic\n</ui>\n\nWhen this paragraph is run, the topic is created or error is displayed if the topic already exists",
      "dateUpdated": "2017-05-19T21:53:41+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "text",
          "editOnDblClick": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/undefined",
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS"
      },
      "apps": [],
      "jobName": "paragraph_1495230821061_1721881989",
      "id": "20170515-105110_1320641479",
      "dateCreated": "2017-05-19T21:53:41+0000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:74348"
    },
    {
      "title": "Create Topic",
      "text": "%spark\nimport java.util.Properties\nimport kafka.admin.AdminUtils\nimport kafka.utils.ZkUtils\n\n//Properties for zookeeper client\nval sessionTimeoutMs = 10000\nval connectionTimeoutMs = 10000\n\n//Properties for Kafak Queue\nval topicName=z.get(\"topicname\").toString\nval numPartitions = 10\nval replicationFactor = 1\n\n// Create a ZooKeeper client\nval zkUtils = ZkUtils.apply(z.get(\"zookeeper\").toString, sessionTimeoutMs, connectionTimeoutMs,\n    false)\n    \n\n// Create  topic\nval topicConfig = new Properties\nAdminUtils.createTopic(zkUtils, topicName, numPartitions, replicationFactor, topicConfig)\n",
      "dateUpdated": "2017-05-19T21:53:41+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1495230821061_1721881989",
      "id": "20170504-130003_598679618",
      "dateCreated": "2017-05-19T21:53:41+0000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:74349"
    },
    {
      "text": "%md\nBefore running this, ensure appropriate file is set to filename variable in the code below.",
      "user": "splice",
      "dateUpdated": "2018-08-02T15:40:47+0000",
      "config": {
        "colWidth": 12,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Before running this, ensure appropriate file is set to filename variable in the code below.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1533224433906_1755425301",
      "id": "20180802-154033_883560831",
      "dateCreated": "2018-08-02T15:40:33+0000",
      "dateStarted": "2018-08-02T15:40:47+0000",
      "dateFinished": "2018-08-02T15:40:47+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:74350"
    },
    {
      "title": "Kafka Producer",
      "text": "%spark\nimport org.apache.commons.io.IOUtils\nimport java.net.URL\nimport java.util.Properties\nimport java.nio.charset.Charset\nimport scala.io.Source\nimport java.io.{FileReader, FileNotFoundException, IOException}\nimport org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\nimport org.apache.kafka.common.serialization.StringSerializer\n\n\n//Properties\nval brokers = z.get(\"brokers\")\nval topic = z.get(\"topicname\").toString\n\nval messagesPerSec=1000\nval pauseBetweenMessages = 500\n\n//val filename = \"https://s3.amazonaws.com/splice-demo/iot/itemflow_small.csv\"\nval filename = \"https://s3.amazonaws.com/splice-demo/iot/itemflow_200k.csv\"\n//val filename = \"https://s3.amazonaws.com/splice-demo/iot/itemflow_600k.csv\"\n     \n//Add properties\nval props =new Properties\nprops.put(\"bootstrap.servers\", brokers)\nprops.put(\"acks\", \"all\")\nprops.put(\"retries\",new Integer( 0))\nprops.put(\"batch.size\",new Integer( 16384))\nprops.put(\"linger.ms\",new Integer( 1))\nprops.put(\"buffer.memory\", new Integer(33554432))\nprops.put(\"key.serializer\",\n        \"org.apache.kafka.common.serialization.StringSerializer\")\nprops.put(\"value.serializer\",\n        \"org.apache.kafka.common.serialization.StringSerializer\")\n    \n//Create Kfka producer\nval producer = new KafkaProducer[String, String](props)\n    \n    \n//Read data file\nval s3fileData = sc.parallelize(\n    IOUtils.toString(new URL( filename), Charset.forName(\"utf8\")).split(\"\\n\"))\n    \n\n//Put each line from file onto Queue in batchs specified by properties\nvar i = 0\ns3fileData.collect().foreach(line =>  {\n        val message =  new ProducerRecord[String, String](topic, null, line)\n        producer.send(message)\n        i= i+1;\n        if (i >= messagesPerSec) {\n            i = 0;\n            Thread.sleep(pauseBetweenMessages)\n         }\n   }\n)\n    \nprintln (\"DONE\")",
      "dateUpdated": "2017-05-19T21:53:41+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1495230821062_1723036236",
      "id": "20170504-170549_1349622410",
      "dateCreated": "2017-05-19T21:53:41+0000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:74351"
    }
  ],
  "name": "4. Reference Applications / 2. IoT / 3. Kafka",
  "id": "2CG53XQM7",
  "angularObjects": {
    "2DNHZSRUQ:shared_process": [],
    "2DP6VF566:shared_process": [],
    "2DM92W265:shared_process": [],
    "2DPD6828Z:shared_process": [],
    "2DPSBXP2Z:shared_process": [],
    "2DNP2K3KJ:shared_process": []
  },
  "config": {
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}