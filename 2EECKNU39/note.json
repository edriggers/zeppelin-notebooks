{
  "paragraphs": [
    {
      "text": "%md\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n# Using spark-submit With the Splice Machine Native Spark DataSource\n\nIn the basic development course, you developed native spark applications in Zeppelin notebooks. In this notebook, we\u0027ll develop code that runs on the cluster via *spark-submit*.\n\n## Using the Splice Machine Native Spark DataSource\n\nThe *Splice Machine Native Spark DataSource*, which is also referred to as the Spark Adapter, allows you to directly connect Spark DataFrames and Splice Machine database tables, bypassing the need to send your data to/from Spark over a JDBC or ODBC connection. You can efficiently insert, upsert, select, update, and delete data in your Splice Machine tables directly from Spark in a transactionally consistent manner. With the Spark Adapter, transfers of data between Spark and your database are completed without serialization/deserialization, which generates tremendous performance boosts over traditional over-the-wire (sequentially over a JDBC/ODBC connection) transfers.\n\nTo use the Spark Adapter in your code, you simply instantiate a `SplicemachineContext` object in your Spark code. You can run Spark applications that interface with your Splice Machine database interactively in the Spark shell, in Zeppelin notebooks, or by using our Spark Submit script. One common use of the Adapter is to ingest data into your Splice Machine database directly from a Spark DataFrame.\n\nThe Native DataSource allows data scientists to bypass the limitations of the SQL-based JDBC interface in favor of the more scalable and powerful Spark DataFrame API, making it possible for them to operate on data at scale and ingest real-time streaming data with outstanding performance. You can craft applications that use Spark and our Native Spark DataSource in Scala, Python, and Java. Note that you can use the Native Spark DataSource in the Splice Machine ML Manager and Zeppelin Notebook interfaces.\n\n",
      "user": "anonymous",
      "dateUpdated": "2019-06-18 17:37:57.893",
      "config": {
        "editorHide": true,
        "enabled": true,
        "results": {},
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\u003ch1\u003eUsing spark-submit With the Splice Machine Native Spark DataSource\u003c/h1\u003e\n\u003cp\u003eIn the basic development course, you developed native spark applications in Zeppelin notebooks. In this notebook, we\u0026rsquo;ll develop code that runs on the cluster via \u003cem\u003espark-submit\u003c/em\u003e.\u003c/p\u003e\n\u003ch2\u003eUsing the Splice Machine Native Spark DataSource\u003c/h2\u003e\n\u003cp\u003eThe \u003cem\u003eSplice Machine Native Spark DataSource\u003c/em\u003e, which is also referred to as the Spark Adapter, allows you to directly connect Spark DataFrames and Splice Machine database tables, bypassing the need to send your data to/from Spark over a JDBC or ODBC connection. You can efficiently insert, upsert, select, update, and delete data in your Splice Machine tables directly from Spark in a transactionally consistent manner. With the Spark Adapter, transfers of data between Spark and your database are completed without serialization/deserialization, which generates tremendous performance boosts over traditional over-the-wire (sequentially over a JDBC/ODBC connection) transfers.\u003c/p\u003e\n\u003cp\u003eTo use the Spark Adapter in your code, you simply instantiate a \u003ccode\u003eSplicemachineContext\u003c/code\u003e object in your Spark code. You can run Spark applications that interface with your Splice Machine database interactively in the Spark shell, in Zeppelin notebooks, or by using our Spark Submit script. One common use of the Adapter is to ingest data into your Splice Machine database directly from a Spark DataFrame.\u003c/p\u003e\n\u003cp\u003eThe Native DataSource allows data scientists to bypass the limitations of the SQL-based JDBC interface in favor of the more scalable and powerful Spark DataFrame API, making it possible for them to operate on data at scale and ingest real-time streaming data with outstanding performance. You can craft applications that use Spark and our Native Spark DataSource in Scala, Python, and Java. Note that you can use the Native Spark DataSource in the Splice Machine ML Manager and Zeppelin Notebook interfaces.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559319019968_-1240214828",
      "id": "20190531-161019_1313997856",
      "dateCreated": "2019-05-31 16:10:19.968",
      "dateStarted": "2019-06-18 17:37:57.893",
      "dateFinished": "2019-06-18 17:37:57.946",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Why Use the Native Data Source?\n\nThe primary reason for using the Native DataSource is that it provides dramatic performance improvements for large scale data operations; this is because the DataSource works directly on native DataFrames and RDDs, thus eliminating the need to serialize data. Spark is optimized to work on DataFrames, which is a distributed collection of data (an RDD) organized into named columns, with a schema that specifies data types, that is designed to support efficiently operating on scalable, massive datasets.\n\nThe Splice Machine DataSource is native to Spark, which means that it operates directly on these DataFrames and in the same Spark executors that your programs are using to analyze or transform the data. Instead of accessing, inserting, or manipulating data one record at a time over a serialized connection, you can use the Splice Machine Native Spark DataSource to pull the contents of an entire DataFrame into your database, and to pull database query results into a DataFrame.\n\nSplice Machine has observed 100x performance increases compared to using JDBC for operations such as inserting millions of records in a database! For example, a typical web application might use a JDBC connection to query the database, pulling information out one record at a time to populate the screen. The results of each query are serialized (turned into a string of data), then sent over a network connection to the app, and then displayed on the customer’s screen.\n\nIn contrast, when you use the Splice Machine Native Spark DataSource, the contents of the database table are typically sitting in a DataFrame in memory that resides on the same Spark executor that’s performing the query. The query takes place in memory, and there’s no need to serialize or stream the results over a wire. Similarly, when the app sends updates to the database, the data is inserted into the database from in-memory DataFrames directly to the tables without serialization. As a result, a great deal of overhead is eliminated, and performance gains can be remarkable.\n\n",
      "user": "anonymous",
      "dateUpdated": "2019-06-05 23:06:28.660",
      "config": {
        "editorHide": true,
        "enabled": false,
        "results": {},
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "msg": [
          {
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\u003ch2\u003eWhy Use the Native Data Source?\u003c/h2\u003e\n\u003cp\u003eThe primary reason for using the Native DataSource is that it provides dramatic performance improvements for large scale data operations; this is because the DataSource works directly on native DataFrames and RDDs, thus eliminating the need to serialize data. Spark is optimized to work on DataFrames, which is a distributed collection of data (an RDD) organized into named columns, with a schema that specifies data types, that is designed to support efficiently operating on scalable, massive datasets.\u003c/p\u003e\n\u003cp\u003eThe Splice Machine DataSource is native to Spark, which means that it operates directly on these DataFrames and in the same Spark executors that your programs are using to analyze or transform the data. Instead of accessing, inserting, or manipulating data one record at a time over a serialized connection, you can use the Splice Machine Native Spark DataSource to pull the contents of an entire DataFrame into your database, and to pull database query results into a DataFrame.\u003c/p\u003e\n\u003cp\u003eSplice Machine has observed 100x performance increases compared to using JDBC for operations such as inserting millions of records in a database! For example, a typical web application might use a JDBC connection to query the database, pulling information out one record at a time to populate the screen. The results of each query are serialized (turned into a string of data), then sent over a network connection to the app, and then displayed on the customer’s screen.\u003c/p\u003e\n\u003cp\u003eIn contrast, when you use the Splice Machine Native Spark DataSource, the contents of the database table are typically sitting in a DataFrame in memory that resides on the same Spark executor that’s performing the query. The query takes place in memory, and there’s no need to serialize or stream the results over a wire. Similarly, when the app sends updates to the database, the data is inserted into the database from in-memory DataFrames directly to the tables without serialization. As a result, a great deal of overhead is eliminated, and performance gains can be remarkable.\u003c/p\u003e\n\u003c/div\u003e",
            "type": "HTML"
          }
        ],
        "code": "SUCCESS"
      },
      "apps": [],
      "jobName": "paragraph_1559506884945_1945185413",
      "id": "20190602-202124_1993675938",
      "dateCreated": "2019-06-02 20:21:24.945",
      "dateStarted": "2019-06-05 23:04:52.241",
      "dateFinished": "2019-06-05 23:04:52.293",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## The SpliceMachineContext Class\n\nThe first thing you need to do when using the Native Spark DataSource is to create an instance of the `SplicemachineContext` class; this is the primary serializable class that you can broadcast in your Spark applications. This class interacts with your Splice Machine cluster in your Spark executors, and provides the methods that you can use to perform operations such as:\n\n* Interfacing with Splice Machine RDD\n* Running inserts, updates and deletes on data\n* Converting data types between Splice Machine and Spark",
      "user": "anonymous",
      "dateUpdated": "2019-06-18 17:40:25.077",
      "config": {
        "editorHide": true,
        "enabled": true,
        "results": {},
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eThe SpliceMachineContext Class\u003c/h2\u003e\n\u003cp\u003eThe first thing you need to do when using the Native Spark DataSource is to create an instance of the \u003ccode\u003eSplicemachineContext\u003c/code\u003e class; this is the primary serializable class that you can broadcast in your Spark applications. This class interacts with your Splice Machine cluster in your Spark executors, and provides the methods that you can use to perform operations such as:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eInterfacing with Splice Machine RDD\u003c/li\u003e\n  \u003cli\u003eRunning inserts, updates and deletes on data\u003c/li\u003e\n  \u003cli\u003eConverting data types between Splice Machine and Spark\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559508964492_919362593",
      "id": "20190602-205604_247573122",
      "dateCreated": "2019-06-02 20:56:04.492",
      "dateStarted": "2019-06-18 17:40:25.078",
      "dateFinished": "2019-06-18 17:40:25.106",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Set Up Maven Coordinates\n\nYou\u0027ll need to set up a dependency resolution system to be able to build the code. Splice Machine uses Maven, and we\u0027ve provide sample code and a `pom.xml` file in a public github repo. You can download the source files here: [Sample Source Code](https://github.com/splicemachine/splice-training/tree/master/spark_adapter_example).\n\nYou can use this sample as a template for Maven setup. It has both CDH or HWX dependency profiles that can be selected via maven configuration at build time.\n\n",
      "user": "anonymous",
      "dateUpdated": "2019-06-18 17:39:56.250",
      "config": {
        "editorHide": false,
        "enabled": true,
        "results": {},
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {
          "spliceVersion": "",
          "project.build.directory": "",
          "artifactId": "",
          "version": ""
        },
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eSet Up Maven Coordinates\u003c/h2\u003e\n\u003cp\u003eYou\u0026rsquo;ll need to set up a dependency resolution system to be able to build the code. Splice Machine uses Maven, and we\u0026rsquo;ve provide sample code and a \u003ccode\u003epom.xml\u003c/code\u003e file in a public github repo. You can download the source files here: \u003ca href\u003d\"https://github.com/splicemachine/splice-training/tree/master/spark_adapter_example\"\u003eSample Source Code\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eYou can use this sample as a template for Maven setup. It has both CDH or HWX dependency profiles that can be selected via maven configuration at build time.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559511543147_1644452621",
      "id": "20190602-213903_1547198808",
      "dateCreated": "2019-06-02 21:39:03.147",
      "dateStarted": "2019-06-18 01:53:59.292",
      "dateFinished": "2019-06-18 01:53:59.306",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Create SpliceMachineContext\n\nThe following code snippet illustrates creating a `SpliceMachineContext` object in Java after setting up the project:\n\n\u003cpre\u003e\u003ccode\u003e\nimport com.splicemachine.derby.impl.SpliceSpark;\nimport com.splicemachine.spark.splicemachine.SplicemachineContext;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n....\n\npublic class TestSpliceMachineContext\n{\n    public static void main(String[] args) throws Exception \n    {\n        String dbUrl \u003d \"jdbc:splice://SPLICESERVERHOST:1527/splicedb;user\u003d\u0026lt;yourUserId\u0026gt;;password\u003d\u0026lt;yourPassword\u0026gt;\";\n        SparkConf conf \u003d new SparkConf();\n        SparkSession spark \u003d SparkSession.builder().appName(\"NativeSparktutor\").config(conf).getOrCreate();\n        SpliceSpark.setContext(spark.sparkContext());\n        SplicemachineContext splicemachineContext \u003d new SplicemachineContext(dbUrl);\n        \n        //use context for DB operations next...\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e",
      "user": "anonymous",
      "dateUpdated": "2019-06-18 17:39:56.082",
      "config": {
        "editorHide": false,
        "enabled": true,
        "results": {},
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\u003ch2\u003eCreate SpliceMachineContext\u003c/h2\u003e\n\u003cp\u003eThe following code snippet illustrates creating a \u003ccode\u003eSpliceMachineContext\u003c/code\u003e object in Java after setting up the project:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\nimport com.splicemachine.derby.impl.SpliceSpark;\nimport com.splicemachine.spark.splicemachine.SplicemachineContext;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n....\n\npublic class TestSpliceMachineContext\n{\n    public static void main(String[] args) throws Exception \n    {\n        String dbUrl \u003d \"jdbc:splice://SPLICESERVERHOST:1527/splicedb;user\u003d\u0026lt;yourUserId\u0026gt;;password\u003d\u0026lt;yourPassword\u0026gt;\";\n        SparkConf conf \u003d new SparkConf();\n        SparkSession spark \u003d SparkSession.builder().appName(\"NativeSparktutor\").config(conf).getOrCreate();\n        SpliceSpark.setContext(spark.sparkContext());\n        SplicemachineContext splicemachineContext \u003d new SplicemachineContext(dbUrl);\n        \n        //use context for DB operations next...\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559511623111_-960005237",
      "id": "20190602-214023_872601603",
      "dateCreated": "2019-06-02 21:40:23.111",
      "dateStarted": "2019-06-12 15:39:41.416",
      "dateFinished": "2019-06-12 15:39:41.449",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Querying with SpliceMachineContext\n\nYou can use the `SpliceMachineContext` to obtain access to Spark DataFrame while querying the DB. \n\n```\nimport com.splicemachine.derby.impl.SpliceSpark;\nimport com.splicemachine.spark.splicemachine.SplicemachineContext;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n....\n\npublic class TestSpliceMachineContext\n{\n    public static void main(String[] args) throws Exception \n    {\n        String dbUrl \u003d \"jdbc:splice://SPLICESERVERHOST:1527/splicedb;user\u003d\u0026lt;yourUserId\u0026gt;;password\u003d\u0026lt;yourPassword\u0026gt;\";\n        SparkConf conf \u003d new SparkConf();\n        SparkSession spark \u003d SparkSession.builder().appName(\"NativeSparktutor\").config(conf).getOrCreate();\n        SpliceSpark.setContext(spark.sparkContext());\n        SplicemachineContext splicemachineContext \u003d new SplicemachineContext(dbUrl);\n        \n        //Query table, use df operations based out of SpliceMachineContext\n        String spliceQuery \u003d \"select count(*) from \" + \u0026lt;TABLE_NAME\u0026gt;;\n        splicemachineContext.df(spliceQuery).show();\n    }\n}\n```",
      "user": "anonymous",
      "dateUpdated": "2019-06-18 17:40:32.906",
      "config": {
        "editorHide": true,
        "enabled": true,
        "results": {},
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eQuerying with SpliceMachineContext\u003c/h2\u003e\n\u003cp\u003eYou can use the \u003ccode\u003eSpliceMachineContext\u003c/code\u003e to obtain access to Spark DataFrame while querying the DB. \u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport com.splicemachine.derby.impl.SpliceSpark;\nimport com.splicemachine.spark.splicemachine.SplicemachineContext;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n....\n\npublic class TestSpliceMachineContext\n{\n    public static void main(String[] args) throws Exception \n    {\n        String dbUrl \u003d \u0026quot;jdbc:splice://SPLICESERVERHOST:1527/splicedb;user\u003d\u0026amp;lt;yourUserId\u0026amp;gt;;password\u003d\u0026amp;lt;yourPassword\u0026amp;gt;\u0026quot;;\n        SparkConf conf \u003d new SparkConf();\n        SparkSession spark \u003d SparkSession.builder().appName(\u0026quot;NativeSparktutor\u0026quot;).config(conf).getOrCreate();\n        SpliceSpark.setContext(spark.sparkContext());\n        SplicemachineContext splicemachineContext \u003d new SplicemachineContext(dbUrl);\n        \n        //Query table, use df operations based out of SpliceMachineContext\n        String spliceQuery \u003d \u0026quot;select count(*) from \u0026quot; + \u0026amp;lt;TABLE_NAME\u0026amp;gt;;\n        splicemachineContext.df(spliceQuery).show();\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559578447544_-125019814",
      "id": "20190603-161407_2067737309",
      "dateCreated": "2019-06-03 16:14:07.544",
      "dateStarted": "2019-06-18 17:40:32.914",
      "dateFinished": "2019-06-18 17:40:32.946",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Loading Data with SpliceMachineContext\n\nYou can use the `SpliceMachineContext` to load data into a table; in the following example, we read from a parquet file and load the \ndata into Splice Machine table.\n\n```\nimport com.splicemachine.derby.impl.SpliceSpark;\nimport com.splicemachine.spark.splicemachine.SplicemachineContext;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n....\n\npublic class TestSpliceMachineContext\n{\n    public static void main(String[] args) throws Exception \n    {\n        String dbUrl \u003d \"jdbc:splice://SPLICESERVERHOST:1527/splicedb;user\u003d\u0026lt;yourUserId\u0026gt;;password\u003d\u0026lt;yourPassword\u0026gt;\";\n        SparkConf conf \u003d new SparkConf();\n        SparkSession spark \u003d SparkSession.builder().appName(\"NativeSparktutor\").config(conf).getOrCreate();\n        SpliceSpark.setContext(spark.sparkContext());\n        SplicemachineContext splicemachineContext \u003d new SplicemachineContext(dbUrl);\n        \n        //create sample dataset\n        List\u0026lt;Row\u0026gt; cars \u003d new ArrayList\u0026lt;Row\u0026gt;();\n    \tStructType schema \u003d DataTypes.createStructType(new StructField[] {\n                DataTypes.createStructField(\"SERIAL\",  DataTypes.IntegerType, true),\n                DataTypes.createStructField(\"MAKE\", DataTypes.StringType, true),\n                DataTypes.createStructField(\"MODEL\", DataTypes.StringType, true)\n        });\n    \t\n    \tcars.add(RowFactory.create(1, \"Toyota\", \"Camry\"));\n    \tcars.add(RowFactory.create(2, \"Honda\", \"Accord\"));\n    \tcars.add(RowFactory.create(3, \"Subaru\", \"Impreza\"));\n    \tcars.add(RowFactory.create(4, \"Chevy\", \"Volt\"));\n    \t\n    \t\n    \tDataset\u003cRow\u003e carsDF \u003d spark.createDataFrame(cars, schema);\n    \tcarsDF.show();\n        \n        //Load data into a table using Parquet file\n        System.out.println(\"start create table ...\");\n    \tsplicemachineContext.createTable(\"test.car\", schema, \n    \t\t\tJavaConverters.asScalaIteratorConverter(Arrays.asList(\"PRIMARY KEY (SERIAL)\").iterator()).asScala().toSeq(), \"\");\n    \tSystem.out.println(\"done create table ... \");\n    \t\n    \tStructType outputSchema \u003d splicemachineContext.getSchema(\"test.car\");\n    \toutputSchema.printTreeString();\n    \t\n    \tSystem.out.println(\"start insert ...\");\n    \tsplicemachineContext.insert(carsDF, \"test.car\");\n    \tSystem.out.println(\"done insert ... \");\n    \t\n    \tSystem.out.println(\"start select ...\");\n    \tDataset\u0026lt;Row\u0026gt; selectedCarsDF \u003d splicemachineContext.df(\"select * from test.car\");\n    \tSystem.out.println(\"done select ...\");\n    \t\n    \tselectedCarsDF.count();\n    \tselectedCarsDF.show();\n    }\n}\n```",
      "user": "anonymous",
      "dateUpdated": "2019-06-18 17:41:32.764",
      "config": {
        "editorHide": true,
        "enabled": true,
        "results": {},
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eLoading Data with SpliceMachineContext\u003c/h2\u003e\n\u003cp\u003eYou can use the \u003ccode\u003eSpliceMachineContext\u003c/code\u003e to load data into a table; in the following example, we read from a parquet file and load the\u003cbr/\u003edata into Splice Machine table.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport com.splicemachine.derby.impl.SpliceSpark;\nimport com.splicemachine.spark.splicemachine.SplicemachineContext;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n....\n\npublic class TestSpliceMachineContext\n{\n    public static void main(String[] args) throws Exception \n    {\n        String dbUrl \u003d \u0026quot;jdbc:splice://SPLICESERVERHOST:1527/splicedb;user\u003d\u0026amp;lt;yourUserId\u0026amp;gt;;password\u003d\u0026amp;lt;yourPassword\u0026amp;gt;\u0026quot;;\n        SparkConf conf \u003d new SparkConf();\n        SparkSession spark \u003d SparkSession.builder().appName(\u0026quot;NativeSparktutor\u0026quot;).config(conf).getOrCreate();\n        SpliceSpark.setContext(spark.sparkContext());\n        SplicemachineContext splicemachineContext \u003d new SplicemachineContext(dbUrl);\n        \n        //create sample dataset\n        List\u0026amp;lt;Row\u0026amp;gt; cars \u003d new ArrayList\u0026amp;lt;Row\u0026amp;gt;();\n    \tStructType schema \u003d DataTypes.createStructType(new StructField[] {\n                DataTypes.createStructField(\u0026quot;SERIAL\u0026quot;,  DataTypes.IntegerType, true),\n                DataTypes.createStructField(\u0026quot;MAKE\u0026quot;, DataTypes.StringType, true),\n                DataTypes.createStructField(\u0026quot;MODEL\u0026quot;, DataTypes.StringType, true)\n        });\n    \t\n    \tcars.add(RowFactory.create(1, \u0026quot;Toyota\u0026quot;, \u0026quot;Camry\u0026quot;));\n    \tcars.add(RowFactory.create(2, \u0026quot;Honda\u0026quot;, \u0026quot;Accord\u0026quot;));\n    \tcars.add(RowFactory.create(3, \u0026quot;Subaru\u0026quot;, \u0026quot;Impreza\u0026quot;));\n    \tcars.add(RowFactory.create(4, \u0026quot;Chevy\u0026quot;, \u0026quot;Volt\u0026quot;));\n    \t\n    \t\n    \tDataset\u0026lt;Row\u0026gt; carsDF \u003d spark.createDataFrame(cars, schema);\n    \tcarsDF.show();\n        \n        //Load data into a table using Parquet file\n        System.out.println(\u0026quot;start create table ...\u0026quot;);\n    \tsplicemachineContext.createTable(\u0026quot;test.car\u0026quot;, schema, \n    \t\t\tJavaConverters.asScalaIteratorConverter(Arrays.asList(\u0026quot;PRIMARY KEY (SERIAL)\u0026quot;).iterator()).asScala().toSeq(), \u0026quot;\u0026quot;);\n    \tSystem.out.println(\u0026quot;done create table ... \u0026quot;);\n    \t\n    \tStructType outputSchema \u003d splicemachineContext.getSchema(\u0026quot;test.car\u0026quot;);\n    \toutputSchema.printTreeString();\n    \t\n    \tSystem.out.println(\u0026quot;start insert ...\u0026quot;);\n    \tsplicemachineContext.insert(carsDF, \u0026quot;test.car\u0026quot;);\n    \tSystem.out.println(\u0026quot;done insert ... \u0026quot;);\n    \t\n    \tSystem.out.println(\u0026quot;start select ...\u0026quot;);\n    \tDataset\u0026amp;lt;Row\u0026amp;gt; selectedCarsDF \u003d splicemachineContext.df(\u0026quot;select * from test.car\u0026quot;);\n    \tSystem.out.println(\u0026quot;done select ...\u0026quot;);\n    \t\n    \tselectedCarsDF.count();\n    \tselectedCarsDF.show();\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559579990934_-1817260673",
      "id": "20190603-163950_1385722937",
      "dateCreated": "2019-06-03 16:39:50.934",
      "dateStarted": "2019-06-18 17:41:32.764",
      "dateFinished": "2019-06-18 17:41:32.786",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Internal Access While Using SpliceMachineContext\n\nBy default, Native Spark DataSource queries execute in the Spark application, which is highly performant and allows access to almost all Splice Machine features. However, when your Native Spark DataSource application uses our Access Control List (ACL) feature, there is a restriction with regard to checking permissions.\n\nThe specific problem is that the Native Spark DataSource does not have the ability to check permissions at the view level or column level; instead, it checks permissions on the base table. This means that if your Native Spark DataSource application doesn’t have access to the table underlying a view or column, it will not have access to that view or column; as a result, a query against the view or colunn fails and throws an exception.\n\nThe workaround for this problem is to tell the Native Spark DataSource to use internal access to the database; this enables view/column permission checking, at a very slight cost in performance. With internal access, the adapter runs queries in Splice Machine and temporarily persists data in HDFS while running the query.\n\nThe ACL feature is enabled by setting `splice.authentication.token.enabled\u003dtrue`.\n\nIn addition, to use the Splice Native Spark DataSource, a user must have `execute` permission on the following four system procedures:\n\n* `SYSCS_HBASE_OPERATION`\n* `SYSCS_HDFS_OPERATION`\n* `SYSCS_GET_SPLICE_TOKEN`\n* `SYSCS_CANCEL_SPLICE_TOKEN`\n\n\u003cp class\u003d\"noteIcon\"\u003eThese procedures are all Splice Machine system procedures that are used internally to efficiently perform direct HBASE and HDFS operations. They \u003cem\u003eare not documented\u003c/em\u003e because they are intended only for use by the Splice Machine code itself; however, the Native Spark DataSource uses these procedures, so any user of the Adapter must have permission to execute them.\u003c/p\u003e\n\nHere\u0027s an example of granting `execute` permission to these procedures to a user named `myUserName`:\n\n```\nsplice\u003e grant execute on procedure SYSCS_UTIL.SYSCS_HBASE_OPERATION to myUserName;\n0 rows inserted/updated/deleted\nsplice\u003e grant execute on procedure SYSCS_UTIL.SYSCS_HDFS_OPERATION to myUserName;\n0 rows inserted/updated/deleted\nsplice\u003e grant execute on procedure SYSCS_UTIL.SYSCS_GET_SPLICE_TOKEN to myUserName;\n0 rows inserted/updated/deleted\nsplice\u003e grant execute on procedure SYSCS_UTIL.SYSCS_CANCEL_SPLICE_TOKEN to myUserName;\n0 rows inserted/updated/deleted\n```\n\nHere is an example to use internalDF instead of Spark DF:\n\n\u003cpre\u003e\u003ccode\u003e\nimport com.splicemachine.derby.impl.SpliceSpark;\nimport com.splicemachine.spark.splicemachine.SplicemachineContext;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n....\n\npublic class TestSpliceMachineContext\n{\n    public static void main(String[] args) throws Exception \n    {\n        String dbUrl \u003d \"jdbc:splice://SPLICESERVERHOST:1527/splicedb;user\u003d\u0026lt;yourUserId\u0026gt;;password\u003d\u0026lt;yourPassword\u0026gt;\";\n        SparkConf conf \u003d new SparkConf();\n        SparkSession spark \u003d SparkSession.builder().appName(\"NativeSparktutor\").config(conf).getOrCreate();\n        SpliceSpark.setContext(spark.sparkContext());\n        SplicemachineContext splicemachineContext \u003d new SplicemachineContext(dbUrl);\n        \n        //Query table, use df operations based out of SpliceMachineContext\n        String spliceQuery \u003d \"select count(*) from \" + \u0026lt;TABLE_NAME\u0026gt;;\n        splicemachineContext.internalDf(spliceQuery).show();\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e",
      "user": "anonymous",
      "dateUpdated": "2019-06-18 17:42:22.600",
      "config": {
        "editorHide": true,
        "enabled": true,
        "results": {},
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eInternal Access While Using SpliceMachineContext\u003c/h2\u003e\n\u003cp\u003eBy default, Native Spark DataSource queries execute in the Spark application, which is highly performant and allows access to almost all Splice Machine features. However, when your Native Spark DataSource application uses our Access Control List (ACL) feature, there is a restriction with regard to checking permissions.\u003c/p\u003e\n\u003cp\u003eThe specific problem is that the Native Spark DataSource does not have the ability to check permissions at the view level or column level; instead, it checks permissions on the base table. This means that if your Native Spark DataSource application doesn’t have access to the table underlying a view or column, it will not have access to that view or column; as a result, a query against the view or colunn fails and throws an exception.\u003c/p\u003e\n\u003cp\u003eThe workaround for this problem is to tell the Native Spark DataSource to use internal access to the database; this enables view/column permission checking, at a very slight cost in performance. With internal access, the adapter runs queries in Splice Machine and temporarily persists data in HDFS while running the query.\u003c/p\u003e\n\u003cp\u003eThe ACL feature is enabled by setting \u003ccode\u003esplice.authentication.token.enabled\u003dtrue\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIn addition, to use the Splice Native Spark DataSource, a user must have \u003ccode\u003eexecute\u003c/code\u003e permission on the following four system procedures:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\u003ccode\u003eSYSCS_HBASE_OPERATION\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ccode\u003eSYSCS_HDFS_OPERATION\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ccode\u003eSYSCS_GET_SPLICE_TOKEN\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ccode\u003eSYSCS_CANCEL_SPLICE_TOKEN\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp class\u003d\"noteIcon\"\u003eThese procedures are all Splice Machine system procedures that are used internally to efficiently perform direct HBASE and HDFS operations. They \u003cem\u003eare not documented\u003c/em\u003e because they are intended only for use by the Splice Machine code itself; however, the Native Spark DataSource uses these procedures, so any user of the Adapter must have permission to execute them.\u003c/p\u003e\n\u003cp\u003eHere\u0026rsquo;s an example of granting \u003ccode\u003eexecute\u003c/code\u003e permission to these procedures to a user named \u003ccode\u003emyUserName\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esplice\u0026gt; grant execute on procedure SYSCS_UTIL.SYSCS_HBASE_OPERATION to myUserName;\n0 rows inserted/updated/deleted\nsplice\u0026gt; grant execute on procedure SYSCS_UTIL.SYSCS_HDFS_OPERATION to myUserName;\n0 rows inserted/updated/deleted\nsplice\u0026gt; grant execute on procedure SYSCS_UTIL.SYSCS_GET_SPLICE_TOKEN to myUserName;\n0 rows inserted/updated/deleted\nsplice\u0026gt; grant execute on procedure SYSCS_UTIL.SYSCS_CANCEL_SPLICE_TOKEN to myUserName;\n0 rows inserted/updated/deleted\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere is an example to use internalDF instead of Spark DF:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\nimport com.splicemachine.derby.impl.SpliceSpark;\nimport com.splicemachine.spark.splicemachine.SplicemachineContext;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n....\n\npublic class TestSpliceMachineContext\n{\n    public static void main(String[] args) throws Exception \n    {\n        String dbUrl \u003d \"jdbc:splice://SPLICESERVERHOST:1527/splicedb;user\u003d\u0026lt;yourUserId\u0026gt;;password\u003d\u0026lt;yourPassword\u0026gt;\";\n        SparkConf conf \u003d new SparkConf();\n        SparkSession spark \u003d SparkSession.builder().appName(\"NativeSparktutor\").config(conf).getOrCreate();\n        SpliceSpark.setContext(spark.sparkContext());\n        SplicemachineContext splicemachineContext \u003d new SplicemachineContext(dbUrl);\n        \n        //Query table, use df operations based out of SpliceMachineContext\n        String spliceQuery \u003d \"select count(*) from \" + \u0026lt;TABLE_NAME\u0026gt;;\n        splicemachineContext.internalDf(spliceQuery).show();\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559580591238_-232357716",
      "id": "20190603-164951_106071028",
      "dateCreated": "2019-06-03 16:49:51.238",
      "dateStarted": "2019-06-18 17:42:22.600",
      "dateFinished": "2019-06-18 17:42:22.631",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Build and Deploy The Application\n\nYou can build the Jar (we suggest building an uber jar) using `maven`, similar to how you build and deploy a typical Spark application.\n\n```\nmvn clean package\n```\n\nOnce the jar is built (typically in the `target` folder within the project folder), you can copy it over to the edge node or region server on the cluster to execute with the spark-submit command.\n\nNote that if you are using the Docker image, you can copy the jar over to the running instance with the following command:\n\n```\ndocker cp \u003cPATH_ON_HOST_MACHINE\u003e spliceserver:\u003cPATH_TO_TARGET_FOLDER_ON_RUNNING_INSTANCE\u003e\n```\n",
      "user": "anonymous",
      "dateUpdated": "2019-06-18 17:43:10.775",
      "config": {
        "editorHide": true,
        "enabled": true,
        "results": {},
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eBuild and Deploy The Application\u003c/h2\u003e\n\u003cp\u003eYou can build the Jar (we suggest building an uber jar) using \u003ccode\u003emaven\u003c/code\u003e, similar to how you build and deploy a typical Spark application.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emvn clean package\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce the jar is built (typically in the \u003ccode\u003etarget\u003c/code\u003e folder within the project folder), you can copy it over to the edge node or region server on the cluster to execute with the spark-submit command.\u003c/p\u003e\n\u003cp\u003eNote that if you are using the Docker image, you can copy the jar over to the running instance with the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker cp \u0026lt;PATH_ON_HOST_MACHINE\u0026gt; spliceserver:\u0026lt;PATH_TO_TARGET_FOLDER_ON_RUNNING_INSTANCE\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559581897072_927636269",
      "id": "20190603-171137_1488177463",
      "dateCreated": "2019-06-03 17:11:37.073",
      "dateStarted": "2019-06-18 17:43:10.776",
      "dateFinished": "2019-06-18 17:43:10.802",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Using spark submit to Launch Spark Adapter Applications\n\nYou can launch your application with the following command on the provided docker image:\n\n```\nspark-submit --conf \"spark.dynamicAllocation.enabled\u003dfalse\" --conf \"spark.task.maxFailures\u003d2\" --conf \"spark.driver.memory\u003d512m\" --conf \"spark.driver.cores\u003d1\" --conf \"spark.kryoserializer.buffer\u003d512\" --conf \"spark.kryoserializer.buffer.max\u003d1024\" --conf \"spark.io.compression.codec\u003dorg.apache.spark.io.SnappyCompressionCodec\" --conf \"spark.executor.extraClassPath\u003d/opt/hadoop/conf/:/opt/spark/jars/*\" --conf \"spark.driver.extraClassPath\u003d/opt/hadoop/conf/:/opt/spark/jars/*\" --class \u0026lt;FULLY_QUALIFIED_EXECUTABLE_CLASS_NAME\u0026gt; --master local[1] --deploy-mode client --num-executors 1 --executor-memory 1G --executor-cores 1 \u0026lt;JAR_FILE_NAME\u0026gt; localhost 1527 splice admin\n```\n\nNote that  various executor settings in the above example are just illustrative and need to be be configured for your job and its execution environment.\n",
      "user": "anonymous",
      "dateUpdated": "2019-06-18 17:44:50.810",
      "config": {
        "editorHide": true,
        "enabled": true,
        "results": {},
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eUsing spark submit to Launch Spark Adapter Applications\u003c/h2\u003e\n\u003cp\u003eYou can launch your application with the following command on the provided docker image:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003espark-submit --conf \u0026quot;spark.dynamicAllocation.enabled\u003dfalse\u0026quot; --conf \u0026quot;spark.task.maxFailures\u003d2\u0026quot; --conf \u0026quot;spark.driver.memory\u003d512m\u0026quot; --conf \u0026quot;spark.driver.cores\u003d1\u0026quot; --conf \u0026quot;spark.kryoserializer.buffer\u003d512\u0026quot; --conf \u0026quot;spark.kryoserializer.buffer.max\u003d1024\u0026quot; --conf \u0026quot;spark.io.compression.codec\u003dorg.apache.spark.io.SnappyCompressionCodec\u0026quot; --conf \u0026quot;spark.executor.extraClassPath\u003d/opt/hadoop/conf/:/opt/spark/jars/*\u0026quot; --conf \u0026quot;spark.driver.extraClassPath\u003d/opt/hadoop/conf/:/opt/spark/jars/*\u0026quot; --class \u0026amp;lt;FULLY_QUALIFIED_EXECUTABLE_CLASS_NAME\u0026amp;gt; --master local[1] --deploy-mode client --num-executors 1 --executor-memory 1G --executor-cores 1 \u0026amp;lt;JAR_FILE_NAME\u0026amp;gt; localhost 1527 splice admin\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNote that various executor settings in the above example are just illustrative and need to be be configured for your job and its execution environment.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559669789849_-819139079",
      "id": "20190604-173629_1491743944",
      "dateCreated": "2019-06-04 17:36:29.849",
      "dateStarted": "2019-06-18 17:44:50.811",
      "dateFinished": "2019-06-18 17:44:50.823",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## For Further Exploration and Understanding\n\nTo gain a better understanding of using statistics, try using `spark-submit` by building a JAR file and running these exercises:\n\n1. Use Spark Adapter to create a set of tables where you can exercise some queries including join operations.\n\n2. Use the tables in Step-1 to load data into tables with Spark Adapter. Use both DF and internalDF to do these operations.\n\n3. Analyze the table using Spark Adapter\n\n4. Query the table and verify data load. \n\n5. Use a Join operation via Spark Adapter\n\n6. Drop the table(s)\n",
      "user": "anonymous",
      "dateUpdated": "2019-06-18 17:45:15.074",
      "config": {
        "editorHide": true,
        "enabled": true,
        "results": {},
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eFor Further Exploration and Understanding\u003c/h2\u003e\n\u003cp\u003eTo gain a better understanding of using statistics, try using \u003ccode\u003espark-submit\u003c/code\u003e by building a JAR file and running these exercises:\u003c/p\u003e\n\u003col\u003e\n  \u003cli\u003e\n  \u003cp\u003eUse Spark Adapter to create a set of tables where you can exercise some queries including join operations.\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eUse the tables in Step-1 to load data into tables with Spark Adapter. Use both DF and internalDF to do these operations.\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eAnalyze the table using Spark Adapter\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eQuery the table and verify data load. \u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eUse a Join operation via Spark Adapter\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eDrop the table(s)\u003c/p\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559773713057_-717419114",
      "id": "20190605-222833_1832541911",
      "dateCreated": "2019-06-05 22:28:33.058",
      "dateStarted": "2019-06-18 17:45:15.075",
      "dateFinished": "2019-06-18 17:45:15.095",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Where to Go Next\nTo finish this class, please complete the exercises in the \u003ca href\u003d\"/#/notebook/2EDNZACZA\"\u003e\u003cem\u003eExercises for This Class\u003c/em\u003e\u003c/a\u003e notebook, which test your understanding of the material we\u0027ve covered.\u003c/td\u003e\n",
      "user": "anonymous",
      "dateUpdated": "2019-06-05 23:23:49.734",
      "config": {
        "editorHide": true,
        "enabled": false,
        "results": {},
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "msg": [
          {
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eWhere to Go Next\u003c/h2\u003e\n\u003cp\u003eTo finish this class, please complete the exercises in the \u003ca href\u003d\"/#/notebook/2EDNZACZA\"\u003e\u003cem\u003eExercises for This Class\u003c/em\u003e\u003c/a\u003e notebook, which test your understanding of the material we\u0026rsquo;ve covered.\u003c/td\u003e\u003c/p\u003e\n\u003c/div\u003e",
            "type": "HTML"
          }
        ],
        "code": "SUCCESS"
      },
      "apps": [],
      "jobName": "paragraph_1559325060276_-1719865439",
      "id": "20190531-175100_779787345",
      "dateCreated": "2019-05-31 17:51:00.276",
      "dateStarted": "2019-06-05 23:23:49.736",
      "dateFinished": "2019-06-05 23:23:49.745",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Splice Machine Training /Advanced Developer/i. Using spark-submit with Native Spark Datasource",
  "id": "2EECKNU39",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "splicemachine:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}