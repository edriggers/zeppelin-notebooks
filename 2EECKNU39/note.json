{
  "paragraphs": [
    {
      "text": "%md\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n# Using spark-submit With the Splice Machine Native Spark DataSource\n\nThe Splice Machine Native Spark DataSource, which is also referred to as the Spark Adapter, allows you to directly connect Spark DataFrames and Splice Machine database tables, bypassing the need to send your data to/from Spark over a JDBC or ODBC connection. You can efficiently insert, upsert, select, update, and delete data in your Splice Machine tables directly from Spark in a transactionally consistent manner. With the Spark Adapter, transfers of data between Spark and your database are completed without serialization/deserialization, which generates tremendous performance boosts over traditional over-the-wire (sequentially over a JDBC/ODBC connection) transfers.\n\nTo use the Spark Adapter in your code, you simply instantiate a SplicemachineContext object in your Spark code. You can run Spark applications that interface with your Splice Machine database interactively in the Spark shell, in Zeppelin notebooks, or by using our Spark Submit script. One common use of the Adapter is to ingest data into your Splice Machine database directly from a Spark DataFrame.\n\nThe Native DataSource allows data scientists to bypass the limitations of the SQL-based JDBC interface in favor of the more scalable and powerful Spark DataFrame API, making it possible for them to operate on data at scale and ingest real-time streaming data with outstanding performance. You can craft applications that use Spark and our Native Spark DataSource in Scala, Python, and Java. Note that you can use the Native Spark DataSource in the Splice Machine ML Manager and Zeppelin Notebook interfaces.\n\n",
      "user": "anonymous",
      "dateUpdated": "2019-06-02 20:22:54.382",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\u003ch1\u003eUsing spark-submit With the Splice Machine Native Spark DataSource\u003c/h1\u003e\n\u003cp\u003eThe Splice Machine Native Spark DataSource, which is also referred to as the Spark Adapter, allows you to directly connect Spark DataFrames and Splice Machine database tables, bypassing the need to send your data to/from Spark over a JDBC or ODBC connection. You can efficiently insert, upsert, select, update, and delete data in your Splice Machine tables directly from Spark in a transactionally consistent manner. With the Spark Adapter, transfers of data between Spark and your database are completed without serialization/deserialization, which generates tremendous performance boosts over traditional over-the-wire (sequentially over a JDBC/ODBC connection) transfers.\u003c/p\u003e\n\u003cp\u003eTo use the Spark Adapter in your code, you simply instantiate a SplicemachineContext object in your Spark code. You can run Spark applications that interface with your Splice Machine database interactively in the Spark shell, in Zeppelin notebooks, or by using our Spark Submit script. One common use of the Adapter is to ingest data into your Splice Machine database directly from a Spark DataFrame.\u003c/p\u003e\n\u003cp\u003eThe Native DataSource allows data scientists to bypass the limitations of the SQL-based JDBC interface in favor of the more scalable and powerful Spark DataFrame API, making it possible for them to operate on data at scale and ingest real-time streaming data with outstanding performance. You can craft applications that use Spark and our Native Spark DataSource in Scala, Python, and Java. Note that you can use the Native Spark DataSource in the Splice Machine ML Manager and Zeppelin Notebook interfaces.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559319019968_-1240214828",
      "id": "20190531-161019_1313997856",
      "dateCreated": "2019-05-31 16:10:19.968",
      "dateStarted": "2019-06-02 20:22:54.382",
      "dateFinished": "2019-06-02 20:22:54.413",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n# Why Use the Native Data Source?\n\nThe primary reason for using the Native DataSource is that it provides dramatic performance improvements for large scale data operations; this is because the DataSource works directly on native DataFrames and RDDs, thus eliminating the need to serialize data. Spark is optimized to work on DataFrames, which is a distributed collection of data (an RDD) organized into named columns, with a schema that specifies data types, that is designed to support efficiently operating on scalable, massive datasets.\n\nThe Splice Machine DataSource is native to Spark, which means that it operates directly on these DataFrames and in the same Spark executors that your programs are using to analyze or transform the data. Instead of accessing, inserting, or manipulating data one record at a time over a serialized connection, you can use the Splice Machine Native Spark DataSource to pull the contents of an entire DataFrame into your database, and to pull database query results into a DataFrame.\n\nSplice Machine has observed 100x performance increases compared to using JDBC for operations such as inserting millions of records in a database! For example, a typical web application might use a JDBC connection to query the database, pulling information out one record at a time to populate the screen. The results of each query are serialized (turned into a string of data), then sent over a network connection to the app, and then displayed on the customer’s screen.\n\nIn contrast, when you use the Splice Machine Native Spark DataSource, the contents of the database table are typically sitting in a DataFrame in memory that resides on the same Spark executor that’s performing the query. The query takes place in memory, and there’s no need to serialize or stream the results over a wire. Similarly, when the app sends updates to the database, the data is inserted into the database from in-memory DataFrames directly to the tables without serialization. As a result, a great deal of overhead is eliminated, and performance gains can be remarkable.\n\n",
      "user": "anonymous",
      "dateUpdated": "2019-06-02 20:58:21.807",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\u003ch1\u003eWhy Use the Native Data Source?\u003c/h1\u003e\n\u003cp\u003eThe primary reason for using the Native DataSource is that it provides dramatic performance improvements for large scale data operations; this is because the DataSource works directly on native DataFrames and RDDs, thus eliminating the need to serialize data. Spark is optimized to work on DataFrames, which is a distributed collection of data (an RDD) organized into named columns, with a schema that specifies data types, that is designed to support efficiently operating on scalable, massive datasets.\u003c/p\u003e\n\u003cp\u003eThe Splice Machine DataSource is native to Spark, which means that it operates directly on these DataFrames and in the same Spark executors that your programs are using to analyze or transform the data. Instead of accessing, inserting, or manipulating data one record at a time over a serialized connection, you can use the Splice Machine Native Spark DataSource to pull the contents of an entire DataFrame into your database, and to pull database query results into a DataFrame.\u003c/p\u003e\n\u003cp\u003eSplice Machine has observed 100x performance increases compared to using JDBC for operations such as inserting millions of records in a database! For example, a typical web application might use a JDBC connection to query the database, pulling information out one record at a time to populate the screen. The results of each query are serialized (turned into a string of data), then sent over a network connection to the app, and then displayed on the customer’s screen.\u003c/p\u003e\n\u003cp\u003eIn contrast, when you use the Splice Machine Native Spark DataSource, the contents of the database table are typically sitting in a DataFrame in memory that resides on the same Spark executor that’s performing the query. The query takes place in memory, and there’s no need to serialize or stream the results over a wire. Similarly, when the app sends updates to the database, the data is inserted into the database from in-memory DataFrames directly to the tables without serialization. As a result, a great deal of overhead is eliminated, and performance gains can be remarkable.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559506884945_1945185413",
      "id": "20190602-202124_1993675938",
      "dateCreated": "2019-06-02 20:21:24.945",
      "dateStarted": "2019-06-02 20:58:21.807",
      "dateFinished": "2019-06-02 20:58:21.826",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n# SpliceMachineContext Class\n\nThe first thing you need to do when using the Native Spark DataSource is to create an instance of the SplicemachineContext class; this is the primary serializable class that you can broadcast in your Spark applications. This class interacts with your Splice Machine cluster in your Spark executors, and provides the methods that you can use to perform operations such as:\n\n#### Interfacing with Splice Machine RDD\n#### Running inserts, updates and deletes on data\n#### Converting data types between Splice Machine and Spark",
      "user": "anonymous",
      "dateUpdated": "2019-06-02 20:58:32.602",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\u003ch1\u003eSpliceMachineContext Class\u003c/h1\u003e\n\u003cp\u003eThe first thing you need to do when using the Native Spark DataSource is to create an instance of the SplicemachineContext class; this is the primary serializable class that you can broadcast in your Spark applications. This class interacts with your Splice Machine cluster in your Spark executors, and provides the methods that you can use to perform operations such as:\u003c/p\u003e\n\u003ch4\u003eInterfacing with Splice Machine RDD\u003c/h4\u003e\n\u003ch4\u003eRunning inserts, updates and deletes on data\u003c/h4\u003e\n\u003ch4\u003eConverting data types between Splice Machine and Spark\u003c/h4\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559508964492_919362593",
      "id": "20190602-205604_247573122",
      "dateCreated": "2019-06-02 20:56:04.492",
      "dateStarted": "2019-06-02 20:58:32.603",
      "dateFinished": "2019-06-02 20:58:32.615",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n# Setup Maven Coordinates\n\nThe following pom.xml file shows dependency resolution setup for various versions of Cloudera and Hortonwork based distribution. You can use this as a template for maven setup.\n~~~html\n\u003c?xml version\u003d\"1.0\" encoding\u003d\"UTF-8\"?\u003e\n\u003cproject xmlns\u003d\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi\u003d\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation\u003d\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"\u003e\n    \u003cmodelVersion\u003e4.0.0\u003c/modelVersion\u003e\n    \u003cgroupId\u003ecom.splicemachine.tutorial\u003c/groupId\u003e\n    \u003cartifactId\u003espark_adapter_example\u003c/artifactId\u003e\n    \u003cversion\u003e1.0-SNAPSHOT\u003c/version\u003e\n    \u003cbuild\u003e\n        \u003cplugins\u003e\n            \u003cplugin\u003e\n                \u003cgroupId\u003eorg.apache.maven.plugins\u003c/groupId\u003e\n                \u003cartifactId\u003emaven-compiler-plugin\u003c/artifactId\u003e\n                \u003cconfiguration\u003e\n                    \u003csource\u003e7\u003c/source\u003e\n                    \u003ctarget\u003e7\u003c/target\u003e\n                \u003c/configuration\u003e\n            \u003c/plugin\u003e\n        \u003c/plugins\u003e\n    \u003c/build\u003e\n    \u003cproperties\u003e\n        \u003cspliceVersion\u003e2.7.0.1907\u003c/spliceVersion\u003e\n    \u003c/properties\u003e\n    \u003crepositories\u003e\n        \u003crepository\u003e\n            \u003cid\u003ecloudera\u003c/id\u003e\n            \u003curl\u003ehttps://repository.cloudera.com/artifactory/cloudera-repos/\u003c/url\u003e\n        \u003c/repository\u003e\n        \u003crepository\u003e\n            \u003cid\u003ehortonworks-releases\u003c/id\u003e\n            \u003curl\u003ehttp://repo.hortonworks.com/content/repositories/releases/\u003c/url\u003e\n        \u003c/repository\u003e\n        \u003crepository\u003e\n            \u003cid\u003ehortonworks-hadoop\u003c/id\u003e\n            \u003curl\u003ehttp://repo.hortonworks.com/content/repositories/jetty-hadoop/\u003c/url\u003e\n        \u003c/repository\u003e\n        \u003crepository\u003e\n            \u003cid\u003esplicemachine-public\u003c/id\u003e\n            \u003curl\u003ehttp://repository.splicemachine.com/nexus/content/groups/public\u003c/url\u003e\n        \u003c/repository\u003e\n    \u003c/repositories\u003e\n    \u003cdependencies\u003e\n        \u003cdependency\u003e\n            \u003cgroupId\u003ecom.splicemachine\u003c/groupId\u003e\n            \u003cartifactId\u003esplice_machine\u003c/artifactId\u003e\n            \u003cversion\u003e${spliceVersion}\u003c/version\u003e\n        \u003c/dependency\u003e\n        \u003cdependency\u003e\n            \u003cgroupId\u003ecom.splicemachine\u003c/groupId\u003e\n            \u003cartifactId\u003edb-engine\u003c/artifactId\u003e\n            \u003cversion\u003e${spliceVersion}\u003c/version\u003e\n        \u003c/dependency\u003e\n        \u003cdependency\u003e\n            \u003cgroupId\u003ecom.splicemachine\u003c/groupId\u003e\n            \u003cartifactId\u003edb-client\u003c/artifactId\u003e\n            \u003cversion\u003e2.5.0.1820\u003c/version\u003e\n        \u003c/dependency\u003e\n    \u003c/dependencies\u003e\n    \u003cprofiles\u003e\n        \u003cprofile\u003e\n            \u003cid\u003ecdh5.16.1\u003c/id\u003e\n            \u003cactivation\u003e\n                \u003cproperty\u003e\n                    \u003cname\u003eenv\u003c/name\u003e\n                    \u003cvalue\u003ecdh5.16.1\u003c/value\u003e\n                \u003c/property\u003e\n            \u003c/activation\u003e\n            \u003cbuild\u003e\n                \u003cplugins\u003e\n                    \u003cplugin\u003e\n                        \u003cartifactId\u003emaven-dependency-plugin\u003c/artifactId\u003e\n                        \u003cexecutions\u003e\n                            \u003cexecution\u003e\n                                \u003cphase\u003einstall\u003c/phase\u003e\n                                \u003cgoals\u003e\n                                    \u003cgoal\u003ecopy-dependencies\u003c/goal\u003e\n                                \u003c/goals\u003e\n                                \u003cconfiguration\u003e\n                                    \u003coutputDirectory\u003e${project.build.directory}/lib\u003c/outputDirectory\u003e\n                                    \u003cincludeArtifactIds\u003esplicemachine-cdh5.16.1-2.2.0.cloudera2_2.11\u003c/includeArtifactIds\u003e\n                                \u003c/configuration\u003e\n                            \u003c/execution\u003e\n                        \u003c/executions\u003e\n                    \u003c/plugin\u003e\n                \u003c/plugins\u003e\n            \u003c/build\u003e\n            \u003cdependencies\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003eorg.apache.spark\u003c/groupId\u003e\n                    \u003cartifactId\u003espark-core_2.11\u003c/artifactId\u003e\n                    \u003cversion\u003e2.2.0.cloudera2\u003c/version\u003e\n                    \u003cscope\u003eprovided\u003c/scope\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003eorg.apache.spark\u003c/groupId\u003e\n                    \u003cartifactId\u003espark-sql_2.11\u003c/artifactId\u003e\n                    \u003cversion\u003e2.2.0.cloudera2\u003c/version\u003e\n                    \u003cscope\u003eprovided\u003c/scope\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003ecom.splicemachine\u003c/groupId\u003e\n                    \u003cartifactId\u003esplicemachine-cdh5.16.1-2.2.0.cloudera2_2.11\u003c/artifactId\u003e\n                    \u003cversion\u003e${spliceVersion}\u003c/version\u003e\n                    \u003cscope\u003eprovided\u003c/scope\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003ecom.splicemachine\u003c/groupId\u003e\n                    \u003cartifactId\u003ehbase_sql-cdh5.16.1\u003c/artifactId\u003e\n                    \u003cversion\u003e${spliceVersion}\u003c/version\u003e\n                    \u003cscope\u003eprovided\u003c/scope\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003eorg.apache.hbase\u003c/groupId\u003e\n                    \u003cartifactId\u003ehbase-client\u003c/artifactId\u003e\n                    \u003cversion\u003e1.2.0-cdh5.16.1\u003c/version\u003e\n                    \u003cscope\u003eprovided\u003c/scope\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003ecom.splicemachine\u003c/groupId\u003e\n                    \u003cartifactId\u003ehbase_storage-cdh5.16.1\u003c/artifactId\u003e\n                    \u003cversion\u003e${spliceVersion}\u003c/version\u003e\n                    \u003cscope\u003eprovided\u003c/scope\u003e\n                \u003c/dependency\u003e\n            \u003c/dependencies\u003e\n        \u003c/profile\u003e\n        \u003cprofile\u003e\n            \u003cid\u003ecdh5.12.2\u003c/id\u003e\n            \u003cactivation\u003e\n                \u003cproperty\u003e\n                    \u003cname\u003eenv\u003c/name\u003e\n                    \u003cvalue\u003ecdh5.12.2\u003c/value\u003e\n                \u003c/property\u003e\n            \u003c/activation\u003e\n            \u003cbuild\u003e\n                \u003cplugins\u003e\n                    \u003cplugin\u003e\n                        \u003cartifactId\u003emaven-dependency-plugin\u003c/artifactId\u003e\n                        \u003cexecutions\u003e\n                            \u003cexecution\u003e\n                                \u003cphase\u003einstall\u003c/phase\u003e\n                                \u003cgoals\u003e\n                                    \u003cgoal\u003ecopy-dependencies\u003c/goal\u003e\n                                \u003c/goals\u003e\n                                \u003cconfiguration\u003e\n                                    \u003coutputDirectory\u003e${project.build.directory}/lib\u003c/outputDirectory\u003e\n                                    \u003cincludeArtifactIds\u003esplicemachine-cdh5.12.2-2.2.0.cloudera2_2.11\u003c/includeArtifactIds\u003e\n                                \u003c/configuration\u003e\n                            \u003c/execution\u003e\n                        \u003c/executions\u003e\n                    \u003c/plugin\u003e\n                \u003c/plugins\u003e\n            \u003c/build\u003e\n            \u003cdependencies\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003eorg.apache.spark\u003c/groupId\u003e\n                    \u003cartifactId\u003espark-core_2.11\u003c/artifactId\u003e\n                    \u003cversion\u003e2.2.0.cloudera2\u003c/version\u003e\n                    \u003cscope\u003eprovided\u003c/scope\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003eorg.apache.spark\u003c/groupId\u003e\n                    \u003cartifactId\u003espark-sql_2.11\u003c/artifactId\u003e\n                    \u003cversion\u003e2.2.0.cloudera2\u003c/version\u003e\n                    \u003cscope\u003eprovided\u003c/scope\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003ecom.splicemachine\u003c/groupId\u003e\n                    \u003cartifactId\u003esplicemachine-cdh5.12.2-2.2.0.cloudera2_2.11\u003c/artifactId\u003e\n                    \u003cversion\u003e${spliceVersion}\u003c/version\u003e\n                    \u003cscope\u003eprovided\u003c/scope\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003ecom.splicemachine\u003c/groupId\u003e\n                    \u003cartifactId\u003ehbase_sql-cdh5.12.2\u003c/artifactId\u003e\n                    \u003cversion\u003e${spliceVersion}\u003c/version\u003e\n                    \u003cscope\u003eprovided\u003c/scope\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003eorg.apache.hbase\u003c/groupId\u003e\n                    \u003cartifactId\u003ehbase-client\u003c/artifactId\u003e\n                    \u003cversion\u003e1.2.0-cdh5.12.2\u003c/version\u003e\n                    \u003cscope\u003eprovided\u003c/scope\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003ecom.splicemachine\u003c/groupId\u003e\n                    \u003cartifactId\u003ehbase_storage-cdh5.12.2\u003c/artifactId\u003e\n                    \u003cversion\u003e${spliceVersion}\u003c/version\u003e\n                    \u003cscope\u003eprovided\u003c/scope\u003e\n                \u003c/dependency\u003e\n            \u003c/dependencies\u003e\n        \u003c/profile\u003e\n        \u003cprofile\u003e\n            \u003cid\u003ecdh5.14.0\u003c/id\u003e\n            \u003cactivation\u003e\n                \u003cproperty\u003e\n                    \u003cname\u003eenv\u003c/name\u003e\n                    \u003cvalue\u003ecdh5.14.0\u003c/value\u003e\n                \u003c/property\u003e\n            \u003c/activation\u003e\n            \u003cbuild\u003e\n                \u003cplugins\u003e\n                    \u003cplugin\u003e\n                        \u003cartifactId\u003emaven-dependency-plugin\u003c/artifactId\u003e\n                        \u003cexecutions\u003e\n                            \u003cexecution\u003e\n                                \u003cphase\u003einstall\u003c/phase\u003e\n                                \u003cgoals\u003e\n                                    \u003cgoal\u003ecopy-dependencies\u003c/goal\u003e\n                                \u003c/goals\u003e\n                                \u003cconfiguration\u003e\n                                    \u003coutputDirectory\u003e${project.build.directory}/lib\u003c/outputDirectory\u003e\n                                    \u003cincludeArtifactIds\u003esplicemachine-cdh5.14.0-2.2.0.cloudera2_2.11\u003c/includeArtifactIds\u003e\n                                \u003c/configuration\u003e\n                            \u003c/execution\u003e\n                        \u003c/executions\u003e\n                    \u003c/plugin\u003e\n                \u003c/plugins\u003e\n            \u003c/build\u003e\n            \u003cdependencies\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003eorg.apache.spark\u003c/groupId\u003e\n                    \u003cartifactId\u003espark-core_2.11\u003c/artifactId\u003e\n                    \u003cversion\u003e2.2.0.cloudera2\u003c/version\u003e\n                    \u003cscope\u003eprovided\u003c/scope\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003eorg.apache.spark\u003c/groupId\u003e\n                    \u003cartifactId\u003espark-sql_2.11\u003c/artifactId\u003e\n                    \u003cversion\u003e2.2.0.cloudera2\u003c/version\u003e\n                    \u003cscope\u003eprovided\u003c/scope\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003ecom.splicemachine\u003c/groupId\u003e\n                    \u003cartifactId\u003esplicemachine-cdh5.14.0-2.2.0.cloudera2_2.11\u003c/artifactId\u003e\n                    \u003cversion\u003e${spliceVersion}\u003c/version\u003e\n                    \u003cscope\u003eprovided\u003c/scope\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003ecom.splicemachine\u003c/groupId\u003e\n                    \u003cartifactId\u003ehbase_sql-cdh5.14.0\u003c/artifactId\u003e\n                    \u003cversion\u003e${spliceVersion}\u003c/version\u003e\n                    \u003cscope\u003eprovided\u003c/scope\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003eorg.apache.hbase\u003c/groupId\u003e\n                    \u003cartifactId\u003ehbase-client\u003c/artifactId\u003e\n                    \u003cversion\u003e1.2.0-cdh5.14.0\u003c/version\u003e\n                    \u003cscope\u003eprovided\u003c/scope\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003ecom.splicemachine\u003c/groupId\u003e\n                    \u003cartifactId\u003ehbase_storage-cdh5.14.0\u003c/artifactId\u003e\n                    \u003cversion\u003e${spliceVersion}\u003c/version\u003e\n                    \u003cscope\u003eprovided\u003c/scope\u003e\n                \u003c/dependency\u003e\n            \u003c/dependencies\u003e\n        \u003c/profile\u003e\n        \u003cprofile\u003e\n            \u003cid\u003ehdp2.6.3\u003c/id\u003e\n            \u003cactivation\u003e\n                \u003cproperty\u003e\n                    \u003cname\u003eenv\u003c/name\u003e\n                    \u003cvalue\u003ehdp2.6.3\u003c/value\u003e\n                \u003c/property\u003e\n            \u003c/activation\u003e\n            \u003cbuild\u003e\n                \u003cplugins\u003e\n                    \u003cplugin\u003e\n                        \u003cartifactId\u003emaven-dependency-plugin\u003c/artifactId\u003e\n                        \u003cexecutions\u003e\n                            \u003cexecution\u003e\n                                \u003cphase\u003einstall\u003c/phase\u003e\n                                \u003cgoals\u003e\n                                    \u003cgoal\u003ecopy-dependencies\u003c/goal\u003e\n                                \u003c/goals\u003e\n                                \u003cconfiguration\u003e\n                                    \u003coutputDirectory\u003e${project.build.directory}/lib\u003c/outputDirectory\u003e\n                                    \u003cincludeArtifactIds\u003esplicemachine-hdp2.6.3-2.2.0.2.6.3.0-235_2.11\u003c/includeArtifactIds\u003e\n                                \u003c/configuration\u003e\n                            \u003c/execution\u003e\n                        \u003c/executions\u003e\n                    \u003c/plugin\u003e\n                \u003c/plugins\u003e\n            \u003c/build\u003e\n            \u003cdependencies\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003eorg.apache.spark\u003c/groupId\u003e\n                    \u003cartifactId\u003espark-core_2.11\u003c/artifactId\u003e\n                    \u003cversion\u003e2.2.0.2.6.3.0-235\u003c/version\u003e\n                    \u003cscope\u003eprovided\u003c/scope\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003eorg.apache.spark\u003c/groupId\u003e\n                    \u003cartifactId\u003espark-sql_2.11\u003c/artifactId\u003e\n                    \u003cversion\u003e2.2.0.2.6.3.0-235\u003c/version\u003e\n                    \u003cscope\u003eprovided\u003c/scope\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003ecom.splicemachine\u003c/groupId\u003e\n                    \u003cartifactId\u003esplicemachine-hdp2.6.3-2.2.0.2.6.3.0-235_2.11\u003c/artifactId\u003e\n                    \u003cversion\u003e${spliceVersion}\u003c/version\u003e\n                    \u003cscope\u003eprovided\u003c/scope\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003ecom.splicemachine\u003c/groupId\u003e\n                    \u003cartifactId\u003ehbase_sql-hdp2.6.3\u003c/artifactId\u003e\n                    \u003cversion\u003e${spliceVersion}\u003c/version\u003e\n                    \u003cscope\u003eprovided\u003c/scope\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003eorg.apache.hbase\u003c/groupId\u003e\n                    \u003cartifactId\u003ehbase-client\u003c/artifactId\u003e\n                    \u003cversion\u003e1.1.2.2.6.3.0-235\u003c/version\u003e\n                    \u003cscope\u003eprovided\u003c/scope\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003ecom.splicemachine\u003c/groupId\u003e\n                    \u003cartifactId\u003ehbase_storage-hdp2.6.3\u003c/artifactId\u003e\n                    \u003cversion\u003e${spliceVersion}\u003c/version\u003e\n                    \u003cscope\u003eprovided\u003c/scope\u003e\n                \u003c/dependency\u003e\n            \u003c/dependencies\u003e\n        \u003c/profile\u003e\n        \u003cprofile\u003e\n            \u003cid\u003ehdp2.6.1\u003c/id\u003e\n            \u003cactivation\u003e\n                \u003cproperty\u003e\n                    \u003cname\u003eenv\u003c/name\u003e\n                    \u003cvalue\u003ehdp2.6.1\u003c/value\u003e\n                \u003c/property\u003e\n            \u003c/activation\u003e\n            \u003cbuild\u003e\n                \u003cplugins\u003e\n                    \u003cplugin\u003e\n                        \u003cartifactId\u003emaven-dependency-plugin\u003c/artifactId\u003e\n                        \u003cexecutions\u003e\n                            \u003cexecution\u003e\n                                \u003cphase\u003einstall\u003c/phase\u003e\n                                \u003cgoals\u003e\n                                    \u003cgoal\u003ecopy-dependencies\u003c/goal\u003e\n                                \u003c/goals\u003e\n                                \u003cconfiguration\u003e\n                                    \u003coutputDirectory\u003e${project.build.directory}/lib\u003c/outputDirectory\u003e\n                                    \u003cincludeArtifactIds\u003esplicemachine-hdp2.6.1-2.1.1.2.6.1.0-129_2.11,hbase_sql-hdp2.6.1,hbase_storage-hdp2.6.1,hbase-client\u003c/includeArtifactIds\u003e\n                                \u003c/configuration\u003e\n                            \u003c/execution\u003e\n                        \u003c/executions\u003e\n                    \u003c/plugin\u003e\n                \u003c/plugins\u003e\n            \u003c/build\u003e\n            \u003cdependencies\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003eorg.apache.spark\u003c/groupId\u003e\n                    \u003cartifactId\u003espark-core_2.11\u003c/artifactId\u003e\n                    \u003cversion\u003e2.1.1.2.6.1.0-129\u003c/version\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003eorg.apache.spark\u003c/groupId\u003e\n                    \u003cartifactId\u003espark-sql_2.11\u003c/artifactId\u003e\n                    \u003cversion\u003e2.1.1.2.6.1.0-129\u003c/version\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003ecom.splicemachine\u003c/groupId\u003e\n                    \u003cartifactId\u003esplicemachine-hdp2.6.1-2.1.1.2.6.1.0-129_2.11\u003c/artifactId\u003e\n                    \u003cversion\u003e${spliceVersion}\u003c/version\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003ecom.splicemachine\u003c/groupId\u003e\n                    \u003cartifactId\u003ehbase_sql-hdp2.6.1\u003c/artifactId\u003e\n                    \u003cversion\u003e${spliceVersion}\u003c/version\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003eorg.apache.hbase\u003c/groupId\u003e\n                    \u003cartifactId\u003ehbase-client\u003c/artifactId\u003e\n                    \u003cversion\u003e1.1.2.2.6.1.0-129\u003c/version\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003ecom.splicemachine\u003c/groupId\u003e\n                    \u003cartifactId\u003ehbase_storage-hdp2.6.1\u003c/artifactId\u003e\n                    \u003cversion\u003e${spliceVersion}\u003c/version\u003e\n                \u003c/dependency\u003e\n            \u003c/dependencies\u003e\n        \u003c/profile\u003e\n        \u003cprofile\u003e\n            \u003cid\u003ehdp2.6.4\u003c/id\u003e\n            \u003cactivation\u003e\n                \u003cproperty\u003e\n                    \u003cname\u003eenv\u003c/name\u003e\n                    \u003cvalue\u003ehdp2.6.4\u003c/value\u003e\n                \u003c/property\u003e\n            \u003c/activation\u003e\n            \u003cbuild\u003e\n                \u003cplugins\u003e\n                    \u003cplugin\u003e\n                        \u003cartifactId\u003emaven-dependency-plugin\u003c/artifactId\u003e\n                        \u003cexecutions\u003e\n                            \u003cexecution\u003e\n                                \u003cphase\u003einstall\u003c/phase\u003e\n                                \u003cgoals\u003e\n                                    \u003cgoal\u003ecopy-dependencies\u003c/goal\u003e\n                                \u003c/goals\u003e\n                                \u003cconfiguration\u003e\n                                    \u003coutputDirectory\u003e${project.build.directory}/lib\u003c/outputDirectory\u003e\n                                    \u003cincludeArtifactIds\u003esplicemachine-hdp2.6.4-2.2.0.2.6.4.0-91_2.11\u003c/includeArtifactIds\u003e\n                                \u003c/configuration\u003e\n                            \u003c/execution\u003e\n                        \u003c/executions\u003e\n                    \u003c/plugin\u003e\n                \u003c/plugins\u003e\n            \u003c/build\u003e\n            \u003cdependencies\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003eorg.apache.spark\u003c/groupId\u003e\n                    \u003cartifactId\u003espark-core_2.11\u003c/artifactId\u003e\n                    \u003cversion\u003e2.2.0.2.6.4.0-91\u003c/version\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003eorg.apache.spark\u003c/groupId\u003e\n                    \u003cartifactId\u003espark-sql_2.11\u003c/artifactId\u003e\n                    \u003cversion\u003e2.2.0.2.6.4.0-91\u003c/version\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003ecom.splicemachine\u003c/groupId\u003e\n                    \u003cartifactId\u003esplicemachine-hdp2.6.4-2.2.0.2.6.4.0-91_2.11\u003c/artifactId\u003e\n                    \u003cversion\u003e${spliceVersion}\u003c/version\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003ecom.splicemachine\u003c/groupId\u003e\n                    \u003cartifactId\u003ehbase_sql-hdp2.6.4\u003c/artifactId\u003e\n                    \u003cversion\u003e${spliceVersion}\u003c/version\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003eorg.apache.hbase\u003c/groupId\u003e\n                    \u003cartifactId\u003ehbase-client\u003c/artifactId\u003e\n                    \u003cversion\u003e1.1.2.2.6.4.0-91\u003c/version\u003e\n                \u003c/dependency\u003e\n                \u003cdependency\u003e\n                    \u003cgroupId\u003ecom.splicemachine\u003c/groupId\u003e\n                    \u003cartifactId\u003ehbase_storage-hdp2.6.4\u003c/artifactId\u003e\n                    \u003cversion\u003e${spliceVersion}\u003c/version\u003e\n                \u003c/dependency\u003e\n            \u003c/dependencies\u003e\n        \u003c/profile\u003e\n    \u003c/profiles\u003e\n\u003c/project\u003e\n~~~",
      "user": "anonymous",
      "dateUpdated": "2019-06-02 21:39:58.493",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {
          "spliceVersion": "",
          "project.build.directory": ""
        },
        "forms": {
          "spliceVersion": {
            "type": "TextBox",
            "name": "spliceVersion",
            "defaultValue": "",
            "hidden": false
          },
          "project.build.directory": {
            "type": "TextBox",
            "name": "project.build.directory",
            "defaultValue": "",
            "hidden": false
          }
        }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\u003ch1\u003eSetup Maven Coordinates\u003c/h1\u003e\n\u003cp\u003eThe following pom.xml file shows dependency resolution setup for various versions of Cloudera and Hortonwork based distribution. You can use this as a template for maven setup.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class\u003d\"html\"\u003e\u0026lt;?xml version\u003d\u0026quot;1.0\u0026quot; encoding\u003d\u0026quot;UTF-8\u0026quot;?\u0026gt;\n\u0026lt;project xmlns\u003d\u0026quot;http://maven.apache.org/POM/4.0.0\u0026quot;\n         xmlns:xsi\u003d\u0026quot;http://www.w3.org/2001/XMLSchema-instance\u0026quot;\n         xsi:schemaLocation\u003d\u0026quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026quot;\u0026gt;\n    \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt;\n    \u0026lt;groupId\u0026gt;com.splicemachine.tutorial\u0026lt;/groupId\u0026gt;\n    \u0026lt;artifactId\u0026gt;spark_adapter_example\u0026lt;/artifactId\u0026gt;\n    \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt;\n    \u0026lt;build\u0026gt;\n        \u0026lt;plugins\u0026gt;\n            \u0026lt;plugin\u0026gt;\n                \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt;\n                \u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt;\n                \u0026lt;configuration\u0026gt;\n                    \u0026lt;source\u0026gt;7\u0026lt;/source\u0026gt;\n                    \u0026lt;target\u0026gt;7\u0026lt;/target\u0026gt;\n                \u0026lt;/configuration\u0026gt;\n            \u0026lt;/plugin\u0026gt;\n        \u0026lt;/plugins\u0026gt;\n    \u0026lt;/build\u0026gt;\n    \u0026lt;properties\u0026gt;\n        \u0026lt;spliceVersion\u0026gt;2.7.0.1907\u0026lt;/spliceVersion\u0026gt;\n    \u0026lt;/properties\u0026gt;\n    \u0026lt;repositories\u0026gt;\n        \u0026lt;repository\u0026gt;\n            \u0026lt;id\u0026gt;cloudera\u0026lt;/id\u0026gt;\n            \u0026lt;url\u0026gt;https://repository.cloudera.com/artifactory/cloudera-repos/\u0026lt;/url\u0026gt;\n        \u0026lt;/repository\u0026gt;\n        \u0026lt;repository\u0026gt;\n            \u0026lt;id\u0026gt;hortonworks-releases\u0026lt;/id\u0026gt;\n            \u0026lt;url\u0026gt;http://repo.hortonworks.com/content/repositories/releases/\u0026lt;/url\u0026gt;\n        \u0026lt;/repository\u0026gt;\n        \u0026lt;repository\u0026gt;\n            \u0026lt;id\u0026gt;hortonworks-hadoop\u0026lt;/id\u0026gt;\n            \u0026lt;url\u0026gt;http://repo.hortonworks.com/content/repositories/jetty-hadoop/\u0026lt;/url\u0026gt;\n        \u0026lt;/repository\u0026gt;\n        \u0026lt;repository\u0026gt;\n            \u0026lt;id\u0026gt;splicemachine-public\u0026lt;/id\u0026gt;\n            \u0026lt;url\u0026gt;http://repository.splicemachine.com/nexus/content/groups/public\u0026lt;/url\u0026gt;\n        \u0026lt;/repository\u0026gt;\n    \u0026lt;/repositories\u0026gt;\n    \u0026lt;dependencies\u0026gt;\n        \u0026lt;dependency\u0026gt;\n            \u0026lt;groupId\u0026gt;com.splicemachine\u0026lt;/groupId\u0026gt;\n            \u0026lt;artifactId\u0026gt;splice_machine\u0026lt;/artifactId\u0026gt;\n            \u0026lt;version\u0026gt;\u0026lt;/version\u0026gt;\n        \u0026lt;/dependency\u0026gt;\n        \u0026lt;dependency\u0026gt;\n            \u0026lt;groupId\u0026gt;com.splicemachine\u0026lt;/groupId\u0026gt;\n            \u0026lt;artifactId\u0026gt;db-engine\u0026lt;/artifactId\u0026gt;\n            \u0026lt;version\u0026gt;\u0026lt;/version\u0026gt;\n        \u0026lt;/dependency\u0026gt;\n        \u0026lt;dependency\u0026gt;\n            \u0026lt;groupId\u0026gt;com.splicemachine\u0026lt;/groupId\u0026gt;\n            \u0026lt;artifactId\u0026gt;db-client\u0026lt;/artifactId\u0026gt;\n            \u0026lt;version\u0026gt;2.5.0.1820\u0026lt;/version\u0026gt;\n        \u0026lt;/dependency\u0026gt;\n    \u0026lt;/dependencies\u0026gt;\n    \u0026lt;profiles\u0026gt;\n        \u0026lt;profile\u0026gt;\n            \u0026lt;id\u0026gt;cdh5.16.1\u0026lt;/id\u0026gt;\n            \u0026lt;activation\u0026gt;\n                \u0026lt;property\u0026gt;\n                    \u0026lt;name\u0026gt;env\u0026lt;/name\u0026gt;\n                    \u0026lt;value\u0026gt;cdh5.16.1\u0026lt;/value\u0026gt;\n                \u0026lt;/property\u0026gt;\n            \u0026lt;/activation\u0026gt;\n            \u0026lt;build\u0026gt;\n                \u0026lt;plugins\u0026gt;\n                    \u0026lt;plugin\u0026gt;\n                        \u0026lt;artifactId\u0026gt;maven-dependency-plugin\u0026lt;/artifactId\u0026gt;\n                        \u0026lt;executions\u0026gt;\n                            \u0026lt;execution\u0026gt;\n                                \u0026lt;phase\u0026gt;install\u0026lt;/phase\u0026gt;\n                                \u0026lt;goals\u0026gt;\n                                    \u0026lt;goal\u0026gt;copy-dependencies\u0026lt;/goal\u0026gt;\n                                \u0026lt;/goals\u0026gt;\n                                \u0026lt;configuration\u0026gt;\n                                    \u0026lt;outputDirectory\u0026gt;/lib\u0026lt;/outputDirectory\u0026gt;\n                                    \u0026lt;includeArtifactIds\u0026gt;splicemachine-cdh5.16.1-2.2.0.cloudera2_2.11\u0026lt;/includeArtifactIds\u0026gt;\n                                \u0026lt;/configuration\u0026gt;\n                            \u0026lt;/execution\u0026gt;\n                        \u0026lt;/executions\u0026gt;\n                    \u0026lt;/plugin\u0026gt;\n                \u0026lt;/plugins\u0026gt;\n            \u0026lt;/build\u0026gt;\n            \u0026lt;dependencies\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;spark-core_2.11\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;2.2.0.cloudera2\u0026lt;/version\u0026gt;\n                    \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;spark-sql_2.11\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;2.2.0.cloudera2\u0026lt;/version\u0026gt;\n                    \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;com.splicemachine\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;splicemachine-cdh5.16.1-2.2.0.cloudera2_2.11\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;\u0026lt;/version\u0026gt;\n                    \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;com.splicemachine\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;hbase_sql-cdh5.16.1\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;\u0026lt;/version\u0026gt;\n                    \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;org.apache.hbase\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;hbase-client\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;1.2.0-cdh5.16.1\u0026lt;/version\u0026gt;\n                    \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;com.splicemachine\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;hbase_storage-cdh5.16.1\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;\u0026lt;/version\u0026gt;\n                    \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n            \u0026lt;/dependencies\u0026gt;\n        \u0026lt;/profile\u0026gt;\n        \u0026lt;profile\u0026gt;\n            \u0026lt;id\u0026gt;cdh5.12.2\u0026lt;/id\u0026gt;\n            \u0026lt;activation\u0026gt;\n                \u0026lt;property\u0026gt;\n                    \u0026lt;name\u0026gt;env\u0026lt;/name\u0026gt;\n                    \u0026lt;value\u0026gt;cdh5.12.2\u0026lt;/value\u0026gt;\n                \u0026lt;/property\u0026gt;\n            \u0026lt;/activation\u0026gt;\n            \u0026lt;build\u0026gt;\n                \u0026lt;plugins\u0026gt;\n                    \u0026lt;plugin\u0026gt;\n                        \u0026lt;artifactId\u0026gt;maven-dependency-plugin\u0026lt;/artifactId\u0026gt;\n                        \u0026lt;executions\u0026gt;\n                            \u0026lt;execution\u0026gt;\n                                \u0026lt;phase\u0026gt;install\u0026lt;/phase\u0026gt;\n                                \u0026lt;goals\u0026gt;\n                                    \u0026lt;goal\u0026gt;copy-dependencies\u0026lt;/goal\u0026gt;\n                                \u0026lt;/goals\u0026gt;\n                                \u0026lt;configuration\u0026gt;\n                                    \u0026lt;outputDirectory\u0026gt;/lib\u0026lt;/outputDirectory\u0026gt;\n                                    \u0026lt;includeArtifactIds\u0026gt;splicemachine-cdh5.12.2-2.2.0.cloudera2_2.11\u0026lt;/includeArtifactIds\u0026gt;\n                                \u0026lt;/configuration\u0026gt;\n                            \u0026lt;/execution\u0026gt;\n                        \u0026lt;/executions\u0026gt;\n                    \u0026lt;/plugin\u0026gt;\n                \u0026lt;/plugins\u0026gt;\n            \u0026lt;/build\u0026gt;\n            \u0026lt;dependencies\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;spark-core_2.11\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;2.2.0.cloudera2\u0026lt;/version\u0026gt;\n                    \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;spark-sql_2.11\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;2.2.0.cloudera2\u0026lt;/version\u0026gt;\n                    \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;com.splicemachine\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;splicemachine-cdh5.12.2-2.2.0.cloudera2_2.11\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;\u0026lt;/version\u0026gt;\n                    \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;com.splicemachine\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;hbase_sql-cdh5.12.2\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;\u0026lt;/version\u0026gt;\n                    \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;org.apache.hbase\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;hbase-client\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;1.2.0-cdh5.12.2\u0026lt;/version\u0026gt;\n                    \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;com.splicemachine\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;hbase_storage-cdh5.12.2\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;\u0026lt;/version\u0026gt;\n                    \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n            \u0026lt;/dependencies\u0026gt;\n        \u0026lt;/profile\u0026gt;\n        \u0026lt;profile\u0026gt;\n            \u0026lt;id\u0026gt;cdh5.14.0\u0026lt;/id\u0026gt;\n            \u0026lt;activation\u0026gt;\n                \u0026lt;property\u0026gt;\n                    \u0026lt;name\u0026gt;env\u0026lt;/name\u0026gt;\n                    \u0026lt;value\u0026gt;cdh5.14.0\u0026lt;/value\u0026gt;\n                \u0026lt;/property\u0026gt;\n            \u0026lt;/activation\u0026gt;\n            \u0026lt;build\u0026gt;\n                \u0026lt;plugins\u0026gt;\n                    \u0026lt;plugin\u0026gt;\n                        \u0026lt;artifactId\u0026gt;maven-dependency-plugin\u0026lt;/artifactId\u0026gt;\n                        \u0026lt;executions\u0026gt;\n                            \u0026lt;execution\u0026gt;\n                                \u0026lt;phase\u0026gt;install\u0026lt;/phase\u0026gt;\n                                \u0026lt;goals\u0026gt;\n                                    \u0026lt;goal\u0026gt;copy-dependencies\u0026lt;/goal\u0026gt;\n                                \u0026lt;/goals\u0026gt;\n                                \u0026lt;configuration\u0026gt;\n                                    \u0026lt;outputDirectory\u0026gt;/lib\u0026lt;/outputDirectory\u0026gt;\n                                    \u0026lt;includeArtifactIds\u0026gt;splicemachine-cdh5.14.0-2.2.0.cloudera2_2.11\u0026lt;/includeArtifactIds\u0026gt;\n                                \u0026lt;/configuration\u0026gt;\n                            \u0026lt;/execution\u0026gt;\n                        \u0026lt;/executions\u0026gt;\n                    \u0026lt;/plugin\u0026gt;\n                \u0026lt;/plugins\u0026gt;\n            \u0026lt;/build\u0026gt;\n            \u0026lt;dependencies\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;spark-core_2.11\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;2.2.0.cloudera2\u0026lt;/version\u0026gt;\n                    \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;spark-sql_2.11\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;2.2.0.cloudera2\u0026lt;/version\u0026gt;\n                    \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;com.splicemachine\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;splicemachine-cdh5.14.0-2.2.0.cloudera2_2.11\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;\u0026lt;/version\u0026gt;\n                    \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;com.splicemachine\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;hbase_sql-cdh5.14.0\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;\u0026lt;/version\u0026gt;\n                    \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;org.apache.hbase\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;hbase-client\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;1.2.0-cdh5.14.0\u0026lt;/version\u0026gt;\n                    \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;com.splicemachine\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;hbase_storage-cdh5.14.0\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;\u0026lt;/version\u0026gt;\n                    \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n            \u0026lt;/dependencies\u0026gt;\n        \u0026lt;/profile\u0026gt;\n        \u0026lt;profile\u0026gt;\n            \u0026lt;id\u0026gt;hdp2.6.3\u0026lt;/id\u0026gt;\n            \u0026lt;activation\u0026gt;\n                \u0026lt;property\u0026gt;\n                    \u0026lt;name\u0026gt;env\u0026lt;/name\u0026gt;\n                    \u0026lt;value\u0026gt;hdp2.6.3\u0026lt;/value\u0026gt;\n                \u0026lt;/property\u0026gt;\n            \u0026lt;/activation\u0026gt;\n            \u0026lt;build\u0026gt;\n                \u0026lt;plugins\u0026gt;\n                    \u0026lt;plugin\u0026gt;\n                        \u0026lt;artifactId\u0026gt;maven-dependency-plugin\u0026lt;/artifactId\u0026gt;\n                        \u0026lt;executions\u0026gt;\n                            \u0026lt;execution\u0026gt;\n                                \u0026lt;phase\u0026gt;install\u0026lt;/phase\u0026gt;\n                                \u0026lt;goals\u0026gt;\n                                    \u0026lt;goal\u0026gt;copy-dependencies\u0026lt;/goal\u0026gt;\n                                \u0026lt;/goals\u0026gt;\n                                \u0026lt;configuration\u0026gt;\n                                    \u0026lt;outputDirectory\u0026gt;/lib\u0026lt;/outputDirectory\u0026gt;\n                                    \u0026lt;includeArtifactIds\u0026gt;splicemachine-hdp2.6.3-2.2.0.2.6.3.0-235_2.11\u0026lt;/includeArtifactIds\u0026gt;\n                                \u0026lt;/configuration\u0026gt;\n                            \u0026lt;/execution\u0026gt;\n                        \u0026lt;/executions\u0026gt;\n                    \u0026lt;/plugin\u0026gt;\n                \u0026lt;/plugins\u0026gt;\n            \u0026lt;/build\u0026gt;\n            \u0026lt;dependencies\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;spark-core_2.11\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;2.2.0.2.6.3.0-235\u0026lt;/version\u0026gt;\n                    \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;spark-sql_2.11\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;2.2.0.2.6.3.0-235\u0026lt;/version\u0026gt;\n                    \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;com.splicemachine\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;splicemachine-hdp2.6.3-2.2.0.2.6.3.0-235_2.11\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;\u0026lt;/version\u0026gt;\n                    \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;com.splicemachine\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;hbase_sql-hdp2.6.3\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;\u0026lt;/version\u0026gt;\n                    \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;org.apache.hbase\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;hbase-client\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;1.1.2.2.6.3.0-235\u0026lt;/version\u0026gt;\n                    \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;com.splicemachine\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;hbase_storage-hdp2.6.3\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;\u0026lt;/version\u0026gt;\n                    \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n            \u0026lt;/dependencies\u0026gt;\n        \u0026lt;/profile\u0026gt;\n        \u0026lt;profile\u0026gt;\n            \u0026lt;id\u0026gt;hdp2.6.1\u0026lt;/id\u0026gt;\n            \u0026lt;activation\u0026gt;\n                \u0026lt;property\u0026gt;\n                    \u0026lt;name\u0026gt;env\u0026lt;/name\u0026gt;\n                    \u0026lt;value\u0026gt;hdp2.6.1\u0026lt;/value\u0026gt;\n                \u0026lt;/property\u0026gt;\n            \u0026lt;/activation\u0026gt;\n            \u0026lt;build\u0026gt;\n                \u0026lt;plugins\u0026gt;\n                    \u0026lt;plugin\u0026gt;\n                        \u0026lt;artifactId\u0026gt;maven-dependency-plugin\u0026lt;/artifactId\u0026gt;\n                        \u0026lt;executions\u0026gt;\n                            \u0026lt;execution\u0026gt;\n                                \u0026lt;phase\u0026gt;install\u0026lt;/phase\u0026gt;\n                                \u0026lt;goals\u0026gt;\n                                    \u0026lt;goal\u0026gt;copy-dependencies\u0026lt;/goal\u0026gt;\n                                \u0026lt;/goals\u0026gt;\n                                \u0026lt;configuration\u0026gt;\n                                    \u0026lt;outputDirectory\u0026gt;/lib\u0026lt;/outputDirectory\u0026gt;\n                                    \u0026lt;includeArtifactIds\u0026gt;splicemachine-hdp2.6.1-2.1.1.2.6.1.0-129_2.11,hbase_sql-hdp2.6.1,hbase_storage-hdp2.6.1,hbase-client\u0026lt;/includeArtifactIds\u0026gt;\n                                \u0026lt;/configuration\u0026gt;\n                            \u0026lt;/execution\u0026gt;\n                        \u0026lt;/executions\u0026gt;\n                    \u0026lt;/plugin\u0026gt;\n                \u0026lt;/plugins\u0026gt;\n            \u0026lt;/build\u0026gt;\n            \u0026lt;dependencies\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;spark-core_2.11\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;2.1.1.2.6.1.0-129\u0026lt;/version\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;spark-sql_2.11\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;2.1.1.2.6.1.0-129\u0026lt;/version\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;com.splicemachine\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;splicemachine-hdp2.6.1-2.1.1.2.6.1.0-129_2.11\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;\u0026lt;/version\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;com.splicemachine\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;hbase_sql-hdp2.6.1\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;\u0026lt;/version\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;org.apache.hbase\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;hbase-client\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;1.1.2.2.6.1.0-129\u0026lt;/version\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;com.splicemachine\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;hbase_storage-hdp2.6.1\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;\u0026lt;/version\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n            \u0026lt;/dependencies\u0026gt;\n        \u0026lt;/profile\u0026gt;\n        \u0026lt;profile\u0026gt;\n            \u0026lt;id\u0026gt;hdp2.6.4\u0026lt;/id\u0026gt;\n            \u0026lt;activation\u0026gt;\n                \u0026lt;property\u0026gt;\n                    \u0026lt;name\u0026gt;env\u0026lt;/name\u0026gt;\n                    \u0026lt;value\u0026gt;hdp2.6.4\u0026lt;/value\u0026gt;\n                \u0026lt;/property\u0026gt;\n            \u0026lt;/activation\u0026gt;\n            \u0026lt;build\u0026gt;\n                \u0026lt;plugins\u0026gt;\n                    \u0026lt;plugin\u0026gt;\n                        \u0026lt;artifactId\u0026gt;maven-dependency-plugin\u0026lt;/artifactId\u0026gt;\n                        \u0026lt;executions\u0026gt;\n                            \u0026lt;execution\u0026gt;\n                                \u0026lt;phase\u0026gt;install\u0026lt;/phase\u0026gt;\n                                \u0026lt;goals\u0026gt;\n                                    \u0026lt;goal\u0026gt;copy-dependencies\u0026lt;/goal\u0026gt;\n                                \u0026lt;/goals\u0026gt;\n                                \u0026lt;configuration\u0026gt;\n                                    \u0026lt;outputDirectory\u0026gt;/lib\u0026lt;/outputDirectory\u0026gt;\n                                    \u0026lt;includeArtifactIds\u0026gt;splicemachine-hdp2.6.4-2.2.0.2.6.4.0-91_2.11\u0026lt;/includeArtifactIds\u0026gt;\n                                \u0026lt;/configuration\u0026gt;\n                            \u0026lt;/execution\u0026gt;\n                        \u0026lt;/executions\u0026gt;\n                    \u0026lt;/plugin\u0026gt;\n                \u0026lt;/plugins\u0026gt;\n            \u0026lt;/build\u0026gt;\n            \u0026lt;dependencies\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;spark-core_2.11\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;2.2.0.2.6.4.0-91\u0026lt;/version\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;spark-sql_2.11\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;2.2.0.2.6.4.0-91\u0026lt;/version\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;com.splicemachine\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;splicemachine-hdp2.6.4-2.2.0.2.6.4.0-91_2.11\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;\u0026lt;/version\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;com.splicemachine\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;hbase_sql-hdp2.6.4\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;\u0026lt;/version\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;org.apache.hbase\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;hbase-client\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;1.1.2.2.6.4.0-91\u0026lt;/version\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n                \u0026lt;dependency\u0026gt;\n                    \u0026lt;groupId\u0026gt;com.splicemachine\u0026lt;/groupId\u0026gt;\n                    \u0026lt;artifactId\u0026gt;hbase_storage-hdp2.6.4\u0026lt;/artifactId\u0026gt;\n                    \u0026lt;version\u0026gt;\u0026lt;/version\u0026gt;\n                \u0026lt;/dependency\u0026gt;\n            \u0026lt;/dependencies\u0026gt;\n        \u0026lt;/profile\u0026gt;\n    \u0026lt;/profiles\u0026gt;\n\u0026lt;/project\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559511543147_1644452621",
      "id": "20190602-213903_1547198808",
      "dateCreated": "2019-06-02 21:39:03.147",
      "dateStarted": "2019-06-02 21:39:58.492",
      "dateFinished": "2019-06-02 21:39:58.626",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n# Create SpliceMachineContext\n\nThe following code snippet illustrates creating an object of SpliceMachineContext in Java after setting up the project:\n\n\u003cpre\u003e\u003ccode\u003e\nimport com.splicemachine.derby.impl.SpliceSpark;\nimport com.splicemachine.spark.splicemachine.SplicemachineContext;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n....\n\npublic class TestSpliceMachineContext\n{\n    public static void main(String[] args) throws Exception \n    {\n        String dbUrl \u003d \"jdbc:splice://SPLICESERVERHOST:1527/splicedb;user\u003d\u0026lt;yourUserId\u0026gt;;password\u003d\u0026lt;yourPassword\u0026gt;\";\n        SparkConf conf \u003d new SparkConf();\n        SparkSession spark \u003d SparkSession.builder().appName(\"Reader\").config(conf).getOrCreate();\n        SpliceSpark.setContext(spark.sparkContext());\n        SplicemachineContext splicemachineContext \u003d new SplicemachineContext(dbUrl);\n        \n        //use context for DB operations next...\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e",
      "user": "anonymous",
      "dateUpdated": "2019-06-03 16:45:55.091",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\u003ch1\u003eCreate SpliceMachineContext\u003c/h1\u003e\n\u003cp\u003eThe following code snippet illustrates creating an object of SpliceMachineContext in Java after setting up the project:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\nimport com.splicemachine.derby.impl.SpliceSpark;\nimport com.splicemachine.spark.splicemachine.SplicemachineContext;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n....\n\npublic class TestSpliceMachineContext\n{\n    public static void main(String[] args) throws Exception \n    {\n        String dbUrl \u003d \"jdbc:splice://SPLICESERVERHOST:1527/splicedb;user\u003d\u0026lt;yourUserId\u0026gt;;password\u003d\u0026lt;yourPassword\u0026gt;\";\n        SparkConf conf \u003d new SparkConf();\n        SparkSession spark \u003d SparkSession.builder().appName(\"Reader\").config(conf).getOrCreate();\n        SpliceSpark.setContext(spark.sparkContext());\n        SplicemachineContext splicemachineContext \u003d new SplicemachineContext(dbUrl);\n        \n        //use context for DB operations next...\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559511623111_-960005237",
      "id": "20190602-214023_872601603",
      "dateCreated": "2019-06-02 21:40:23.111",
      "dateStarted": "2019-06-03 16:45:55.092",
      "dateFinished": "2019-06-03 16:45:55.115",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n# Query using SpliceMachineContext\n\nUsing the SpliceMachineContext, you can obtain access to Spark DataFrame within this context while querying the DB. \n\n\u003cpre\u003e\u003ccode\u003e\nimport com.splicemachine.derby.impl.SpliceSpark;\nimport com.splicemachine.spark.splicemachine.SplicemachineContext;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n....\n\npublic class TestSpliceMachineContext\n{\n    public static void main(String[] args) throws Exception \n    {\n        String dbUrl \u003d \"jdbc:splice://SPLICESERVERHOST:1527/splicedb;user\u003d\u0026lt;yourUserId\u0026gt;;password\u003d\u0026lt;yourPassword\u0026gt;\";\n        SparkConf conf \u003d new SparkConf();\n        SparkSession spark \u003d SparkSession.builder().appName(\"Reader\").config(conf).getOrCreate();\n        SpliceSpark.setContext(spark.sparkContext());\n        SplicemachineContext splicemachineContext \u003d new SplicemachineContext(dbUrl);\n        \n        //Query table, use df operations based out of SpliceMachineContext\n        String spliceQuery \u003d \"select count(*) from \" + \u0026lt;TABLE_NAME\u0026gt;;\n        splicemachineContext.df(spliceQuery).show();\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e",
      "user": "anonymous",
      "dateUpdated": "2019-06-03 16:58:09.454",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\u003ch1\u003eQuery using SpliceMachineContext\u003c/h1\u003e\n\u003cp\u003eUsing the SpliceMachineContext, you can obtain access to Spark DataFrame within this context while querying the DB. \u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\nimport com.splicemachine.derby.impl.SpliceSpark;\nimport com.splicemachine.spark.splicemachine.SplicemachineContext;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n....\n\npublic class TestSpliceMachineContext\n{\n    public static void main(String[] args) throws Exception \n    {\n        String dbUrl \u003d \"jdbc:splice://SPLICESERVERHOST:1527/splicedb;user\u003d\u0026lt;yourUserId\u0026gt;;password\u003d\u0026lt;yourPassword\u0026gt;\";\n        SparkConf conf \u003d new SparkConf();\n        SparkSession spark \u003d SparkSession.builder().appName(\"Reader\").config(conf).getOrCreate();\n        SpliceSpark.setContext(spark.sparkContext());\n        SplicemachineContext splicemachineContext \u003d new SplicemachineContext(dbUrl);\n        \n        //Query table, use df operations based out of SpliceMachineContext\n        String spliceQuery \u003d \"select count(*) from \" + \u0026lt;TABLE_NAME\u0026gt;;\n        splicemachineContext.df(spliceQuery).show();\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559578447544_-125019814",
      "id": "20190603-161407_2067737309",
      "dateCreated": "2019-06-03 16:14:07.544",
      "dateStarted": "2019-06-03 16:58:09.460",
      "dateFinished": "2019-06-03 16:58:09.475",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n# Load data using SpliceMachineContext\n\nUsing the SpliceMachineContext, you can load data into a table. In the following, we read from a parquet file and load data into Splice Machine table.\n\n\u003cpre\u003e\u003ccode\u003e\nimport com.splicemachine.derby.impl.SpliceSpark;\nimport com.splicemachine.spark.splicemachine.SplicemachineContext;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n....\n\npublic class TestSpliceMachineContext\n{\n    public static void main(String[] args) throws Exception \n    {\n        String dbUrl \u003d \"jdbc:splice://SPLICESERVERHOST:1527/splicedb;user\u003d\u0026lt;yourUserId\u0026gt;;password\u003d\u0026lt;yourPassword\u0026gt;\";\n        SparkConf conf \u003d new SparkConf();\n        SparkSession spark \u003d SparkSession.builder().appName(\"Reader\").config(conf).getOrCreate();\n        SpliceSpark.setContext(spark.sparkContext());\n        SplicemachineContext splicemachineContext \u003d new SplicemachineContext(dbUrl);\n        \n        //Load data into a table using Parquet file\n        String parquetFilePath \u003d \u0026lt;PATH_TO_PARQUET_FILE\u0026gt;;\n        Dataset\u003cRow\u003e ds \u003d spark.read().parquet(parquetFilePath);\n        splicemachineContext.truncateTable(\u0026lt;TABLE_NAME\u0026gt;);\n        splicemachineContext.insert(ds, \u0026lt;TABLE_NAME\u0026gt;);\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e",
      "user": "anonymous",
      "dateUpdated": "2019-06-03 16:48:05.547",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\u003ch1\u003eLoad data using SpliceMachineContext\u003c/h1\u003e\n\u003cp\u003eUsing the SpliceMachineContext, you can load data into a table. In the following, we read from a parquet file and load data into Splice Machine table.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\nimport com.splicemachine.derby.impl.SpliceSpark;\nimport com.splicemachine.spark.splicemachine.SplicemachineContext;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n....\n\npublic class TestSpliceMachineContext\n{\n    public static void main(String[] args) throws Exception \n    {\n        String dbUrl \u003d \"jdbc:splice://SPLICESERVERHOST:1527/splicedb;user\u003d\u0026lt;yourUserId\u0026gt;;password\u003d\u0026lt;yourPassword\u0026gt;\";\n        SparkConf conf \u003d new SparkConf();\n        SparkSession spark \u003d SparkSession.builder().appName(\"Reader\").config(conf).getOrCreate();\n        SpliceSpark.setContext(spark.sparkContext());\n        SplicemachineContext splicemachineContext \u003d new SplicemachineContext(dbUrl);\n        \n        //Load data into a table using Parquet file\n        String parquetFilePath \u003d \u0026lt;PATH_TO_PARQUET_FILE\u0026gt;;\n        Dataset\u003cRow\u003e ds \u003d spark.read().parquet(parquetFilePath);\n        splicemachineContext.truncateTable(\u0026lt;TABLE_NAME\u0026gt;);\n        splicemachineContext.insert(ds, \u0026lt;TABLE_NAME\u0026gt;);\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559579990934_-1817260673",
      "id": "20190603-163950_1385722937",
      "dateCreated": "2019-06-03 16:39:50.934",
      "dateStarted": "2019-06-03 16:48:05.552",
      "dateFinished": "2019-06-03 16:48:05.572",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n# InternalAccess while using SpliceMachineContext\n\nBy default, Native Spark DataSource queries execute in the Spark application, which is highly performant and allows access to almost all Splice Machine features. However, when your Native Spark DataSource application uses our Access Control List (ACL) feature, there is a restriction with regard to checking permissions.\n\nThe specific problem is that the Native Spark DataSource does not have the ability to check permissions at the view level or column level; instead, it checks permissions on the base table. This means that if your Native Spark DataSource application doesn’t have access to the table underlying a view or column, it will not have access to that view or column; as a result, a query against the view or colunn fails and throws an exception.\n\nThe workaround for this problem is to tell the Native Spark DataSource to use internal access to the database; this enables view/column permission checking, at a very slight cost in performance. With internal access, the adapter runs queries in Splice Machine and temporarily persists data in HDFS while running the query.\n\nThe ACL feature is enabled by setting splice.authentication.token.enabled\u003dtrue\n\nIn addition, you must make sure that each user who is going to use the Splice Machine Native Spark DataSource has execute permission on the SYSCS_UTIL.SYSCS_HDFS_OPERATION system procedure.\n\nSYSCS_UTIL.SYSCS_HDFS_OPERATION is a Splice Machine system procedure that is used internally to efficiently perform direct HDFS operations. This procedure is not documented because it is intended only for use by the Splice Machine code itself; however, the Native Spark DataSource uses it, so any user of the Adapter must have permission to execute the SYSCS_UTIL.SYSCS_HDFS_OPERATION procedure.\n\nAn example of granting this permission is below:\n\n\u003cpre\u003e\u003ccode\u003e\nsplice\u003e grant execute on procedure SYSCS_UTIL.SYSCS_HDFS_OPERATION to someuser;\n0 rows inserted/updated/deleted\n\u003c/code\u003e\u003c/pre\u003e\n\nFollwing is an example to use internalDF instead of Spark DF:\n\n\u003cpre\u003e\u003ccode\u003e\nimport com.splicemachine.derby.impl.SpliceSpark;\nimport com.splicemachine.spark.splicemachine.SplicemachineContext;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n....\n\npublic class TestSpliceMachineContext\n{\n    public static void main(String[] args) throws Exception \n    {\n        String dbUrl \u003d \"jdbc:splice://SPLICESERVERHOST:1527/splicedb;user\u003d\u0026lt;yourUserId\u0026gt;;password\u003d\u0026lt;yourPassword\u0026gt;\";\n        SparkConf conf \u003d new SparkConf();\n        SparkSession spark \u003d SparkSession.builder().appName(\"Reader\").config(conf).getOrCreate();\n        SpliceSpark.setContext(spark.sparkContext());\n        SplicemachineContext splicemachineContext \u003d new SplicemachineContext(dbUrl);\n        \n        //Query table, use df operations based out of SpliceMachineContext\n        String spliceQuery \u003d \"select count(*) from \" + \u0026lt;TABLE_NAME\u0026gt;;\n        splicemachineContext.internalDf(spliceQuery).show();\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e",
      "user": "anonymous",
      "dateUpdated": "2019-06-03 16:58:26.882",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\u003ch1\u003eInternalAccess while using SpliceMachineContext\u003c/h1\u003e\n\u003cp\u003eBy default, Native Spark DataSource queries execute in the Spark application, which is highly performant and allows access to almost all Splice Machine features. However, when your Native Spark DataSource application uses our Access Control List (ACL) feature, there is a restriction with regard to checking permissions.\u003c/p\u003e\n\u003cp\u003eThe specific problem is that the Native Spark DataSource does not have the ability to check permissions at the view level or column level; instead, it checks permissions on the base table. This means that if your Native Spark DataSource application doesn’t have access to the table underlying a view or column, it will not have access to that view or column; as a result, a query against the view or colunn fails and throws an exception.\u003c/p\u003e\n\u003cp\u003eThe workaround for this problem is to tell the Native Spark DataSource to use internal access to the database; this enables view/column permission checking, at a very slight cost in performance. With internal access, the adapter runs queries in Splice Machine and temporarily persists data in HDFS while running the query.\u003c/p\u003e\n\u003cp\u003eThe ACL feature is enabled by setting splice.authentication.token.enabled\u003dtrue\u003c/p\u003e\n\u003cp\u003eIn addition, you must make sure that each user who is going to use the Splice Machine Native Spark DataSource has execute permission on the SYSCS_UTIL.SYSCS_HDFS_OPERATION system procedure.\u003c/p\u003e\n\u003cp\u003eSYSCS_UTIL.SYSCS_HDFS_OPERATION is a Splice Machine system procedure that is used internally to efficiently perform direct HDFS operations. This procedure is not documented because it is intended only for use by the Splice Machine code itself; however, the Native Spark DataSource uses it, so any user of the Adapter must have permission to execute the SYSCS_UTIL.SYSCS_HDFS_OPERATION procedure.\u003c/p\u003e\n\u003cp\u003eAn example of granting this permission is below:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\nsplice\u003e grant execute on procedure SYSCS_UTIL.SYSCS_HDFS_OPERATION to someuser;\n0 rows inserted/updated/deleted\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFollwing is an example to use internalDF instead of Spark DF:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\nimport com.splicemachine.derby.impl.SpliceSpark;\nimport com.splicemachine.spark.splicemachine.SplicemachineContext;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n....\n\npublic class TestSpliceMachineContext\n{\n    public static void main(String[] args) throws Exception \n    {\n        String dbUrl \u003d \"jdbc:splice://SPLICESERVERHOST:1527/splicedb;user\u003d\u0026lt;yourUserId\u0026gt;;password\u003d\u0026lt;yourPassword\u0026gt;\";\n        SparkConf conf \u003d new SparkConf();\n        SparkSession spark \u003d SparkSession.builder().appName(\"Reader\").config(conf).getOrCreate();\n        SpliceSpark.setContext(spark.sparkContext());\n        SplicemachineContext splicemachineContext \u003d new SplicemachineContext(dbUrl);\n        \n        //Query table, use df operations based out of SpliceMachineContext\n        String spliceQuery \u003d \"select count(*) from \" + \u0026lt;TABLE_NAME\u0026gt;;\n        splicemachineContext.internalDf(spliceQuery).show();\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559580591238_-232357716",
      "id": "20190603-164951_106071028",
      "dateCreated": "2019-06-03 16:49:51.238",
      "dateStarted": "2019-06-03 16:58:26.883",
      "dateFinished": "2019-06-03 16:58:26.926",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n# Build and Deploy The Application\n\nSimilar to a typical build and deploy process of a Spark application, you can build the Jar (we suggest building uber jar) using maven.\n\n\u003cpre\u003e\u003ccode\u003emvn clean package\u003c/code\u003e\u003c/pre\u003e\n\nOnce the jar is built (typically in \"target\" folder within the project folder), you can copy it over to the edge node or region server on the cluster to execute using spark-submit command.\n\nNote that if you are using the docker image, you can copy the jar over to the running instance using the following command:\n\n\u003cpre\u003e\u003ccode\u003edocker cp \u0026lt;PATH_ON_HOST_MACHINE\u0026gt; spliceserver:\u0026lt;PATH_TO_TARGET_FOLDER_ON_RUNNING_INSTANCE\u0026gt; \u003c/code\u003e\u003c/pre\u003e",
      "user": "anonymous",
      "dateUpdated": "2019-06-04 17:43:47.971",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\u003ch1\u003eBuild and Deploy The Application\u003c/h1\u003e\n\u003cp\u003eSimilar to a typical build and deploy process of a Spark application, you can build the Jar (we suggest building uber jar) using maven.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emvn clean package\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce the jar is built (typically in \u0026ldquo;target\u0026rdquo; folder within the project folder), you can copy it over to the edge node or region server on the cluster to execute using spark-submit command.\u003c/p\u003e\n\u003cp\u003eNote that if you are using the docker image, you can copy the jar over to the running instance using the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker cp \u0026lt;PATH_ON_HOST_MACHINE\u0026gt; spliceserver:\u0026lt;PATH_TO_TARGET_FOLDER_ON_RUNNING_INSTANCE\u0026gt; \u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559581897072_927636269",
      "id": "20190603-171137_1488177463",
      "dateCreated": "2019-06-03 17:11:37.073",
      "dateStarted": "2019-06-04 17:43:47.993",
      "dateFinished": "2019-06-04 17:43:48.018",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n# Using spark submit to launch spark adapter applications.\n\nFollowing command can be used on on CDH cluster. Note that various executor settings are just illustrative and should be configured for respective job appropriately.\n\u003cpre\u003e\u003ccode\u003e\nspark2-submit --conf \"spark.dynamicAllocation.enabled\u003dfalse\" --conf \"spark.task.maxFailures\u003d2\" --conf \"spark.driver.memory\u003d4g\" --conf \"spark.driver.cores\u003d1\" --conf \"spark.kryoserializer.buffer\u003d1024\" --conf \"spark.kryoserializer.buffer.max\u003d2047\" --conf \"spark.io.compression.codec\u003dorg.apache.spark.io.SnappyCompressionCodec\" --conf \"spark.executor.extraClassPath\u003d/etc/hadoop/conf/:/etc/hbase/conf/:/opt/cloudera/parcels/SPLICEMACHINE/lib/*:/opt/cloudera/parcels/SPARK2/lib/spark2/jars/*:/opt/cloudera/parcels/CDH/lib/hbase/lib/*\" --conf \"spark.driver.extraClassPath\u003d/etc/hadoop/conf/:/etc/hbase/conf/:/opt/cloudera/parcels/SPLICEMACHINE/lib/*:/opt/cloudera/parcels/SPARK2/lib/spark2/jars/*:/opt/cloudera/parcels/CDH/lib/hbase/lib/*\" --jars \"sparksplice.jar\" --class \u0026lt;MAIN_CLASS_NAME\u0026gt; --master yarn --deploy-mode client --num-executors 8 --executor-memory 2G --executor-cores 1 \u0026lt;JAR_FILE_NAME\u0026gt; localhost 1527 splice admin\n\u003c/code\u003e\u003c/pre\u003e",
      "user": "anonymous",
      "dateUpdated": "2019-06-04 17:36:33.597",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\u003ch1\u003eUsing spark submit to launch spark adapter applications.\u003c/h1\u003e\n\u003cp\u003eFollowing command can be used on on CDH cluster. Note that various executor settings are just illustrative and should be configured for respective job appropriately.\u003cbr/\u003e\u003cpre\u003e\u003ccode\u003e\u003cbr/\u003espark2-submit \u0026ndash;conf \u0026ldquo;spark.dynamicAllocation.enabled\u003dfalse\u0026rdquo; \u0026ndash;conf \u0026ldquo;spark.task.maxFailures\u003d2\u0026rdquo; \u0026ndash;conf \u0026ldquo;spark.driver.memory\u003d4g\u0026rdquo; \u0026ndash;conf \u0026ldquo;spark.driver.cores\u003d1\u0026rdquo; \u0026ndash;conf \u0026ldquo;spark.kryoserializer.buffer\u003d1024\u0026rdquo; \u0026ndash;conf \u0026ldquo;spark.kryoserializer.buffer.max\u003d2047\u0026rdquo; \u0026ndash;conf \u0026ldquo;spark.io.compression.codec\u003dorg.apache.spark.io.SnappyCompressionCodec\u0026rdquo; \u0026ndash;conf \u0026ldquo;spark.executor.extraClassPath\u003d/etc/hadoop/conf/:/etc/hbase/conf/:/opt/cloudera/parcels/SPLICEMACHINE/lib/*:/opt/cloudera/parcels/SPARK2/lib/spark2/jars/*:/opt/cloudera/parcels/CDH/lib/hbase/lib/*\u0026rdquo; \u0026ndash;conf \u0026ldquo;spark.driver.extraClassPath\u003d/etc/hadoop/conf/:/etc/hbase/conf/:/opt/cloudera/parcels/SPLICEMACHINE/lib/*:/opt/cloudera/parcels/SPARK2/lib/spark2/jars/*:/opt/cloudera/parcels/CDH/lib/hbase/lib/*\u0026rdquo; \u0026ndash;jars \u0026ldquo;sparksplice.jar\u0026rdquo; \u0026ndash;class \u0026lt;MAIN_CLASS_NAME\u0026gt; \u0026ndash;master yarn \u0026ndash;deploy-mode client \u0026ndash;num-executors 8 \u0026ndash;executor-memory 2G \u0026ndash;executor-cores 1 \u0026lt;JAR_FILE_NAME\u0026gt; localhost 1527 splice admin\u003cbr/\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559669789849_-819139079",
      "id": "20190604-173629_1491743944",
      "dateCreated": "2019-06-04 17:36:29.849",
      "dateStarted": "2019-06-04 17:36:33.616",
      "dateFinished": "2019-06-04 17:36:36.093",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Where to Go Next\nTo finish this class, please complete the exercises below. Use spark2-submit by building a jar file to submit these exercises.\n\n1. Use Spark Adapter to create a set of tables where you can exercise some queries including join operations.\n2. Use the tables in Step-1 to load data into tables with Spark Adapter. Use both DF and internalDF to do these operations.\n3. Analyze the table using Spark Adapter\n4. Query the table and verify data load. \n5. Use a Join operation via Spark Adapter\n6. Drop the table(s)\n",
      "user": "anonymous",
      "dateUpdated": "2019-06-03 18:55:15.918",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eWhere to Go Next\u003c/h2\u003e\n\u003cp\u003eTo finish this class, please complete the exercises below. Use spark2-submit by building a jar file to submit these exercises.\u003c/p\u003e\n\u003col\u003e\n  \u003cli\u003eUse Spark Adapter to create a set of tables where you can exercise some queries including join operations.\u003c/li\u003e\n  \u003cli\u003eUse the tables in Step-1 to load data into tables with Spark Adapter. Use both DF and internalDF to do these operations.\u003c/li\u003e\n  \u003cli\u003eAnalyze the table using Spark Adapter\u003c/li\u003e\n  \u003cli\u003eQuery the table and verify data load.\u003c/li\u003e\n  \u003cli\u003eUse a Join operation via Spark Adapter\u003c/li\u003e\n  \u003cli\u003eDrop the table(s)\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559325060276_-1719865439",
      "id": "20190531-175100_779787345",
      "dateCreated": "2019-05-31 17:51:00.276",
      "dateStarted": "2019-06-03 18:55:15.922",
      "dateFinished": "2019-06-03 18:55:19.023",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Splice Machine Training /Advanced Developer/i. Using spark-submit with Native Spark Datasource",
  "id": "2EECKNU39",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}