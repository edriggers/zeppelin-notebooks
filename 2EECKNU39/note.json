{
  "paragraphs": [
    {
      "text": "%md\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n# Using spark-submit With the Splice Machine Native Spark DataSource\n\nIn the basic development course, you have developed native spark applications via the notebook. In this course, we will develop code that will be submitted via spark-submit to the cluster.\n\nThe Splice Machine Native Spark DataSource, which is also referred to as the Spark Adapter, allows you to directly connect Spark DataFrames and Splice Machine database tables, bypassing the need to send your data to/from Spark over a JDBC or ODBC connection. You can efficiently insert, upsert, select, update, and delete data in your Splice Machine tables directly from Spark in a transactionally consistent manner. With the Spark Adapter, transfers of data between Spark and your database are completed without serialization/deserialization, which generates tremendous performance boosts over traditional over-the-wire (sequentially over a JDBC/ODBC connection) transfers.\n\nTo use the Spark Adapter in your code, you simply instantiate a `SplicemachineContext` object in your Spark code. You can run Spark applications that interface with your Splice Machine database interactively in the Spark shell, in Zeppelin notebooks, or by using our Spark Submit script. One common use of the Adapter is to ingest data into your Splice Machine database directly from a Spark DataFrame.\n\nThe Native DataSource allows data scientists to bypass the limitations of the SQL-based JDBC interface in favor of the more scalable and powerful Spark DataFrame API, making it possible for them to operate on data at scale and ingest real-time streaming data with outstanding performance. You can craft applications that use Spark and our Native Spark DataSource in Scala, Python, and Java. Note that you can use the Native Spark DataSource in the Splice Machine ML Manager and Zeppelin Notebook interfaces.\n\n",
      "user": "anonymous",
      "dateUpdated": "2019-06-12 15:32:26.354",
      "config": {
        "editorHide": true,
        "enabled": true,
        "results": {},
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\u003ch1\u003eUsing spark-submit With the Splice Machine Native Spark DataSource\u003c/h1\u003e\n\u003cp\u003eIn the basic development course, you have developed native spark applications via the notebook. In this course, we will develop code that will be submitted via spark-submit to the cluster.\u003c/p\u003e\n\u003cp\u003eThe Splice Machine Native Spark DataSource, which is also referred to as the Spark Adapter, allows you to directly connect Spark DataFrames and Splice Machine database tables, bypassing the need to send your data to/from Spark over a JDBC or ODBC connection. You can efficiently insert, upsert, select, update, and delete data in your Splice Machine tables directly from Spark in a transactionally consistent manner. With the Spark Adapter, transfers of data between Spark and your database are completed without serialization/deserialization, which generates tremendous performance boosts over traditional over-the-wire (sequentially over a JDBC/ODBC connection) transfers.\u003c/p\u003e\n\u003cp\u003eTo use the Spark Adapter in your code, you simply instantiate a \u003ccode\u003eSplicemachineContext\u003c/code\u003e object in your Spark code. You can run Spark applications that interface with your Splice Machine database interactively in the Spark shell, in Zeppelin notebooks, or by using our Spark Submit script. One common use of the Adapter is to ingest data into your Splice Machine database directly from a Spark DataFrame.\u003c/p\u003e\n\u003cp\u003eThe Native DataSource allows data scientists to bypass the limitations of the SQL-based JDBC interface in favor of the more scalable and powerful Spark DataFrame API, making it possible for them to operate on data at scale and ingest real-time streaming data with outstanding performance. You can craft applications that use Spark and our Native Spark DataSource in Scala, Python, and Java. Note that you can use the Native Spark DataSource in the Splice Machine ML Manager and Zeppelin Notebook interfaces.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559319019968_-1240214828",
      "id": "20190531-161019_1313997856",
      "dateCreated": "2019-05-31 16:10:19.968",
      "dateStarted": "2019-06-12 15:32:26.364",
      "dateFinished": "2019-06-12 15:32:29.114",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Why Use the Native Data Source?\n\nThe primary reason for using the Native DataSource is that it provides dramatic performance improvements for large scale data operations; this is because the DataSource works directly on native DataFrames and RDDs, thus eliminating the need to serialize data. Spark is optimized to work on DataFrames, which is a distributed collection of data (an RDD) organized into named columns, with a schema that specifies data types, that is designed to support efficiently operating on scalable, massive datasets.\n\nThe Splice Machine DataSource is native to Spark, which means that it operates directly on these DataFrames and in the same Spark executors that your programs are using to analyze or transform the data. Instead of accessing, inserting, or manipulating data one record at a time over a serialized connection, you can use the Splice Machine Native Spark DataSource to pull the contents of an entire DataFrame into your database, and to pull database query results into a DataFrame.\n\nSplice Machine has observed 100x performance increases compared to using JDBC for operations such as inserting millions of records in a database! For example, a typical web application might use a JDBC connection to query the database, pulling information out one record at a time to populate the screen. The results of each query are serialized (turned into a string of data), then sent over a network connection to the app, and then displayed on the customer’s screen.\n\nIn contrast, when you use the Splice Machine Native Spark DataSource, the contents of the database table are typically sitting in a DataFrame in memory that resides on the same Spark executor that’s performing the query. The query takes place in memory, and there’s no need to serialize or stream the results over a wire. Similarly, when the app sends updates to the database, the data is inserted into the database from in-memory DataFrames directly to the tables without serialization. As a result, a great deal of overhead is eliminated, and performance gains can be remarkable.\n\n",
      "user": "anonymous",
      "dateUpdated": "2019-06-05 23:06:28.660",
      "config": {
        "editorHide": true,
        "enabled": false,
        "results": {},
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "msg": [
          {
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\u003ch2\u003eWhy Use the Native Data Source?\u003c/h2\u003e\n\u003cp\u003eThe primary reason for using the Native DataSource is that it provides dramatic performance improvements for large scale data operations; this is because the DataSource works directly on native DataFrames and RDDs, thus eliminating the need to serialize data. Spark is optimized to work on DataFrames, which is a distributed collection of data (an RDD) organized into named columns, with a schema that specifies data types, that is designed to support efficiently operating on scalable, massive datasets.\u003c/p\u003e\n\u003cp\u003eThe Splice Machine DataSource is native to Spark, which means that it operates directly on these DataFrames and in the same Spark executors that your programs are using to analyze or transform the data. Instead of accessing, inserting, or manipulating data one record at a time over a serialized connection, you can use the Splice Machine Native Spark DataSource to pull the contents of an entire DataFrame into your database, and to pull database query results into a DataFrame.\u003c/p\u003e\n\u003cp\u003eSplice Machine has observed 100x performance increases compared to using JDBC for operations such as inserting millions of records in a database! For example, a typical web application might use a JDBC connection to query the database, pulling information out one record at a time to populate the screen. The results of each query are serialized (turned into a string of data), then sent over a network connection to the app, and then displayed on the customer’s screen.\u003c/p\u003e\n\u003cp\u003eIn contrast, when you use the Splice Machine Native Spark DataSource, the contents of the database table are typically sitting in a DataFrame in memory that resides on the same Spark executor that’s performing the query. The query takes place in memory, and there’s no need to serialize or stream the results over a wire. Similarly, when the app sends updates to the database, the data is inserted into the database from in-memory DataFrames directly to the tables without serialization. As a result, a great deal of overhead is eliminated, and performance gains can be remarkable.\u003c/p\u003e\n\u003c/div\u003e",
            "type": "HTML"
          }
        ],
        "code": "SUCCESS"
      },
      "apps": [],
      "jobName": "paragraph_1559506884945_1945185413",
      "id": "20190602-202124_1993675938",
      "dateCreated": "2019-06-02 20:21:24.945",
      "dateStarted": "2019-06-05 23:04:52.241",
      "dateFinished": "2019-06-05 23:04:52.293",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n## The SpliceMachineContext Class\n\nThe first thing you need to do when using the Native Spark DataSource is to create an instance of the SplicemachineContext class; this is the primary serializable class that you can broadcast in your Spark applications. This class interacts with your Splice Machine cluster in your Spark executors, and provides the methods that you can use to perform operations such as:\n\n* Interfacing with Splice Machine RDD\n* Running inserts, updates and deletes on data\n* Converting data types between Splice Machine and Spark",
      "user": "anonymous",
      "dateUpdated": "2019-06-05 23:05:46.122",
      "config": {
        "editorHide": true,
        "enabled": false,
        "results": {},
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "msg": [
          {
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\u003ch2\u003eThe SpliceMachineContext Class\u003c/h2\u003e\n\u003cp\u003eThe first thing you need to do when using the Native Spark DataSource is to create an instance of the SplicemachineContext class; this is the primary serializable class that you can broadcast in your Spark applications. This class interacts with your Splice Machine cluster in your Spark executors, and provides the methods that you can use to perform operations such as:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eInterfacing with Splice Machine RDD\u003c/li\u003e\n  \u003cli\u003eRunning inserts, updates and deletes on data\u003c/li\u003e\n  \u003cli\u003eConverting data types between Splice Machine and Spark\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e",
            "type": "HTML"
          }
        ],
        "code": "SUCCESS"
      },
      "apps": [],
      "jobName": "paragraph_1559508964492_919362593",
      "id": "20190602-205604_247573122",
      "dateCreated": "2019-06-02 20:56:04.492",
      "dateStarted": "2019-06-05 23:05:46.123",
      "dateFinished": "2019-06-05 23:05:46.140",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Set Up Maven Coordinates\n\nYou\u0027ll need to setup a dependency resolution system to be able to build the code. We have used Maven for it and provided sample code and pom.xml file at public github repo. You can use this as a template for maven setup. It has both CDH or HWX dependency profiles that can be selected via maven configuration at build time.\n\n[Sample Source Code](https://github.com/splicemachine/splice-training/tree/master/spark_adapter_example)\n",
      "user": "anonymous",
      "dateUpdated": "2019-06-12 15:38:07.006",
      "config": {
        "editorHide": true,
        "enabled": true,
        "results": {},
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {
          "spliceVersion": "",
          "project.build.directory": "",
          "artifactId": "",
          "version": ""
        },
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eSet Up Maven Coordinates\u003c/h2\u003e\n\u003cp\u003eYou\u0026rsquo;ll need to setup a dependency resolution system to be able to build the code. We have used Maven for it and provided sample code and pom.xml file at public github repo. You can use this as a template for maven setup. It has both CDH or HWX dependency profiles that can be selected via maven configuration at build time.\u003c/p\u003e\n\u003cp\u003e\u003ca href\u003d\"https://github.com/splicemachine/splice-training/tree/master/spark_adapter_example\"\u003eSample Source Code\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559511543147_1644452621",
      "id": "20190602-213903_1547198808",
      "dateCreated": "2019-06-02 21:39:03.147",
      "dateStarted": "2019-06-12 15:38:07.011",
      "dateFinished": "2019-06-12 15:38:07.041",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n## Create SpliceMachineContext\n\nThe following code snippet illustrates creating a `SpliceMachineContext` object in Java after setting up the project:\n\n\u003cpre\u003e\u003ccode\u003e\nimport com.splicemachine.derby.impl.SpliceSpark;\nimport com.splicemachine.spark.splicemachine.SplicemachineContext;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n....\n\npublic class TestSpliceMachineContext\n{\n    public static void main(String[] args) throws Exception \n    {\n        String dbUrl \u003d \"jdbc:splice://SPLICESERVERHOST:1527/splicedb;user\u003d\u0026lt;yourUserId\u0026gt;;password\u003d\u0026lt;yourPassword\u0026gt;\";\n        SparkConf conf \u003d new SparkConf();\n        SparkSession spark \u003d SparkSession.builder().appName(\"NativeSparktutor\").config(conf).getOrCreate();\n        SpliceSpark.setContext(spark.sparkContext());\n        SplicemachineContext splicemachineContext \u003d new SplicemachineContext(dbUrl);\n        \n        //use context for DB operations next...\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e",
      "user": "anonymous",
      "dateUpdated": "2019-06-12 15:39:41.415",
      "config": {
        "editorHide": true,
        "enabled": true,
        "results": {},
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\u003ch2\u003eCreate SpliceMachineContext\u003c/h2\u003e\n\u003cp\u003eThe following code snippet illustrates creating a \u003ccode\u003eSpliceMachineContext\u003c/code\u003e object in Java after setting up the project:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\nimport com.splicemachine.derby.impl.SpliceSpark;\nimport com.splicemachine.spark.splicemachine.SplicemachineContext;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n....\n\npublic class TestSpliceMachineContext\n{\n    public static void main(String[] args) throws Exception \n    {\n        String dbUrl \u003d \"jdbc:splice://SPLICESERVERHOST:1527/splicedb;user\u003d\u0026lt;yourUserId\u0026gt;;password\u003d\u0026lt;yourPassword\u0026gt;\";\n        SparkConf conf \u003d new SparkConf();\n        SparkSession spark \u003d SparkSession.builder().appName(\"NativeSparktutor\").config(conf).getOrCreate();\n        SpliceSpark.setContext(spark.sparkContext());\n        SplicemachineContext splicemachineContext \u003d new SplicemachineContext(dbUrl);\n        \n        //use context for DB operations next...\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559511623111_-960005237",
      "id": "20190602-214023_872601603",
      "dateCreated": "2019-06-02 21:40:23.111",
      "dateStarted": "2019-06-12 15:39:41.416",
      "dateFinished": "2019-06-12 15:39:41.449",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n# Query using SpliceMachineContext\n\nUsing the `SpliceMachineContext`, you can obtain access to Spark DataFrame within this context while querying the DB. \n\n\u003cpre\u003e\u003ccode\u003e\nimport com.splicemachine.derby.impl.SpliceSpark;\nimport com.splicemachine.spark.splicemachine.SplicemachineContext;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n....\n\npublic class TestSpliceMachineContext\n{\n    public static void main(String[] args) throws Exception \n    {\n        String dbUrl \u003d \"jdbc:splice://SPLICESERVERHOST:1527/splicedb;user\u003d\u0026lt;yourUserId\u0026gt;;password\u003d\u0026lt;yourPassword\u0026gt;\";\n        SparkConf conf \u003d new SparkConf();\n        SparkSession spark \u003d SparkSession.builder().appName(\"NativeSparktutor\").config(conf).getOrCreate();\n        SpliceSpark.setContext(spark.sparkContext());\n        SplicemachineContext splicemachineContext \u003d new SplicemachineContext(dbUrl);\n        \n        //Query table, use df operations based out of SpliceMachineContext\n        String spliceQuery \u003d \"select count(*) from \" + \u0026lt;TABLE_NAME\u0026gt;;\n        splicemachineContext.df(spliceQuery).show();\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e",
      "user": "anonymous",
      "dateUpdated": "2019-06-12 15:39:56.330",
      "config": {
        "editorHide": true,
        "enabled": true,
        "results": {},
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\u003ch1\u003eQuery using SpliceMachineContext\u003c/h1\u003e\n\u003cp\u003eUsing the \u003ccode\u003eSpliceMachineContext\u003c/code\u003e, you can obtain access to Spark DataFrame within this context while querying the DB. \u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\nimport com.splicemachine.derby.impl.SpliceSpark;\nimport com.splicemachine.spark.splicemachine.SplicemachineContext;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n....\n\npublic class TestSpliceMachineContext\n{\n    public static void main(String[] args) throws Exception \n    {\n        String dbUrl \u003d \"jdbc:splice://SPLICESERVERHOST:1527/splicedb;user\u003d\u0026lt;yourUserId\u0026gt;;password\u003d\u0026lt;yourPassword\u0026gt;\";\n        SparkConf conf \u003d new SparkConf();\n        SparkSession spark \u003d SparkSession.builder().appName(\"NativeSparktutor\").config(conf).getOrCreate();\n        SpliceSpark.setContext(spark.sparkContext());\n        SplicemachineContext splicemachineContext \u003d new SplicemachineContext(dbUrl);\n        \n        //Query table, use df operations based out of SpliceMachineContext\n        String spliceQuery \u003d \"select count(*) from \" + \u0026lt;TABLE_NAME\u0026gt;;\n        splicemachineContext.df(spliceQuery).show();\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559578447544_-125019814",
      "id": "20190603-161407_2067737309",
      "dateCreated": "2019-06-03 16:14:07.544",
      "dateStarted": "2019-06-12 15:39:56.351",
      "dateFinished": "2019-06-12 15:39:56.394",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n# Load data using SpliceMachineContext\n\nYou can use the `SpliceMachineContext` to load data into a table. In the following, we read from a parquet file and load data into Splice Machine table.\n\n\u003cpre\u003e\u003ccode\u003e\nimport com.splicemachine.derby.impl.SpliceSpark;\nimport com.splicemachine.spark.splicemachine.SplicemachineContext;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n....\n\npublic class TestSpliceMachineContext\n{\n    public static void main(String[] args) throws Exception \n    {\n        String dbUrl \u003d \"jdbc:splice://SPLICESERVERHOST:1527/splicedb;user\u003d\u0026lt;yourUserId\u0026gt;;password\u003d\u0026lt;yourPassword\u0026gt;\";\n        SparkConf conf \u003d new SparkConf();\n        SparkSession spark \u003d SparkSession.builder().appName(\"NativeSparktutor\").config(conf).getOrCreate();\n        SpliceSpark.setContext(spark.sparkContext());\n        SplicemachineContext splicemachineContext \u003d new SplicemachineContext(dbUrl);\n        \n        //Load data into a table using Parquet file\n        System.out.println(\"start create table ...\");\n    \tsplicemachineContext.createTable(\"test.car\", schema, \n    \t\t\tJavaConverters.asScalaIteratorConverter(Arrays.asList(\"PRIMARY KEY (SERIAL)\").iterator()).asScala().toSeq(), \"\");\n    \tSystem.out.println(\"done create table ... \");\n    \t\n    \tStructType outputSchema \u003d splicemachineContext.getSchema(\"test.car\");\n    \toutputSchema.printTreeString();\n    \t\n    \tSystem.out.println(\"start insert ...\");\n    \tsplicemachineContext.insert(carsDF, \"test.car\");\n    \tSystem.out.println(\"done insert ... \");\n    \t\n    \tSystem.out.println(\"start select ...\");\n    \tDataset\u0026lt;Row\u0026gt; selectedCarsDF \u003d splicemachineContext.df(\"select * from test.car\");\n    \tSystem.out.println(\"done select ...\");\n    \t\n    \tselectedCarsDF.count();\n    \tselectedCarsDF.show();\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e",
      "user": "anonymous",
      "dateUpdated": "2019-06-12 15:40:06.208",
      "config": {
        "editorHide": true,
        "enabled": true,
        "results": {},
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" /\u003e\n\u003ch1\u003eLoad data using SpliceMachineContext\u003c/h1\u003e\n\u003cp\u003eYou can use the \u003ccode\u003eSpliceMachineContext\u003c/code\u003e to load data into a table. In the following, we read from a parquet file and load data into Splice Machine table.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\nimport com.splicemachine.derby.impl.SpliceSpark;\nimport com.splicemachine.spark.splicemachine.SplicemachineContext;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n....\n\npublic class TestSpliceMachineContext\n{\n    public static void main(String[] args) throws Exception \n    {\n        String dbUrl \u003d \"jdbc:splice://SPLICESERVERHOST:1527/splicedb;user\u003d\u0026lt;yourUserId\u0026gt;;password\u003d\u0026lt;yourPassword\u0026gt;\";\n        SparkConf conf \u003d new SparkConf();\n        SparkSession spark \u003d SparkSession.builder().appName(\"NativeSparktutor\").config(conf).getOrCreate();\n        SpliceSpark.setContext(spark.sparkContext());\n        SplicemachineContext splicemachineContext \u003d new SplicemachineContext(dbUrl);\n        \n        //Load data into a table using Parquet file\n        System.out.println(\"start create table ...\");\n    \tsplicemachineContext.createTable(\"test.car\", schema, \n    \t\t\tJavaConverters.asScalaIteratorConverter(Arrays.asList(\"PRIMARY KEY (SERIAL)\").iterator()).asScala().toSeq(), \"\");\n    \tSystem.out.println(\"done create table ... \");\n    \t\n    \tStructType outputSchema \u003d splicemachineContext.getSchema(\"test.car\");\n    \toutputSchema.printTreeString();\n    \t\n    \tSystem.out.println(\"start insert ...\");\n    \tsplicemachineContext.insert(carsDF, \"test.car\");\n    \tSystem.out.println(\"done insert ... \");\n    \t\n    \tSystem.out.println(\"start select ...\");\n    \tDataset\u0026lt;Row\u0026gt; selectedCarsDF \u003d splicemachineContext.df(\"select * from test.car\");\n    \tSystem.out.println(\"done select ...\");\n    \t\n    \tselectedCarsDF.count();\n    \tselectedCarsDF.show();\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559579990934_-1817260673",
      "id": "20190603-163950_1385722937",
      "dateCreated": "2019-06-03 16:39:50.934",
      "dateStarted": "2019-06-12 15:40:06.221",
      "dateFinished": "2019-06-12 15:40:06.261",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## InternalAccess While Using SpliceMachineContext\n\nBy default, Native Spark DataSource queries execute in the Spark application, which is highly performant and allows access to almost all Splice Machine features. However, when your Native Spark DataSource application uses our Access Control List (ACL) feature, there is a restriction with regard to checking permissions.\n\nThe specific problem is that the Native Spark DataSource does not have the ability to check permissions at the view level or column level; instead, it checks permissions on the base table. This means that if your Native Spark DataSource application doesn’t have access to the table underlying a view or column, it will not have access to that view or column; as a result, a query against the view or colunn fails and throws an exception.\n\nThe workaround for this problem is to tell the Native Spark DataSource to use internal access to the database; this enables view/column permission checking, at a very slight cost in performance. With internal access, the adapter runs queries in Splice Machine and temporarily persists data in HDFS while running the query.\n\nThe ACL feature is enabled by setting `splice.authentication.token.enabled\u003dtrue`.\n\nIn addition, to use the Splice Native Spark DataSource, a user must have `execute` permission on the following four system procedures:\n\n* `SYSCS_HBASE_OPERATION`\n* `SYSCS_HDFS_OPERATION`\n* `SYSCS_GET_SPLICE_TOKEN`\n* `SYSCS_CANCEL_SPLICE_TOKEN`\n\n\u003cp class\u003d\"noteIcon\"\u003eThese procedures are all Splice Machine system procedures that are used internally to efficiently perform direct HBASE and HDFS operations. They *are not documented* because they are intended only for use by the Splice Machine code itself; however, the Native Spark DataSource uses these procedures, so any user of the Adapter must have permission to execute them.\u003c/p\u003e\n\nHere\u0027s an example of granting `execute` permission to these procedures to a user named `myUserName`:\n\n```\nsplice\u003e grant execute on procedure SYSCS_UTIL.SYSCS_HBASE_OPERATION to myUserName;\n0 rows inserted/updated/deleted\nsplice\u003e grant execute on procedure SYSCS_UTIL.SYSCS_HDFS_OPERATION to myUserName;\n0 rows inserted/updated/deleted\nsplice\u003e grant execute on procedure SYSCS_UTIL.SYSCS_GET_SPLICE_TOKEN to myUserName;\n0 rows inserted/updated/deleted\nsplice\u003e grant execute on procedure SYSCS_UTIL.SYSCS_CANCEL_SPLICE_TOKEN to myUserName;\n0 rows inserted/updated/deleted\n```\n\nHere is an example to use internalDF instead of Spark DF:\n\n\u003cpre\u003e\u003ccode\u003e\nimport com.splicemachine.derby.impl.SpliceSpark;\nimport com.splicemachine.spark.splicemachine.SplicemachineContext;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n....\n\npublic class TestSpliceMachineContext\n{\n    public static void main(String[] args) throws Exception \n    {\n        String dbUrl \u003d \"jdbc:splice://SPLICESERVERHOST:1527/splicedb;user\u003d\u0026lt;yourUserId\u0026gt;;password\u003d\u0026lt;yourPassword\u0026gt;\";\n        SparkConf conf \u003d new SparkConf();\n        SparkSession spark \u003d SparkSession.builder().appName(\"NativeSparktutor\").config(conf).getOrCreate();\n        SpliceSpark.setContext(spark.sparkContext());\n        SplicemachineContext splicemachineContext \u003d new SplicemachineContext(dbUrl);\n        \n        //Query table, use df operations based out of SpliceMachineContext\n        String spliceQuery \u003d \"select count(*) from \" + \u0026lt;TABLE_NAME\u0026gt;;\n        splicemachineContext.internalDf(spliceQuery).show();\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e",
      "user": "anonymous",
      "dateUpdated": "2019-06-12 15:40:25.775",
      "config": {
        "editorHide": true,
        "enabled": true,
        "results": {},
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eInternalAccess While Using SpliceMachineContext\u003c/h2\u003e\n\u003cp\u003eBy default, Native Spark DataSource queries execute in the Spark application, which is highly performant and allows access to almost all Splice Machine features. However, when your Native Spark DataSource application uses our Access Control List (ACL) feature, there is a restriction with regard to checking permissions.\u003c/p\u003e\n\u003cp\u003eThe specific problem is that the Native Spark DataSource does not have the ability to check permissions at the view level or column level; instead, it checks permissions on the base table. This means that if your Native Spark DataSource application doesn’t have access to the table underlying a view or column, it will not have access to that view or column; as a result, a query against the view or colunn fails and throws an exception.\u003c/p\u003e\n\u003cp\u003eThe workaround for this problem is to tell the Native Spark DataSource to use internal access to the database; this enables view/column permission checking, at a very slight cost in performance. With internal access, the adapter runs queries in Splice Machine and temporarily persists data in HDFS while running the query.\u003c/p\u003e\n\u003cp\u003eThe ACL feature is enabled by setting \u003ccode\u003esplice.authentication.token.enabled\u003dtrue\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIn addition, to use the Splice Native Spark DataSource, a user must have \u003ccode\u003eexecute\u003c/code\u003e permission on the following four system procedures:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\u003ccode\u003eSYSCS_HBASE_OPERATION\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ccode\u003eSYSCS_HDFS_OPERATION\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ccode\u003eSYSCS_GET_SPLICE_TOKEN\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ccode\u003eSYSCS_CANCEL_SPLICE_TOKEN\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp class\u003d\"noteIcon\"\u003eThese procedures are all Splice Machine system procedures that are used internally to efficiently perform direct HBASE and HDFS operations. They *are not documented* because they are intended only for use by the Splice Machine code itself; however, the Native Spark DataSource uses these procedures, so any user of the Adapter must have permission to execute them.\u003c/p\u003e\n\u003cp\u003eHere\u0026rsquo;s an example of granting \u003ccode\u003eexecute\u003c/code\u003e permission to these procedures to a user named \u003ccode\u003emyUserName\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esplice\u0026gt; grant execute on procedure SYSCS_UTIL.SYSCS_HBASE_OPERATION to myUserName;\n0 rows inserted/updated/deleted\nsplice\u0026gt; grant execute on procedure SYSCS_UTIL.SYSCS_HDFS_OPERATION to myUserName;\n0 rows inserted/updated/deleted\nsplice\u0026gt; grant execute on procedure SYSCS_UTIL.SYSCS_GET_SPLICE_TOKEN to myUserName;\n0 rows inserted/updated/deleted\nsplice\u0026gt; grant execute on procedure SYSCS_UTIL.SYSCS_CANCEL_SPLICE_TOKEN to myUserName;\n0 rows inserted/updated/deleted\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere is an example to use internalDF instead of Spark DF:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\nimport com.splicemachine.derby.impl.SpliceSpark;\nimport com.splicemachine.spark.splicemachine.SplicemachineContext;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n....\n\npublic class TestSpliceMachineContext\n{\n    public static void main(String[] args) throws Exception \n    {\n        String dbUrl \u003d \"jdbc:splice://SPLICESERVERHOST:1527/splicedb;user\u003d\u0026lt;yourUserId\u0026gt;;password\u003d\u0026lt;yourPassword\u0026gt;\";\n        SparkConf conf \u003d new SparkConf();\n        SparkSession spark \u003d SparkSession.builder().appName(\"NativeSparktutor\").config(conf).getOrCreate();\n        SpliceSpark.setContext(spark.sparkContext());\n        SplicemachineContext splicemachineContext \u003d new SplicemachineContext(dbUrl);\n        \n        //Query table, use df operations based out of SpliceMachineContext\n        String spliceQuery \u003d \"select count(*) from \" + \u0026lt;TABLE_NAME\u0026gt;;\n        splicemachineContext.internalDf(spliceQuery).show();\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559580591238_-232357716",
      "id": "20190603-164951_106071028",
      "dateCreated": "2019-06-03 16:49:51.238",
      "dateStarted": "2019-06-12 15:40:25.775",
      "dateFinished": "2019-06-12 15:40:25.833",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Build and Deploy The Application\n\nYou can build the Jar (we suggest building uber jar) using `maven`, similar to how you build and deploy a typical Spark application.\n\n```\nmvn clean package\n```\n\nOnce the jar is built (typically in `target` folder within the project folder), you can copy it over to the edge node or region server on the cluster to execute with the spark-submit command.\n\nNote that if you are using the docker image, you can copy the jar over to the running instance using the following command:\n\n```\ndocker cp \u003cPATH_ON_HOST_MACHINE\u003e spliceserver:\u003cPATH_TO_TARGET_FOLDER_ON_RUNNING_INSTANCE\u003e\n```\n",
      "user": "anonymous",
      "dateUpdated": "2019-06-05 23:21:53.682",
      "config": {
        "editorHide": true,
        "enabled": false,
        "results": {},
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "msg": [
          {
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eBuild and Deploy The Application\u003c/h2\u003e\n\u003cp\u003eYou can build the Jar (we suggest building uber jar) using \u003ccode\u003emaven\u003c/code\u003e, similar to how you build and deploy a typical Spark application.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emvn clean package\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce the jar is built (typically in \u003ccode\u003etarget\u003c/code\u003e folder within the project folder), you can copy it over to the edge node or region server on the cluster to execute with the spark-submit command.\u003c/p\u003e\n\u003cp\u003eNote that if you are using the docker image, you can copy the jar over to the running instance using the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker cp \u0026lt;PATH_ON_HOST_MACHINE\u0026gt; spliceserver:\u0026lt;PATH_TO_TARGET_FOLDER_ON_RUNNING_INSTANCE\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e",
            "type": "HTML"
          }
        ],
        "code": "SUCCESS"
      },
      "apps": [],
      "jobName": "paragraph_1559581897072_927636269",
      "id": "20190603-171137_1488177463",
      "dateCreated": "2019-06-03 17:11:37.073",
      "dateStarted": "2019-06-05 23:21:53.683",
      "dateFinished": "2019-06-05 23:21:53.698",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Using spark submit to Launch Spark Adapter Applications\n\nYou can use the following command on the provided docker image. Note that various executor settings are just illustrative and should be configured for respective job appropriately and according to the execution environment:\n\n\u003cpre\u003e\u003ccode\u003e\nspark-submit --conf \"spark.dynamicAllocation.enabled\u003dfalse\" --conf \"spark.task.maxFailures\u003d2\" --conf \"spark.driver.memory\u003d512m\" --conf \"spark.driver.cores\u003d1\" --conf \"spark.kryoserializer.buffer\u003d512\" --conf \"spark.kryoserializer.buffer.max\u003d1024\" --conf \"spark.io.compression.codec\u003dorg.apache.spark.io.SnappyCompressionCodec\" --conf \"spark.executor.extraClassPath\u003d/opt/hadoop/conf/:/opt/spark/jars/*\" --conf \"spark.driver.extraClassPath\u003d/opt/hadoop/conf/:/opt/spark/jars/*\" --class \u0026lt;FULLY_QUALIFIED_EXECUTABLE_CLASS_NAME\u0026gt; --master local[1] --deploy-mode client --num-executors 1 --executor-memory 1G --executor-cores 1 \u0026lt;JAR_FILE_NAME\u0026gt; localhost 1527 splice admin\n\u003c/code\u003e\u003c/pre\u003e",
      "user": "anonymous",
      "dateUpdated": "2019-06-11 20:51:47.765",
      "config": {
        "editorHide": true,
        "enabled": true,
        "results": {},
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eUsing spark submit to Launch Spark Adapter Applications\u003c/h2\u003e\n\u003cp\u003eYou can use the following command on the provided docker image. Note that various executor settings are just illustrative and should be configured for respective job appropriately and according to the execution environment:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\nspark-submit --conf \"spark.dynamicAllocation.enabled\u003dfalse\" --conf \"spark.task.maxFailures\u003d2\" --conf \"spark.driver.memory\u003d512m\" --conf \"spark.driver.cores\u003d1\" --conf \"spark.kryoserializer.buffer\u003d512\" --conf \"spark.kryoserializer.buffer.max\u003d1024\" --conf \"spark.io.compression.codec\u003dorg.apache.spark.io.SnappyCompressionCodec\" --conf \"spark.executor.extraClassPath\u003d/opt/hadoop/conf/:/opt/spark/jars/*\" --conf \"spark.driver.extraClassPath\u003d/opt/hadoop/conf/:/opt/spark/jars/*\" --class \u0026lt;FULLY_QUALIFIED_EXECUTABLE_CLASS_NAME\u0026gt; --master local[1] --deploy-mode client --num-executors 1 --executor-memory 1G --executor-cores 1 \u0026lt;JAR_FILE_NAME\u0026gt; localhost 1527 splice admin\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559669789849_-819139079",
      "id": "20190604-173629_1491743944",
      "dateCreated": "2019-06-04 17:36:29.849",
      "dateStarted": "2019-06-11 20:51:47.761",
      "dateFinished": "2019-06-11 20:51:47.786",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## For Further Exploration and Understanding\n\nTo gain a better understanding of using statistics, try using `spark-submit` by building a JAR file and running these exercises:\n\n1. Use Spark Adapter to create a set of tables where you can exercise some queries including join operations.\n2. Use the tables in Step-1 to load data into tables with Spark Adapter. Use both DF and internalDF to do these operations.\n3. Analyze the table using Spark Adapter\n4. Query the table and verify data load. \n5. Use a Join operation via Spark Adapter\n6. Drop the table(s)\n",
      "user": "anonymous",
      "dateUpdated": "2019-06-11 20:52:45.449",
      "config": {
        "editorHide": true,
        "enabled": true,
        "results": {},
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eFor Further Exploration and Understanding\u003c/h2\u003e\n\u003cp\u003eTo gain a better understanding of using statistics, try using \u003ccode\u003espark-submit\u003c/code\u003e by building a JAR file and running these exercises:\u003c/p\u003e\n\u003col\u003e\n  \u003cli\u003eUse Spark Adapter to create a set of tables where you can exercise some queries including join operations.\u003c/li\u003e\n  \u003cli\u003eUse the tables in Step-1 to load data into tables with Spark Adapter. Use both DF and internalDF to do these operations.\u003c/li\u003e\n  \u003cli\u003eAnalyze the table using Spark Adapter\u003c/li\u003e\n  \u003cli\u003eQuery the table and verify data load.\u003c/li\u003e\n  \u003cli\u003eUse a Join operation via Spark Adapter\u003c/li\u003e\n  \u003cli\u003eDrop the table(s)\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559773713057_-717419114",
      "id": "20190605-222833_1832541911",
      "dateCreated": "2019-06-05 22:28:33.058",
      "dateStarted": "2019-06-11 20:52:45.451",
      "dateFinished": "2019-06-11 20:52:45.528",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Where to Go Next\nTo finish this class, please complete the exercises in the \u003ca href\u003d\"/#/notebook/2EDNZACZA\"\u003e\u003cem\u003eExercises for This Class\u003c/em\u003e\u003c/a\u003e notebook, which test your understanding of the material we\u0027ve covered.\u003c/td\u003e\n",
      "user": "anonymous",
      "dateUpdated": "2019-06-05 23:23:49.734",
      "config": {
        "editorHide": true,
        "enabled": false,
        "results": {},
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "msg": [
          {
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eWhere to Go Next\u003c/h2\u003e\n\u003cp\u003eTo finish this class, please complete the exercises in the \u003ca href\u003d\"/#/notebook/2EDNZACZA\"\u003e\u003cem\u003eExercises for This Class\u003c/em\u003e\u003c/a\u003e notebook, which test your understanding of the material we\u0026rsquo;ve covered.\u003c/td\u003e\u003c/p\u003e\n\u003c/div\u003e",
            "type": "HTML"
          }
        ],
        "code": "SUCCESS"
      },
      "apps": [],
      "jobName": "paragraph_1559325060276_-1719865439",
      "id": "20190531-175100_779787345",
      "dateCreated": "2019-05-31 17:51:00.276",
      "dateStarted": "2019-06-05 23:23:49.736",
      "dateFinished": "2019-06-05 23:23:49.745",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Splice Machine Training /Advanced Developer/i. Using spark-submit with Native Spark Datasource",
  "id": "2EECKNU39",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}