{"paragraphs":[{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1497986349673_462024923","id":"20170620-141909_771490751","dateCreated":"2017-06-20T14:19:09-0500","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6676","text":"%md\n# Decision Trees\nThe [Decision Tree](https://spark.apache.org/docs/latest/mllib-decision-tree.html) is a greedy algorithm that performs a recursive binary partitioning of the feature space for predictive modeling. \n* Locally optimal decisions are made at each node in hopes of a globally optimal decision\n* Because of it's greedy nature, it cannot guarantee the globally optimal tree\n\nAt its core (and most simplified), decision trees are simply a system of if-else statements, always taking the most optimal answer, resulting in (hopefully) the most optimal decision. See [here](http://mines.humanoriented.com/classes/2010/fall/csci568/portfolio_exports/lguo/image/decisionTree/classification.jpg)\n\nThe example below demonstrates how to load a [LIBSVM](https://github.com/apache/spark/blob/master/data/mllib/sample_libsvm_data.txt) data file, parse it as an RDD of LabeledPoint and then perform classification using a decision tree with Gini impurity as an impurity measure and a maximum tree depth of 5. The test error is calculated to measure the algorithm accuracy. For more information, check out Spark's Decision Tree [page](https://spark.apache.org/docs/latest/mllib-decision-tree.html)\n","dateUpdated":"2017-06-20T15:43:24-0500","dateFinished":"2017-06-20T15:43:24-0500","dateStarted":"2017-06-20T15:43:24-0500","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Decision Trees</h1>\n<p>The <a href=\"https://spark.apache.org/docs/latest/mllib-decision-tree.html\">Decision Tree</a> is a greedy algorithm that performs a recursive binary partitioning of the feature space for predictive modeling.<br/>* Locally optimal decisions are made at each node in hopes of a globally optimal decision<br/>* Because of it&rsquo;s greedy nature, it cannot guarantee the globally optimal tree</p>\n<p>At its core (and most simplified), decision trees are simply a system of if-else statements, always taking the most optimal answer, resulting in (hopefully) the most optimal decision. See <a href=\"http://mines.humanoriented.com/classes/2010/fall/csci568/portfolio_exports/lguo/image/decisionTree/classification.jpg\">here</a></p>\n<p>The example below demonstrates how to load a <a href=\"https://github.com/apache/spark/blob/master/data/mllib/sample_libsvm_data.txt\">LIBSVM</a> data file, parse it as an RDD of LabeledPoint and then perform classification using a decision tree with Gini impurity as an impurity measure and a maximum tree depth of 5. The test error is calculated to measure the algorithm accuracy. For more information, check out Spark&rsquo;s Decision Tree <a href=\"https://spark.apache.org/docs/latest/mllib-decision-tree.html\">page</a></p>\n</div>"}]}},{"text":"%spark\nimport org.apache.spark.mllib.tree.DecisionTree\nimport org.apache.spark.mllib.tree.model.DecisionTreeModel\nimport org.apache.spark.mllib.util.MLUtils\n\n// Load and parse the data file.\nval data = MLUtils.loadLibSVMFile(sc, \"/Users/benepstein/Desktop/sample_libsvm_data.txt\")\n// Split the data into training and test sets (30% held out for testing)\nval splits = data.randomSplit(Array(0.7, 0.3))\nval (trainingData, testData) = (splits(0), splits(1))\n\n// Train a DecisionTree model.\n//  Empty categoricalFeaturesInfo indicates all features are continuous.\nval numClasses = 2\nval categoricalFeaturesInfo = Map[Int, Int]()\nval impurity = \"gini\"\nval maxDepth = 5\nval maxBins = 32\n\nval model = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,\n  impurity, maxDepth, maxBins)\n\n// Evaluate model on test instances and compute test error\nval labelAndPreds = testData.map { point =>\n  val prediction = model.predict(point.features)\n  (point.label, prediction)\n}\nval testErr = labelAndPreds.filter(r => r._1 != r._2).count().toDouble / testData.count()\nprintln(\"Test Error = \" + testErr)\nprintln(\"Learned classification tree model:\\n\" + model.toDebugString)\n\n// Save and load model\n// model.save(sc, \"target/tmp/myDecisionTreeClassificationModel\")\n// val sameModel = DecisionTreeModel.load(sc, \"target/tmp/myDecisionTreeClassificationModel\")","user":"anonymous","dateUpdated":"2017-06-20T15:43:13-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1497990805906_-708317472","id":"20170620-153325_126659046","dateCreated":"2017-06-20T15:33:25-0500","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6744","dateFinished":"2017-06-20T15:43:18-0500","dateStarted":"2017-06-20T15:43:13-0500","errorMessage":""},{"text":"%spark\n","user":"anonymous","dateUpdated":"2017-06-20T15:36:20-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1497990980786_1206252708","id":"20170620-153620_626385294","dateCreated":"2017-06-20T15:36:20-0500","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6824"}],"name":"Decision tree","id":"2CM14K96J","angularObjects":{"2CJYTPUHP:shared_process":[],"2CK8F8QJE:shared_process":[],"2CM5C3S1U:shared_process":[],"2CK9GZN4S:shared_process":[],"2CJTYKKUU:shared_process":[],"2CMQ6E8UF:shared_process":[],"2CM8J7BJM:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}